generate a system propmt/template:


<your-info>

This is a crucial question. You are asking about System Prompts (often referred to as "System Instructions," "Custom Instructions," or "Rules").

In the world of CLI AI agents (like Claude Code, Cursor, Aider, etc.), files like .claude.md, .cursorrules, or .gemini.md act as the Constitution for your agent. They are not just "settings"; they define the agent's intelligence level, coding style, and reliability.

To write these at an Enterprise/Master Level, you must stop treating them as simple notes and start treating them as programmable logic.

Here is the deep breakdown of how to craft a perfect system instruction file.

1. The Core Philosophy: "Context is King, Constraint is Queen"

An expert-level configuration file does two things perfectly:

Reduces Ambiguity: It leaves zero room for the AI to "guess" your style.

Enforces Standards: It acts like a Senior Code Reviewer that is watching over the shoulder of the AI.

2. The Architecture of a Master File

To achieve an "Enterprise Grade" instruction file, you must structure it like a technical specification. Do not write paragraphs of messy text. Use Markdown headers, bullet points, and clear sections.

Your file must contain these 5 Key Components:

A. Role & Persona (The "Who")

Don't just say "You are a coder." Be specific.

Bad: "You are a helpful coding assistant."

Master Level: "You are a Principal Software Engineer specializing in Python backend architecture. You prioritize clean, scalable code over quick fixes. You communicate concisely and authoritatively."

B. Operational Context (The "What")

Tell the agent about the specific project environment.

Features: "This project uses Next.js 14 (App Router) with Tailwind CSS."

State: "We are currently in the refactoring phase, so prioritize modularity."

C. Strict Directives & Constraints (The "Rules")

This is where you define the Acceptance Criteria. You must explicitly tell the AI what NOT to do.

No Yapping: "Do not provide moral lectures or fluff text. Give the solution directly."

Code Style: "Always use Arrow Functions." "Never use var, always use const or let."

Error Handling: "Never leave empty catch blocks."

D. The "Chain of Thought" Requirement (The "How")

For complex tasks, force the agent to think before it acts. This massively reduces bugs.

Instruction: "Before writing any code, analyze the user's request and list the 3 steps you will take to solve it inside a <thinking> tag."

E. Few-Shot Examples (The "Standard")

Show, don't just tell. Provide examples of "Good Response" vs "Bad Response" if your requirements are complex.

3. Detailed Breakdown of Features & Passing Criteria

To evaluate if your file is "Perfect," it must pass these criteria:

FeatureEnterprise StandardWhy?ConsistencyThe agent uses the exact same naming convention (e.g., camelCase) every single time.Prevents messy codebases where styles mix.DeterminismIf asked the same question twice, the structure of the answer remains the same.Critical for automated pipelines.ConcisenessThe agent skips "Here is your code" and "I hope this helps" pleasantries.Saves tokens (money) and time.SafetyThe agent refuses to delete files without explicit confirmation.Prevents accidental data loss.Self-CorrectionThe agent reviews its own code for common bugs (like infinite loops) before outputting.Increases first-shot success rate.4. The "Master Template" (Copy & Customize)

Here is a template designed for high-performance coding agents. You can save this as .claude.md or .cursorrules.

Markdown



# AGENT PERSONA

You are an expert Senior Full-Stack Developer and System Architect.- Your code is production-ready, modular, and DRY (Don't Repeat Yourself).- You prioritize security and performance over brevity.- You act as a partner, suggesting better approaches if the user's request is flawed.# TECH STACK STANDARDS- Language: [e.g., TypeScript, Python 3.11]- Framework: [e.g., Next.js 14, Django]- Styling: [e.g., Tailwind CSS]- Database: [e.g., PostgreSQL with Prisma]# CRITICAL RULES (MUST FOLLOW)1. **No Placeholders:** Never use comments like `// ... rest of code`. Always output full, working functions.2. **Type Safety:** Strict typing is mandatory. No `any` types allowed.3. **Error Handling:** Wrap external API calls in try/catch blocks and provide meaningful error messages.4. **Comments:** Only comment on *complex logic*. Do not comment on obvious code (e.g., do not say "// variable assignment").# COMMUNICATION PROTOCOL- **Concise Mode:** Do not use conversational filler ("Sure, I can help with that"). Start directly with the solution or the analysis.- **Thinking Process:** For complex logical problems, use a `<thinking>` block to outline your logic before providing the code.- **File Management:** When creating files, always include the full path at the top of the code block.# ACCEPTANCE CRITERIA FOR OUTPUT- Code must pass standard linting rules.- Code must be modular (functions under 50 lines where possible).- Variables must be descriptive (`userAge` NOT `x`).

5. Advanced Tip: The "Update Loop"

A true expert doesn't write this file once. They treat it as a living document.

When the agent makes a mistake: Do not just correct the code. Go to your .*.md file and add a rule to prevent that mistake from happening ever again.

Example: If the agent keeps using an old version of a library, add a line: "ALWAYS use Syntax X, NEVER use Syntax Y."

Next Step

Would you like me to draft a specific .*.md file for your current Hair Oil Brand or Hackathon project? Tell me the tech stack, and I will write the "Expert Grade" rules for it.

</your-info>













<outside-resource>

-   [](/)

-   [Part 2: AI Tool Landscape](/docs/AI-Tool-Landscape)

-   [Chapter 5: Claude Code: Your First General Agent](/docs/AI-Tool-Landscape/claude-code-features-and-workflows)

-   CLAUDE.md Context Files



# CLAUDE.md Context Files



Imagine this: You've been working with Claude Code on your Python project for weeks. Claude has learned your naming conventions, understood your project structure, and adapted to your coding style. You close the terminal for the evening.



The next morning, you open a new Claude Code session and type a question about your project. Claude responds with generic adviceâ€”treating your project like it's starting fresh. You have to re-explain your tech stack, your directory structure, your team's conventions.



**This is context friction.** And it's a productivity killer.



Every session starts with zero context. You either repeat explanations repeatedly, or Claude gives generic answers that don't match your project's reality.



**There's a better way.**



* * *



## What Is CLAUDE.md?



**CLAUDE.md is a simple markdown file placed in your project root that Claude Code automatically loads at the start of every session.** It contains the persistent context your AI companion needsâ€”without you repeating it.



Think of it as a **persistent project brief** that travels with your code:



-   Your project does X, Y, and Z

-   You use Python 3.13 with FastAPI and PostgreSQL

-   Files go in `src/`, tests in `tests/`, database migrations in `alembic/`

-   You prefer type hints, Google-style docstrings, and error handling with custom exceptions

-   Key commands to run: `uvicorn main:app --reload`, `pytest`, `alembic upgrade head`



When Claude Code starts a new session, it reads CLAUDE.md automatically. Claude **immediately understands your project** without you saying a word.



![Four-layer context architecture showing Working Directory (base), .claude/context files (project knowledge), Message History (conversation state), and Current Tools (active capabilities), with information flow arrows](https://pub-80f166e40b854371ac7b05053b435162.r2.dev/books/ai-native-dev/static/images/part-2/chapter-05/claude-code-context-architecture-four-layers.png)



#### ðŸ’¬ AI Colearning Prompt



> "Why is having persistent context in CLAUDE.md more efficient than repeating project details in every session?"



* * *



## How CLAUDE.md Auto-Loads



Here's the mechanism (you don't need to do anythingâ€”it's automatic):



1.  **You create** `CLAUDE.md` in your project root (same directory as `package.json`, `pyproject.toml`, `go.mod`, or `.git`)

2.  **You start** Claude Code in that directory: `claude`

3.  **Claude Code detects** the CLAUDE.md file automatically

4.  **Claude reads** the file and loads it into context

5.  **Your session begins** with full project understandingâ€”no setup required



Every new session repeats this process. CLAUDE.md is always loaded, always available.



**One-time setup. Automatic benefit forever.**



#### ðŸ’¬ AI Colearning Prompt



> "Explain how Claude Code loads CLAUDE.md automatically at session start. What's the mechanism that makes this work without manual commands?"



#### ðŸŽ“ Expert Insight



> In AI-driven development, context is gold. CLAUDE.md is the cheapest way to give Claude continuous project awareness. Write it once; benefit every session. This is specification-first thinking applied to AI companionship.



* * *



## What Goes Into CLAUDE.md



CLAUDE.md typically contains 6 sections. Use this structure as your template:



1.  Project Overview: What does your project do? What problem does it solve?

2.  Technology Stack: Languages, frameworks, databases, key dependencies.

3.  Directory Structure: Show the layout so Claude understands where code lives.

4.  Coding Conventions: Style, naming, patterns your team follows.

5.  Key Commands: Common commands to run the project.

6.  Important Notes: Gotchas, dependencies, security considerations.



* * *



## How to Create Your CLAUDE.md



You could type this all manually. Orâ€”and this is the Claude Code wayâ€”**ask Claude to generate it for you.**



Here's the process:



### Step 1: Ask Claude Code to Generate CLAUDE.md



Start Claude Code in your project directory and ask:



```

claude "Help me create a CLAUDE.md file for this project.What are the main sections I should include, and can you generate a templatebased on what you see in the codebase?"

```



Claude will analyze your actual files and propose a CLAUDE.md structure based on your real project.



#### ðŸ¤ Practice Exercise



> **Ask your AI**: "Create a CLAUDE.md for my \[Python/Node/Go/etc\] \[project type\] project. Include: Project Overview (2 sentences), Technology Stack (list), Directory Structure (tree), Coding Conventions (list), Key Commands (list), Important Notes (gotchas). Make it specific to what you see in the codebase."

> 

> **Expected Outcome**: Claude generates a CLAUDE.md with all sections populated based on your actual project structure.



### Step 2: Review and Refine



Claude's output is a starting point. Read it carefully. Does it match your project? Are conventions accurate? If Claude guessed wrong or missed details, refine it.



### Step 3: Save the File



Save Claude's output as `CLAUDE.md` in your project root (same directory as `package.json`, `pyproject.toml`, etc.)



### Step 4: Verify Auto-Loading



Exit Claude Code (`exit` or close terminal). Open a new terminal session in the same directory:



```

claude

```



In the new session, ask Claude a question about your project:



```

"What's the tech stack for this project?"

```



**If Claude mentions your stack without you repeating itâ€”CLAUDE.md loaded successfully.**



* * *



## Why This Matters: Context as Productivity



Here's what you've accomplished:



-   âœ… **One-time creation**: 10-15 minutes to write CLAUDE.md

-   âœ… **Automatic benefit**: Every session starts with full context

-   âœ… **No friction**: No re-explaining project structure, conventions, or setup

-   âœ… **Team alignment**: New team members read CLAUDE.md to understand the project



This is the principle of **"specify once, benefit always"**â€”define your project context one time, and every future session starts with full understanding.



In later lessons, you'll see how subagents (Lesson 09) and skills (Lesson 06) inherit and extend this CLAUDE.md contextâ€”making it the foundation for all Claude Code intelligence.



#### ðŸ’¬ AI Colearning Prompt



> "How does having persistent context in CLAUDE.md improve the quality of Claude Code's suggestions compared to starting fresh each session?"



* * *



## Co-Learning: Improving CLAUDE.md Together



Let's see the **Three Roles Framework** in actionâ€”where you and Claude Code learn from each other to create a better CLAUDE.md.



### Stage 1: Your First Draft



You create an initial CLAUDE.md:



```

# Project OverviewA Python web app# Tech Stack- Python- FastAPI

```



It's basic, but it's a start.



### Stage 2: AI as Teacher (Claude Teaches You)



Ask Claude to review:



```

"Review my CLAUDE.md. What important sections am I missing?"

```



**Claude responds:**



> "I notice your CLAUDE.md is missing several critical sections:

> 

> 1.  **Directory Structure** - Where are your route files, models, tests?

> 2.  **Coding Conventions** - What style guide? Type hints required?

> 3.  **Key Commands** - How do I run the app? Run tests?

> 4.  **Database Info** - What database? Migration strategy?

> 5.  **API Documentation** - Where's the OpenAPI spec?

> 

> For a FastAPI project, these sections help me give better suggestions about imports, file locations, and best practices."



**What just happened?** ðŸŽ“ **AI taught you** CLAUDE.md best practices you didn't know. You learned what makes context useful.



### Stage 3: AI as Student (You Teach Claude)



Now you provide domain-specific knowledge Claude doesn't have:



```

"Good points, but our team uses a custom auth pattern. Update CLAUDE.md to reflect:- All auth goes through middleware/auth.py- We use custom decorators: @require_auth, @require_admin- No third-party auth libraries (company policy)- JWT tokens stored in httpOnly cookies"

```



**Claude responds:**



> "Got it. I'm updating the Coding Conventions section to reflect your team's auth architecture. This helps me:

> 

> -   Suggest the correct decorator imports

> -   Avoid recommending third-party auth libraries

> -   Understand your security model when reviewing code

> 

> Here's the updated CLAUDE.md..."



**What just happened?** ðŸŽ“ **You taught AI** your team's constraints and patterns. Claude now understands context that doesn't exist in its training data.



### Stage 4: AI as Co-Worker (Converging Together)



You and Claude iterate to refine:



```

"This is better, but the directory structure doesn't show where database migrations live.How should we organize database-related files?"

```



**Claude responds:**



> "Two common patterns:

> 

> 1.  **Separated**: `alembic/` at root for migrations + `app/models/` for schemas

> 2.  **Grouped**: `database/` folder containing both migrations and models

> 

> Which matches your team's style? Or is there a third pattern you prefer?"



**You respond:**



```

"Option 1â€”we separate concerns. Migrations are infrastructure, models are app code. Update it."

```



**What just happened?** ðŸŽ“ **Neither of you** had the complete picture at start. Through conversation, you **converged** on the right solutionâ€”Claude offered options, you selected based on team context.



### The Result: Context Created Through Co-Learning



Your final CLAUDE.md is now:



```

# Project OverviewA Python FastAPI web application for internal business workflows# Tech Stack- Python 3.13- FastAPI 0.104- PostgreSQL 15- SQLAlchemy 2.0# Directory Structure

```



app/ â”œâ”€â”€ models/ # SQLAlchemy models â”œâ”€â”€ routes/ # API endpoints â”œâ”€â”€ middleware/ # Custom middleware (auth) â””â”€â”€ tests/ # Pytest test files alembic/ # Database migrations



```

# Coding Conventions- Type hints required on all functions- Google-style docstrings- Custom auth: @require_auth, @require_admin decorators- All auth logic in middleware/auth.py- NO third-party auth libraries (company policy)- JWT tokens in httpOnly cookies# Key Commands- Run app: `uvicorn app.main:app --reload`- Run tests: `pytest`- Run migrations: `alembic upgrade head`- Create migration: `alembic revision --autogenerate -m "description"`# Important Notes- Database migrations MUST be reviewed before merge- All endpoints require authentication except /health

```



**This CLAUDE.md is better** because:



-   âœ… Claude taught you what sections to include

-   âœ… You taught Claude your team's specific patterns

-   âœ… You converged together on the right organization



**This is the Three Roles Framework**â€”AI collaboration that makes both you and Claude smarter.



* * *



## Edge Cases and Troubleshooting



### CLAUDE.md Not Loading?



**Symptom**: You created CLAUDE.md, but Claude Code doesn't reference it in new sessions.



**Checklist**:



-   âœ… File is named exactly `CLAUDE.md` (case-sensitive)

-   âœ… File is in project root (same level as `.git`, `package.json`, etc.)

-   âœ… You restarted Claude Code session (new terminal, not same session)

-   âœ… File has content (not empty)



**Solution**: If all above are true, restart your terminal completely. Sometimes the session needs a fresh start.



### Unclear What Goes in CLAUDE.md?



**Simple rule**: Ask yourself: *"Does Claude need to know this to give good suggestions?"* If Claude would ask "What's your tech stack?" without CLAUDE.md, then that information belongs in CLAUDE.md.



### Concerns About File Size?



A typical CLAUDE.md is 1-3 KB. Context is cheap; clarity is expensive. A well-organized CLAUDE.md saves repeated explanations every session and improves Claude's suggestions.



* * *



## The Universal Standard: AGENTS.md



You've learned how CLAUDE.md provides project context for Claude Code. But what about other AI coding agentsâ€”Cursor, GitHub Copilot, Gemini CLI, Devin, and dozens more?



**Enter AGENTS.md**â€”a universal standard that works across ALL AI coding tools.



### What is AGENTS.md?



AGENTS.md is a simple markdown file (similar to CLAUDE.md) that provides project-specific guidance to **any** AI coding agent. Created by OpenAI and now adopted by 60,000+ open source projects, it's become the industry standard for agent instructions.



**Key difference**:



-   **CLAUDE.md** â†’ Claude Code specific (rich features, detailed context)

-   **AGENTS.md** â†’ Universal standard (works everywhere)



### Why This Matters: The Agentic AI Foundation



On December 9, 2025, something significant happened. OpenAI, Anthropic, and Block donated their open standards to the Linux Foundation, creating the **Agentic AI Foundation (AAIF)**:



Project



Donated By



Purpose



**MCP**



Anthropic



Protocol for connecting AI to tools/data



**AGENTS.md**



OpenAI



Universal project instructions for agents



**Goose**



Block



Open-source agent framework



This means AGENTS.md is now a **neutral, vendor-independent standard**â€”like how Kubernetes standardized containers or how HTTP standardized the web.



### The Best of Both Worlds



Here's the practical approach: **use both**.



```

your-project/â”œâ”€â”€ CLAUDE.md      # Rich context for Claude Code (your primary tool)â”œâ”€â”€ AGENTS.md      # Universal context for any AI agentâ”œâ”€â”€ src/â””â”€â”€ ...

```



**In your CLAUDE.md**, simply reference AGENTS.md:



```

# Project ContextSee @AGENTS.md for universal project guidelines that apply to all AI agents.## Claude-Specific Instructions[Additional Claude Code specific context, skills, hooks, etc.]

```



This approach gives you:



-   âœ… **Portability**: Any AI agent understands your project via AGENTS.md

-   âœ… **Depth**: Claude Code gets rich context via CLAUDE.md

-   âœ… **No duplication**: Common info in AGENTS.md, Claude-specific in CLAUDE.md



### What Goes in AGENTS.md vs CLAUDE.md?



Content



AGENTS.md



CLAUDE.md



Project overview



âœ…



Reference @AGENTS.md



Tech stack



âœ…



Reference @AGENTS.md



Directory structure



âœ…



Reference @AGENTS.md



Coding conventions



âœ…



Reference @AGENTS.md



Key commands



âœ…



Reference @AGENTS.md



Claude-specific skills



âŒ



âœ…



MCP server configs



âŒ



âœ…



Subagent definitions



âŒ



âœ…



Hooks configuration



âŒ



âœ…



**Simple rule**: Universal project context â†’ AGENTS.md. Claude Code features â†’ CLAUDE.md.



### Creating Your AGENTS.md



Ask Claude Code to generate both files:



```

"Create an AGENTS.md file with universal project context that any AI coding agentcan understand. Then update my CLAUDE.md to reference @AGENTS.md for common contextand add Claude-specific instructions separately."

```



#### ðŸŽ“ Expert Insight



> The AAIF announcement signals a maturing industry. Instead of every AI tool having its own context format, we're converging on standards. This is good for developersâ€”write your project context once in AGENTS.md, and it works everywhere. Add Claude-specific power features in CLAUDE.md. One universal standard + one specialized configuration = maximum productivity.



#### ðŸ’¬ AI Colearning Prompt



> "Explain why having a universal standard like AGENTS.md benefits the AI development ecosystem. How does this compare to other standardization efforts like HTTP, Kubernetes, or USB?"



* * *



## Try With AI



Let's create a CLAUDE.md file that eliminates context friction and makes every session productive from the start.



**ðŸ” Explore Auto-Generation:**



> "Help me create a complete CLAUDE.md file for this project. Analyze what you can see in the codebase and generate a file with these sections: Project Overview (1-2 sentences), Technology Stack (bulleted list), Directory Structure (tree diagram), Coding Conventions (bulleted list), Key Commands (for running, testing, deploying), and Important Notes (gotchas or critical context). Base everything on the actual files you can read."



**ðŸŽ¯ Practice Collaborative Refinement:**



> "Review the CLAUDE.md you just created. I need to add my team's specific constraints: \[describe your specific conventions, patterns, or requirements\]. Update the file to reflect these details, and ask me clarifying questions if anything is ambiguous."



**ðŸ§ª Test Context Persistence:**



> "I'm going to test if CLAUDE.md auto-loads correctly. First, tell me what tech stack this project uses based on the CLAUDE.md. Then I'll exit and start a new session to verify you remember it without me repeating anything."



**ðŸš€ Optimize for Your Workflow:**



> "Now that CLAUDE.md is working, help me identify what ELSE should go in it based on my workflow: I frequently \[describe your common tasks: write tests, add API endpoints, update documentation, etc.\]. What context would help you give better suggestions for these tasks?"



### Core Concept



CLAUDE.md is a markdown file in your project root that Claude Code automatically loads at session startâ€”giving your AI companion persistent project context without you repeating explanations every time. For universal compatibility across ALL AI coding agents, pair it with AGENTS.md.



### Key Mental Models



-   **Context Friction â†’ Productivity Loss**: Every session without persistent context forces re-explanation; CLAUDE.md eliminates this friction

-   **Specify Once, Benefit Always**: One-time setup (10-15 minutes) provides automatic context loading forever

-   **Iterative Refinement**: Your first CLAUDE.md draft improves through AI review (suggests missing sections) and your domain knowledge (adds team-specific constraints)

-   **Universal + Specialized**: AGENTS.md provides universal project context (works with any AI agent), CLAUDE.md adds Claude-specific features (skills, hooks, MCP configs)



### Critical Patterns



-   Place CLAUDE.md in project root (same level as `.git`, `package.json`, `pyproject.toml`)

-   Include 6 standard sections: Project Overview, Technology Stack, Directory Structure, Coding Conventions, Key Commands, Important Notes

-   Ask Claude to generate initial CLAUDE.md: `"Help me create a CLAUDE.md file for this project based on what you see in the codebase"`

-   Verify auto-loading by starting new session and asking about your tech stackâ€”Claude should know without you telling it

-   Refine iteratively: Claude suggests missing sections â†’ you add team-specific patterns â†’ converge on complete context

-   **Use both files**: AGENTS.md for universal context (all AI agents), CLAUDE.md references `@AGENTS.md` and adds Claude-specific instructions



### The AGENTS.md Standard



-   Universal standard created by OpenAI, now under Linux Foundation's Agentic AI Foundation (AAIF)

-   Adopted by 60,000+ projects, works with Cursor, GitHub Copilot, Gemini CLI, Devin, and all major AI coding tools

-   Simple rule: Universal project context â†’ AGENTS.md, Claude-specific features â†’ CLAUDE.md

-   In CLAUDE.md, reference with: `See @AGENTS.md for universal project guidelines`



### Common Mistakes



-   File not in project root (must be same directory as `.git`, not in subdirectory)

-   Wrong filename (must be exactly `CLAUDE.md`, case-sensitive)

-   Testing in same session (auto-loading only happens when starting new Claude Code session)

-   Including task-specific details (CLAUDE.md is for project-wide context, not individual task instructions)

-   Duplicating content between AGENTS.md and CLAUDE.md (use references instead)



### Connections



-   **Builds on**: Claude Code installation and authentication (Lesson 2)

-   **Leads to**: Subagents and Skills (Lessons 5-6) inherit and extend CLAUDE.md context

-   **Related standard**: AGENTS.md (universal), MCP (tool connections)â€”all now under AAIF governance



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/AI-Tool-Landscape/claude-code-features-and-workflows/claude-md-context-files

</outside-resource>











add skills concept if need:

<skill-concept>

-   [](/)

-   [Part 2: AI Tool Landscape](/docs/AI-Tool-Landscape)

-   [Chapter 5: Claude Code: Your First General Agent](/docs/AI-Tool-Landscape/claude-code-features-and-workflows)

-   The Concept Behind Skills



# The Concept Behind Skills



"Stop building agents. Build skills instead."



That's not advice from a random blog post. It's the conclusion Anthropic reached after building Claude Code and watching how people actually extended it. The talk that introduced Agent Skills challenged a fundamental assumption about AI customization.



![lesson-4-skills-growth](https://pub-80f166e40b854371ac7b05053b435162.r2.dev/books/ai-native-dev/static/images/part-2/chapter-05/lesson-4-skills-growth.png)



**The assumption**: Making AI useful for YOUR domain requires building a specialized agent. Finance needs a finance agent. Legal needs a legal agent. Each domain demands its own infrastructure.



**The discovery**: The agent underneath is far more universal than anyone expected. Code isn't just a use caseâ€”it's the universal interface to the digital world. A coding agent can pull data from APIs, organize files, analyze with Python, synthesize reports. The scaffolding can be as thin as Bash and a file system.



The agent already exists. What's missing is your expertise in a format it can access.



* * *



## Intelligence + Code = Execution (But Not Expertise)



Here's how the Anthropic team frames what they built:



**Models provide intelligence.** Claude can reason, analyze, synthesize, and generate. That's the cognitive capabilityâ€”the "thinking" part.



**Code provides execution.** Through a terminal, Claude can call APIs to pull data, organize information in the file system, analyze with Python, and output results in any format. Code is the universal interface to the digital world.



Put them together: an intelligent agent that can execute. Claude Code isn't just a chatbotâ€”it can actually DO things. Read your files. Run your tests. Commit your code. Generate reports.



**But intelligence + execution still isn't expertise.**



Agents today have intelligence and capabilities, but not always the expertise needed for real work. They can do amazing things when you provide detailed guidance. But they're missing the important context up front. They can't absorb your organizational knowledge efficiently. They don't learn from your feedback over time.



That's the gap skills fill. Not more intelligence. Not more execution capability. **Expertise**â€”the domain-specific knowledge that makes generic capability specifically useful.



* * *



## Skills Are Universal, Not Just for Coding



Here's a critical insight: **skills work for ANY domain, not just software development.**



![skills-universal-domains](https://pub-80f166e40b854371ac7b05053b435162.r2.dev/books/ai-native-dev/static/images/part-2/chapter-05/skills-universal-domains.png)



Claude Code runs in a terminal, yes. But "code" is the universal interface to the digital world. Through code, Claude can:



-   Write and format documents (Word, PowerPoint, Excel, PDF)

-   Analyze data and create visualizations

-   Process and organize information

-   Generate reports in any format

-   Automate repetitive digital tasks



**Skills encode expertise in ANY field:**



Domain



Example Skills



**Finance**



Quarterly report formatting, audit procedures, compliance checklists



**Legal**



Contract review workflow, clause analysis, due diligence procedures



**Marketing**



Brand voice guidelines, campaign brief templates, social media style



**Education**



Lesson plan structure, assessment rubrics, student feedback formats



**Healthcare**



Clinical documentation standards, patient communication templates



**Recruiting**



Candidate evaluation criteria, interview question frameworks



The accountant who knows exactly how audits should be structured? That's a skill. The recruiter who knows what makes candidate evaluations useful? That's a skill. The legal team's contract review workflow? That's a skill.



**You don't need to be a programmer to create or use skills.** You need domain expertise and the willingness to document your procedures clearly.



* * *



## The Real Bottleneck Isn't Intelligence



Who do you want doing your taxes? The 300-IQ mathematical genius who figures out tax code from first principles? Or the experienced tax professional who knows the patterns, the edge cases, the specific procedures that work?



You want the professional. Not because they're smarterâ€”because they have encoded expertise.



This is exactly the gap with AI agents today. Claude is brilliantâ€”it can do amazing things when you invest effort in guidance. But it often lacks the context you've built up over years. It can't absorb your organizational expertise efficiently. And it doesn't learn from your feedback over time.



The solution isn't making the model smarter. The solution is giving it access to expertise it doesn't have.



The question becomes: how do you give Claude that same advantage?



* * *



## Won't Many Skills Overload Context? No.



You might worry: "If I have 50 skills, won't Claude run out of memory loading them all?"



This is exactly the problem skills are designed to solve.



**Skills are organized collections of files**â€”in simpler terms, they're folders. This simplicity is deliberate. The design principle: anything that anyoneâ€”human OR agentâ€”can create and use, as long as they have a computer.



**The folder structure:**



```

.claude/skills/â”œâ”€â”€ meeting-notes/           # Each skill is a folderâ”‚   â”œâ”€â”€ SKILL.md             # Main instructions (loaded on-demand)â”‚   â””â”€â”€ templates/           # Supporting files (loaded if needed)â”‚       â””â”€â”€ standup.mdâ”œâ”€â”€ code-review/â”‚   â”œâ”€â”€ SKILL.mdâ”‚   â””â”€â”€ checklist.mdâ””â”€â”€ blog-planner/    â””â”€â”€ SKILL.md

```



Skills work with what you already have. Version them in Git. Throw them in Google Drive. Zip them up and share with your team. Files have been a primitive for decades. Why change now?



**The three-level architecture protects context:**



Claude has limited working memory (context window). If it loaded every skill's full instructions at startup, it would run out of space before doing any actual work. Skills solve this with progressive disclosure:



**Level 1 â€” Brief Metadata (Always Loaded)**: A short description of what the skill does and when it's relevant. Just enough for Claude to know the skill exists.



**Level 2 â€” Full Instructions (On-Demand)**: When Claude decides a skill applies, it loads the complete SKILL.md with detailed procedures, workflows, examples.



**Level 3 â€” Supporting Files (If Needed)**: Scripts, reference documentation, tools in the skill's directory. Accessed only when executing the skill.



Think of your smartphone. You have 100 apps installed. Your phone doesn't run all 100 at onceâ€”it would crash. Apps stay closed until you tap them. Skills work the same way: available when needed, dormant otherwise.



![skills-standardization](https://pub-80f166e40b854371ac7b05053b435162.r2.dev/books/ai-native-dev/static/images/part-2/chapter-05/skills-standardization.png)



This architecture enables scale. You can have hundreds of skills without overwhelming context. Claude activates the relevant ones and ignores the rest.



But if skills are just folders, where do they come from?



* * *



## Three Sources of Encoded Expertise



Skills emerge from different places, each serving different needs.



**Foundational Skills**: Basic capabilities that extend what Claude can do out of the box.



Examples: Creating Word documents, PowerPoint presentations, Excel spreadsheets, PDFs. These are like basic life skillsâ€”everyone needs them.



**Partner Skills**: Help Claude work effectively with specific software or services.



Examples: Browserbase built a skill for their open-source browser automation tooling (Stagehand). Now Claude equipped with this skill can navigate the web and automate browser tasks far more effectively. Notion launched skills that help Claude understand your entire workspace and do deep research across your documents.



These are like specialized certificationsâ€”expertise in specific tools.



**Enterprise and Custom Skills**: Created by organizations for their specific needs.



Examples: Your company's coding style guide. Internal documentation standards. Organization-specific workflows that encode "how we do things here."



**Here's where the real traction is**: Anthropic has been talking to Fortune 100 companies using skills to teach agents about organizational best practices and the unique ways they use bespoke internal software. Developer productivity teamsâ€”serving thousands or tens of thousands of engineersâ€”deploy skills to teach Claude Code about internal code style standards and preferred workflows.



Within five weeks of launch, thousands of skills were created across all three categories. The ecosystem is growing fast because the format is simple enough that anyone can contribute.



What makes skills especially powerful is what they combine with.



* * *



## Skills + MCP = Expertise + Connectivity



MCP servers (covered later in this chapter) connect Claude to external data and toolsâ€”APIs, databases, file systems, web browsers.



Skills and MCP serve different functions:



Component



What It Provides



**MCP Servers**



Connection to outside world (data, tools, APIs)



**Skills**



Expertise for USING those connections effectively



**Consider a concrete example:**



An MCP server connects Claude to your company's database. Claude can now query data, retrieve records, run reports.



A skill encodes your company's data analysis proceduresâ€”what reports to generate, what format to use, what insights to highlight, what anomalies to flag.



Without the skill: Claude can access data but doesn't know your reporting standards. It produces generic output.



Without MCP: Claude knows your standards but can't access the data. Knowledge without action.



Together: Claude queries the database (MCP), analyzes using your procedures (skill), and produces reports that match your organization's expectations.



**The pattern developers are building**: Skills that orchestrate workflows across multiple MCP tools stitched together. MCP provides the connectivity. Skills provide the expertise for using that connectivity well.



They're complementary, not competing. Equip Claude with the right MCP servers AND the right library of skills, and you have an agent configured for your specific domain.



This architecture is already enabling rapid deployment. When Anthropic launched Claude for financial services and life sciences, each offering came with a set of MCP servers for domain-specific data access AND a set of skills encoding domain-specific expertise.



But this raises a question: who creates these skills?



* * *



## The Accessibility Revolution



You might still be thinking: "This sounds like a developer thing."



Here's what Anthropic observed in the first weeks after launch: skills are being built by people who aren't technical. People in finance. Recruiting. Accounting. Legal.



This is early validation of the design principle. Skills help people who don't write code extend general-purpose agents. They make AI assistants accessible for day-to-day work across functions.



**Why this works:**



Domain experts have the knowledge. A senior accountant knows exactly how audits should be structured. A recruiting lead knows what makes candidate evaluations useful. A legal team knows their contract review workflow inside and out.



What these experts lacked was a mechanism to transfer that knowledge to AI. Traditional tools required technical implementation. Skills require clear instructions in a folder.



A recruiter's candidate evaluation checklist becomes a skill. An accountant's audit procedure becomes a skill. A legal team's contract review workflow becomes a skill.



**The writing analogy:**



You don't need to be a programmer to write a great email template. You don't need to be a programmer to document your meeting note procedure. Skills extend this principle: document your expertise clearly, and Claude applies it.



The formatâ€”markdown files with YAML metadataâ€”is accessible to anyone who can write structured text. The barrier isn't technical skill. It's willingness to articulate your procedures.



If skills can be created by anyone, what happens when many people create them?



* * *



## Skills as Strategic Assets



Here's what most people miss: **skills aren't just convenientâ€”they're valuable intellectual property.**



Compare manual prompting (explaining your preferences every time) to encoded skills:



Aspect



Manual Prompting



Agent Skills



**Reliability**



Ad-hoc, best effort



Deterministic, script-backed



**Token Cost**



Pay for "rules" in every conversation



Load rules only when triggered



**Asset Type**



Disposable conversation



**Reusable, scalable IP**



**Integration**



Requires human copy-paste



**API-ready via Agent SDKs**



That last row is critical. Skills you create in Claude Code can be:



-   **Shared** with your team (everyone benefits from your expertise)

-   **Versioned** in Git (track improvements over time)

-   **Integrated** into Custom Agents (Part 6 teaches this)

-   **Monetized** as part of vertical AI solutions



When you create a skill, you're not just saving keystrokes. You're encoding expertise that compounds in valueâ€”for yourself, your team, and potentially your business.



* * *



## The Compounding Value of Shared Knowledge



The vision that excites the Anthropic team most: a collective, evolving knowledge base of capabilities curated by people AND agents inside an organization.



Skills provide the procedural knowledge for your agents to do useful things. As you interact and give feedback, skills improve. The agent gets better.



**The organizational multiplier:**



When your skills improve, ALL agents in your team and organization get better. Not just yours. Everyone using the same skill library benefits from the refinement.



**The onboarding transformation:**



When someone new joins your team and starts using Claude for the first time, it already knows what your team cares about. It knows your day-to-day workflows. It knows how to be most effective for YOUR work.



No ramp-up period. No weeks of context-sharing. The expertise is already encoded.



**Beyond your organization:**



Just as someone else building an MCP server makes your agent more useful (you gain connectivity you didn't build), someone else building a skill makes your agent more capable.



A skill built by someone in the community helps agents everywhere. Contribution compounds capability.



**The continuous learning direction:**



Skills are designed as a concrete step toward continuous learning. Here's the key insight: anything Claude writes down can be used efficiently by a future version of itself.



This makes learning transferable. As you build context with Claude over time, skills make memory tangible. They don't capture everythingâ€”just procedural knowledge Claude can use on specific tasks.



Claude can already create skills for you today using a "skill creator" skill. The goal: Claude on Day 30 of working with you is dramatically better than Claude on Day 1.



Where does this all fit in the bigger picture?



* * *



## The Stack Analogy



The Anthropic team draws a parallel to what we've already seen in computing.



![lesson-4-skills-in-stack](https://pub-80f166e40b854371ac7b05053b435162.r2.dev/books/ai-native-dev/static/images/part-2/chapter-05/lesson-4-skills-in-stack.png)



**Models are like processors.** Both require massive investment and contain immense potential. But a processor by itself isn't that useful. You need systems built around it.



**Agent runtimes are like operating systems.** The OS made processors far more valuable by orchestrating processes, resources, and data around the processor. In AI, agent runtimes (like Claude Code) play this roleâ€”the cleanest, most efficient abstractions to get the right tokens in and out of the model.



**Skills are like applications.** A few companies build processors and operating systems. But millions of developers build software that encodes domain expertise and unique points of view.



**This is the layer that opens for everyone.**



You don't need to build the model. Anthropic did that. You don't need to build the agent runtime. Claude Code exists. What you need to build: the applications layer. Skills that encode YOUR expertise, YOUR procedures, YOUR organizational knowledge.



The paradigm shift: **Stop building agents. Build skills instead.**



The agent infrastructure is mature. The extensibility mechanisms (MCP for connectivity, skills for expertise) are in place. The value creation happens in what YOU contributeâ€”the domain knowledge that makes the general-purpose agent specifically useful for your work.



* * *



## From Understanding to Action



You now understand the architecture:



-   The bottleneck isn't AI intelligenceâ€”it's access to specialized expertise

-   Skills are intentionally simple (folders) to enable universal adoption

-   Three-level loading keeps context efficient at scale

-   Skills complement MCP (expertise + connectivity)

-   Non-technical users can create high-value skills

-   Skills compound across teams and organizations

-   Skills are the "applications layer" of the agent stack



Lesson 06 teaches execution:



-   The SKILL.md format and YAML metadata structure

-   Creating a working blog-planner skill from scratch

-   Testing skill activation and refinement

-   Decision criteria: when skills vs. subagents



The procedure you mapped in Lesson 04â€”your repeated task with clear preferences and quality criteriaâ€”that's your raw material. Lesson 06 shows you how to encode it.



* * *



## Try With AI



**Understand the Architecture:**



> "Anthropic says 'code is all we need' as the universal interface for agents. Help me understand: how does a coding agent (like Claude Code) become useful for non-coding tasks like finance reports or legal research? Walk me through the architectureâ€”what role do skills play in making a coding agent general-purpose?"



**Identify Skill Opportunities in Your Domain:**



> "I work in \[your domain: marketing / finance / research / operations / legal / etc.\]. Based on the three skill sources (foundational, partner, enterprise), help me identify: (1) What foundational skills probably already exist for my work? (2) What third-party skills might exist for tools I use? (3) What custom skills would capture MY team's specific procedures?"



**Evaluate the Stack Analogy:**



> "Explain the 'skills are like applications' analogy. If Claude is the processor and Claude Code is the operating system, what makes skills different from just being good at prompting? Why is encoding expertise as a skill different from getting better at asking questions?"



**Challenge the Paradigm:**



> "The Anthropic talk claims 'stop building agents, build skills instead.' When WOULD someone still need to build a custom agent? What can't skills do? Help me understand where the boundary is between 'a skill is enough' and 'you need something more.'"



Note: These prompts explore the conceptual framework. Lesson 06 teaches hands-on skill creationâ€”the syntax, structure, and workflow for actually building your first skill.



* * *



## Watch the Original Talk



This lesson is based on the Anthropic talk "Don't Build Agents, Build Skills Instead" by Barry Zhang and Mahesh Murag. Watch the full presentation to hear these concepts directly from the team that built Claude Code and Agent Skills:



**Key timestamps:**



-   **0:21** â€” Introduction and the expertise gap

-   **1:16** â€” "Code is all we need" â€” the universal interface insight

-   **2:14** â€” The tax professional vs. mathematical genius analogy

-   **2:59** â€” What are Agent Skills? (folders!)

-   **4:20** â€” Progressive disclosure architecture

-   **5:00** â€” The skills ecosystem (foundational, partner, enterprise)

-   **9:06** â€” The complete architecture diagram

-   **12:02** â€” Sharing and distribution vision

-   **14:32** â€” The stack analogy (processors â†’ OS â†’ applications)



# Lesson Summary: The Architecture of Agent Expertise



## Key Concepts



### Intelligence + Code = Execution (But Not Expertise)



-   **Models provide intelligence**: reasoning, analysis, synthesis, generation

-   **Code provides execution**: APIs, file system, Python analysis, output generation

-   Together: an intelligent agent that can execute (Claude Code)

-   **Missing piece**: Domain expertiseâ€”the knowledge that makes generic capability specifically useful



### The Expertise Gap



-   AI agents are brilliant but lack specialized, practical knowledge

-   The bottleneck isn't intelligenceâ€”it's access to domain expertise

-   Dr. Claude analogy: knows medicine in theory, never done surgery at THIS hospital

-   Solution: give Claude access to expertise it doesn't have (via skills)



### Why Skills Are "Just Folders"



-   Intentionally simple: organized files anyone (human or agent) can create

-   Works with existing tools: Git, Google Drive, zip files

-   Three-level architecture protects context window:

    1.  **Metadata** (always loaded): description of what skill does

    2.  **Instructions** (on-demand): full SKILL.md when needed

    3.  **Supporting files** (if needed): scripts, reference docs



### Three Sources of Skills



1.  **Foundational**: Basic capabilities (document creation, PDF handling)

2.  **Partner/Third-Party**: Software-specific expertise (browser automation, Notion)

3.  **Enterprise/Custom**: Organizational knowledge (coding standards, internal workflows)



### Skills + MCP Complementarity



-   **MCP servers**: Connection to external data and tools

-   **Skills**: Expertise for USING those connections

-   Together: Claude queries data (MCP) + analyzes using your procedures (skill)

-   Complementary, not competing



### The Stack Analogy



-   Models â‰ˆ Processors (massive investment, limited alone)

-   Agent runtimes â‰ˆ Operating Systems (orchestrate resources)

-   Skills â‰ˆ Applications (where domain expertise lives)

-   Paradigm shift: "Stop building agents, build skills instead"



## Mental Models



### Dr. Claude



Brilliant doctor who graduated top of class but has never done surgery at THIS hospital. Knows theory, lacks practical expertise for specific context.



### Phone Apps



100 apps installed, but phone doesn't run all 100 at once. Apps stay closed until needed. Skills work the sameâ€”available when relevant, dormant otherwise.



### Computing Stack



You don't build processors (models) or operating systems (runtimes). You build applications (skills) that encode domain expertise.



## Key Insight



The agent infrastructure is mature. Claude Code exists. What's missing is YOUR expertise in a format Claude can access. Skills open the "applications layer" for everyoneâ€”including non-technical domain experts.



## Connection to Other Lessons



-   **Lesson 04**: WHY you'd encode personal procedures â†’ skills as individual benefit

-   **Lesson 05 (this)**: WHERE skills fit architecturally â†’ skills as platform paradigm

-   **Lesson 06**: HOW to create skills â†’ SKILL.md syntax and hands-on creation



## Common Misconceptions



| Misconception | Reality | |---------------|---------| | "I need to build a custom agent" | The agent exists; you teach it via skills | | "Skills are just for developers" | Non-technical users (finance, legal, recruiting) create high-value skills | | "Skills compete with MCP servers" | They're complementary: connectivity + expertise | | "Skills are fancy saved prompts" | Skills encode reasoning patterns, not commands |



## Questions to Test Understanding



1.  What do models provide vs. what does code provide? What's still missing?

2.  Why is "expertise, not intelligence" the real bottleneck with AI agents?

3.  How does the three-level loading architecture enable scale?

4.  What's the difference between what MCP provides and what skills provide?

5.  Why does the "skills are applications" analogy matter for adoption?

6.  How do skills compound value across an organization?



## Source Material



This lesson is based on the Anthropic talk "Don't Build Agents, Build Skills Instead" by Barry Zhang and Mahesh Murag.



**Watch**: https://www.youtube.com/watch?v=CEvIs9y1uog



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/AI-Tool-Landscape/claude-code-features-and-workflows/concept-behind-skills

</skill-concept>









add this too if need "building your own skills":

<build-own-skills>

-   [](/)

-   [Part 2: AI Tool Landscape](/docs/AI-Tool-Landscape)

-   [Chapter 5: Claude Code: Your First General Agent](/docs/AI-Tool-Landscape/claude-code-features-and-workflows)

-   Building Your Own Skills



# Building Your Own Skills



You've experienced skills in action. You've mapped a procedure worth encoding. Now you build.



![skills-strategic-value](https://pub-80f166e40b854371ac7b05053b435162.r2.dev/books/ai-native-dev/static/images/part-2/chapter-05/skills-strategic-value.png)



This lesson takes you from understanding skills to creating them. By the end, you'll have a working skill in your `.claude/skills/` folderâ€”and know how to use Claude itself to create more.



Markdown Refresher



Skills are written in Markdown with YAML frontmatter. If you're not comfortable with headings, bullet points, code blocks, or YAML syntax, take 15 minutes to review **Chapter 10: Markdown for AI-Native Development** before continuing. You'll write better skills with that foundation.



* * *



## The SKILL.md File: Anatomy of Encoded Expertise



Every skill lives in a folder. The folder contains one required file: `SKILL.md`.



```

.claude/skills/meeting-notes/â””â”€â”€ SKILL.md    â† This is what you create

```



That's it. A skill can be a single markdown file. The simplicity is intentionalâ€”anyone can create one.



### The Two Parts of SKILL.md



**Part 1: YAML Frontmatter (The ID Card)**



```

---name: "meeting-notes"description: "Transform meeting transcripts or raw notes into structured summaries with action items, decisions, and follow-ups. Use when user shares meeting content or asks for meeting notes."version: "1.0.0"---

```



**Part 2: Markdown Body (The Instructions)**



```

# Meeting Notes Skill## When to Use- User shares a meeting transcript- User asks to summarize meeting notes- User mentions "action items" or "meeting summary"## Procedure1. Extract action items with owners and deadlines2. Highlight decisions made (with who made them)3. Summarize discussion points (don't transcribe verbatim)4. Flag open questions for follow-up5. Keep to one page maximum## Output Format**Action Items** (top of document)- [ ] Task â€” Owner â€” Deadline**Decisions Made**- Decision: [what] â€” Made by: [who]**Discussion Summary**Brief bullets, not transcription.**Open Questions**- Question needing follow-up

```



That's a complete skill. No scripts required. No complex setup. Just clear instructions in a format Claude can read.



* * *



## The Description Field: Your Skill's Activation Trigger



The `description` in YAML frontmatter is the most important line you write. It determines **when Claude activates your skill**.



Claude sees skill descriptions at startup (Level 1 of the three-level architecture). When you ask for help, Claude scans these descriptions to decide which skills apply.



### Good vs. Bad Descriptions



**Bad description** (too vague):



```

description: "Helps with notes"

```



Problem: "notes" could mean anything. Claude won't know when to activate.



**Bad description** (too narrow):



```

description: "Summarizes Zoom meeting transcripts from the marketing team"

```



Problem: Won't activate for Teams calls, non-marketing meetings, or live notes.



**Good description** (clear context + action):



```

description: "Transform meeting transcripts or raw notes into structured summaries with action items, decisions, and follow-ups. Use when user shares meeting content or asks for meeting notes."

```



Why it works:



-   States WHAT it does (transform â†’ structured summaries)

-   Lists KEY OUTPUTS (action items, decisions, follow-ups)

-   Specifies WHEN to use (meeting content, meeting notes request)



### The Description Formula



```

[Action verb] + [input type] + [into/for] + [output type] + [key features].Use when [trigger conditions].

```



Examples:



```

# Blog planningdescription: "Plan blog posts with topic research, outline creation, headline variations, and introduction drafts. Use when user asks to plan, outline, or write blog content."# Code reviewdescription: "Perform systematic code reviews checking security, performance, maintainability, and best practices. Use when user asks to review code or check for issues."# Email draftingdescription: "Draft professional emails matching specified tone and purpose. Use when user needs to write emails or requests communication help."

```



* * *



## Hands-On: Create Your First Skill



You mapped a procedure in Lesson 04. Now encode it.



### Step 1: Create the Folder



```

mkdir -p .claude/skills/blog-planner

```



### Step 2: Create SKILL.md



Create `.claude/skills/blog-planner/SKILL.md`:



```

---name: "blog-planner"description: "Plan engaging blog posts with topic research, structured outlines, headline variations, and compelling introductions. Use when user asks to plan, outline, or write blog content."version: "1.0.0"---# Blog Planning Skill## When to Use This Skill- User asks to "plan a blog post" or "write an article"- User mentions blog topics, headlines, or content strategy- User needs help structuring written content## Procedure1. **Understand the topic**: Clarify subject and target audience2. **Create outline**: Structure into 3-5 main sections3. **Generate headlines**: Provide 5 variations (curiosity-driven, benefit-focused, direct)4. **Draft introduction**: Write a hook that challenges assumptions or poses a question## Output Format**Topic Summary**: 2-3 sentence overview**Target Audience**: Who should read this?**Outline**: Numbered list of main sections with brief descriptions**Headline Options**: 5 variations**Introduction Draft**: 1-2 paragraph hook## Quality Criteria- Headlines: Curiosity-driven, never clickbait- Introductions: Challenge assumptions or pose unexpected questions- Outlines: Problem â†’ failed solutions â†’ insight â†’ application structure- Specificity: Use numbers over vague claims ("30% improvement" not "significant gains")## Example**Input**: "Help me plan a blog post about sustainable living"**Output**:- **Topic Summary**: Practical sustainable living tips for busy professionals who want impact without lifestyle overhaul.- **Target Audience**: Working adults seeking eco-friendly changes that fit their schedule.- **Outline**:  1. The myth of the perfect eco-warrior  2. 5 swaps that take 10 minutes or less  3. The compound effect of small changes  4. What NOT to waste time on  5. Your 30-day sustainable starter plan- **Headlines**:  1. "You Don't Need to Be Perfect to Live Sustainably"  2. "5 Sustainable Swaps That Take Less Time Than Your Coffee Break"  3. "Why Most Sustainability Advice Is Wrong (And What Works Instead)"  4. "The Lazy Person's Guide to Environmental Impact"  5. "Sustainable Living for People Who Don't Have Time for Sustainable Living"- **Introduction**: "You've seen the Instagram influencers with their zero-waste pantries and composting systems. You've felt guilty scrolling past. Here's what they don't show: most of those lifestyles require hours of maintenance. The good news? You can cut your environmental impact by 40% with changes that take less time than your morning scroll. No mason jars required."

```



### Step 3: Test Your Skill



Start Claude Code in your project:



```

claude

```



Then ask:



```

Help me plan a blog post about learning AI tools as a beginner

```



Watch for:



-   Does Claude follow the procedure?

-   Does output match the format?

-   Does the quality match your criteria?



### Step 4: Verify Activation



Ask Claude directly:



```

What skills are you using in our conversation? Did you activate the blog-planner skill?

```



This confirms the skill loaded and helps you understand the activation mechanism.



* * *



## Refining Skills Through Co-Learning



Your first version won't be perfect. That's expected. Use Claude to improve it.



### The Co-Learning Cycle



**AI as Teacher**: Claude suggests improvements you didn't think of.



```

Review my blog-planner skill. What could be improved?Suggest 2-3 specific enhancements.

```



Claude might suggest:



-   Add SEO considerations to the outline section

-   Include word count targets for each section

-   Add a "common mistakes to avoid" section



**You as Teacher**: You specify constraints Claude doesn't know.



```

Good suggestions, but I have constraints:- Headlines must be curiosity-driven, NEVER clickbait- I prefer problem â†’ insight â†’ application structure- Keep introductions under 100 wordsUpdate the skill with these constraints.

```



**Convergence**: Together you refine until the skill matches your actual workflow.



### Iteration Prompts



After using the skill a few times:



```

I've used the blog-planner skill 3 times now. Here's what worked and what didn't:- Worked: Headline variations are great- Didn't work: Outlines are too generic, need more specific section descriptionsHelp me update the skill to fix the outline issue.

```



```

Compare the blog-planner output to how I actually wrote my last blog post.What's different? Should we update the skill to match my real style?

```



* * *



## Skills vs. Subagents: Decision Criteria



When do you create a skill vs. a subagent?



Factor



Choose Skill



Choose Subagent



**Invocation**



Automatic (Claude decides)



Explicit (you invoke)



**Context**



Shared with main conversation



Isolated context window



**Complexity**



Lightweight, single-focus



Multi-step, complex workflows



**Guarantee**



Soft activation (might not trigger)



Hard invocation (always runs)



**Best for**



Repeated patterns, formatting, procedures



Audits, refactoring, comprehensive analysis



**Use skill when**: "I want Claude to automatically do this whenever it's relevant."



**Use subagent when**: "I need guaranteed execution with isolated context for this complex task."



### Examples



**Skill appropriate**:



-   Meeting notes formatting (happens often, simple procedure)

-   Blog post planning (repeated task, consistent structure)

-   Code comment style (automatic enforcement)



**Subagent appropriate**:



-   Comprehensive security audit (complex, needs isolation)

-   Multi-file refactoring (guaranteed execution required)

-   Full codebase analysis (too large for skill context)



* * *



## Skill Design Principles



### Keep It Concise



Claude is intelligent. Don't over-explain.



**Too verbose**:



```

## How to Create an OutlineAn outline is a structured plan for your content. It helps organize thoughts.To create an outline, first think about your main points. Then arrange themin logical order. Each point should support your thesis. Consider youraudience when deciding how much detail to include...

```



**Just right**:



```

## Outline Creation- 3-5 main sections- Problem â†’ insight â†’ application structure- Each section: clear topic + brief description

```



### Show, Don't Just Tell



Include examples. Claude learns better from concrete demonstrations.



```

## Headline Format**Formula**: [Unexpected element] + [Benefit/Promise]**Examples**:- "Why Most Productivity Advice Makes You Less Productive"- "The Lazy Developer's Guide to Clean Code"- "Stop Trying to Be Consistent (Do This Instead)"

```



### Specify Quality Criteria



Make your standards explicit:



```

## Quality Standards- Paragraphs: 3 sentences max- Headlines: Curiosity-driven, never clickbait- Numbers: Specific ("37%") over vague ("significant")- Tone: Conversational, not academic

```



* * *



## Adding Supporting Files (Optional)



For complex skills, add scripts or references in subdirectories:



```

.claude/skills/pdf-extractor/â”œâ”€â”€ SKILL.md                    # Main instructionsâ”œâ”€â”€ scripts/â”‚   â””â”€â”€ extract_tables.py       # Python extraction toolâ””â”€â”€ references/    â””â”€â”€ pdf-standards.md        # Format specifications

```



**Level 3 loading**: Claude only accesses supporting files when executing the skillâ€”they don't consume context until needed.



For most skills, SKILL.md alone is sufficient. Add supporting files only when your skill needs:



-   Executable scripts for deterministic tasks

-   Reference documentation for complex domains

-   Templates or assets for output generation



* * *



## Building Skills with the Skill-Creator



Here's where it gets powerful: use Claude to create skills.



Remember the Skills Lab you downloaded in Lesson 04? It includes a `skill-creator` skill. This is a meta-skillâ€”a skill for creating other skills.



If you haven't downloaded it yet:



1.  Go to [github.com/panaversity/claude-code-skills-lab](https://github.com/panaversity/claude-code-skills-lab)

2.  Click the green **Code** button â†’ **Download ZIP**

3.  Extract and open the folder in your terminal



### Using Skill-Creator



From the skills lab directory:



```

cd claude-code-skills-labclaude

```



Then:



```

I want to create a skill for [your procedure]. Use the skill-creator to help me build it.

```



For example:



```

I want to create a skill for writing technical documentation.My procedure:1. Start with a one-sentence summary2. List prerequisites3. Provide step-by-step instructions with code examples4. End with troubleshooting section5. Keep jargon minimal, explain terms on first useUse the skill-creator to build this into a proper skill.

```



### What Skill-Creator Does



The skill-creator guides you through:



1.  **Understanding your skill**: Asks clarifying questions about your procedure

2.  **Planning structure**: Determines what goes in SKILL.md vs. supporting files

3.  **Writing the description**: Crafts activation triggers that work

4.  **Creating the file**: Generates complete SKILL.md with proper format

5.  **Testing guidance**: Suggests prompts to verify the skill works



### The Meta-Skill Advantage



Using skill-creator, you can build skills for:



-   Your meeting note format

-   Your code review checklist

-   Your email writing style

-   Your research summary template

-   Any repeated procedure you identified in Lesson 04



Each skill you create makes Claude more useful for YOUR work. And you can share these skills with your teamâ€”everyone benefits from encoded expertise.



* * *



## Your Skill Creation Workflow



Here's the process to follow for each new skill:



### 1\. Identify the Procedure



-   What task do you repeat?

-   What makes your approach distinctive?

-   What would take 10+ minutes to explain to someone new?



### 2\. Draft the Skill



-   Write SKILL.md with YAML frontmatter

-   Include clear "when to use" triggers

-   Document your procedure step-by-step

-   Add an example input/output



### 3\. Test and Iterate



-   Try the skill with real tasks

-   Note what works and what doesn't

-   Ask Claude for improvement suggestions

-   Refine based on actual usage



### 4\. Share (Optional)



-   Skills are portableâ€”same format across Claude.ai, Claude Code, API

-   Share with teammates by sharing the folder

-   Consider contributing to open skill repositories



* * *



## Try With AI



**Create Your Custom Skill:**



> "I have a procedure I want to encode as a skill: \[describe your procedure from Lesson 04\]. Walk me through creating a SKILL.md file. Help me write: (1) an effective description that triggers at the right times, (2) clear instructions Claude can follow, (3) quality criteria that match my standards, (4) an example input/output."



**Use Skill-Creator:**



> "Use the skill-creator to help me build a skill for \[your domain: technical writing / project planning / code review / research summaries\]. Start by asking me questions about my procedure, then generate the complete SKILL.md file."



**Refine an Existing Skill:**



> "Here's my current \[skill-name\] skill: \[paste SKILL.md content\]. I've used it 3 times and noticed: \[what worked\], \[what didn't work\]. Help me improve the skill to fix these issues."



**Design a Skill Suite:**



> "I work on \[describe your project type\]. Help me identify 3-5 skills I should create that would work together. For each skill, suggest: what it does, when it activates, and how it complements the others."



* * *



## Skills Beyond Software: Universal Applicability



Remember: skills aren't just for coding. They work for ANY domain where you have expertise worth encoding.



**Examples from non-technical fields:**



Role



Skill Example



What It Encodes



**Financial Analyst**



`quarterly-report`



Company's specific KPI formatting, variance analysis approach



**HR Manager**



`candidate-evaluation`



Interview rubric, scoring criteria, red flag checklist



**Marketing Lead**



`campaign-brief`



Brand voice, audience targeting framework, channel strategy



**Legal Counsel**



`contract-review`



Clause analysis workflow, risk flagging criteria



**Educator**



`lesson-plan`



Learning objectives structure, assessment alignment



The pattern is universal: **if you repeat a procedure and it has your expertise embedded in it, it's a skill candidate.**



* * *



## The Bigger Picture: Skills as Building Blocks



Here's what makes skills strategically valuable:



**Today**: Skills make Claude more useful for your personal work.



**Tomorrow**: Skills you create become building blocks for larger systems.



In Part 6 of this book, you'll learn to build **Custom Agents** using SDKs (OpenAI, Claude, Google). Here's the insight: **skills you create now can be integrated into those Custom Agents.**



```

Your Expertise â†’ Skills â†’ Custom Agent â†’ Deployed Product

```



A skill for "quarterly financial analysis" doesn't just help you today. It becomes a component in a financial analysis agent you could deploy for clients. A skill for "candidate evaluation" becomes part of a recruiting automation system.



**Skills are reusable intellectual property.** They compound in valueâ€”first for your personal productivity, then for your team, and potentially for products you build or sell.



* * *



## Reflection



You've gone from understanding skills (Lesson 04) to understanding their architecture (Lesson 05) to building them yourself.



The skills you create now will compound. Each one makes Claude more useful for YOUR specific work. Share them with teammates, and the whole team benefits from encoded expertise. And as you'll see in Part 6, these same skills become building blocks for Custom Agents you can deploy.



The next lessons explore features that complement skills: project context files, external connectivity, and complex task handling. Skills are just the beginning.



# Lesson 6 Summary: Building Your Own Skills



## Core Concept



Skills are folders with instructions that teach Claude how to execute specific tasks your way. They activate automatically based on contextâ€”you describe what you want, and Claude loads the right skill without explicit invocation.



## SKILL.md Structure



Every skill needs one file: `SKILL.md`



```yaml

---

name: "skill-name"

description: "What it does + when to use it"

version: "1.0.0"

---



# Skill Name



## When to Use

- Trigger conditions



## Procedure

1. Step one

2. Step two



## Output Format

- Expected structure



## Example

Input â†’ Output demonstration



```



## The Description Formula



The most critical line. Determines when Claude activates your skill.



```

[Action verb] + [input type] + [into/for] + [output type] + [key features].

Use when [trigger conditions].



```



**Example**:



```yaml

description: "Transform meeting transcripts into structured summaries with action items and decisions. Use when user shares meeting content."



```



## Three-Level Loading



1.  **Level 1 (Always)**: Brief descriptionsâ€”Claude knows skills exist

2.  **Level 2 (On-demand)**: Full SKILL.mdâ€”loaded when triggered

3.  **Level 3 (If needed)**: Scripts, referencesâ€”accessed during execution



## Skills vs. Subagents



| Factor | Skill | Subagent | |--------|-------|----------| | Invocation | Automatic | Explicit | | Context | Shared | Isolated | | Best for | Repeated patterns | Complex workflows | | Guarantee | Soft (might not trigger) | Hard (always runs) |



## Co-Learning Refinement



1.  **AI as Teacher**: Ask Claude to review and suggest improvements

2.  **You as Teacher**: Specify constraints Claude doesn't know

3.  **Convergence**: Iterate until skill matches your workflow



```

Review my [skill-name] skill. What could be improved?



```

```

Good suggestions, but I have constraints: [your requirements].

Update the skill with these constraints.



```



## Skill Design Principles



1.  **Be concise** â€” Claude is smart, don't over-explain

2.  **Show examples** â€” Concrete demonstrations beat abstract rules

3.  **Specify quality criteria** â€” Make your standards explicit

4.  **Include "when to use"** â€” Clear activation triggers



## Skill-Creator Meta-Skill



Use Claude to create skills:



```bash

cd claude-code-skills-lab

claude



```

```

Use the skill-creator to help me build a skill for [your procedure].



```



The skill-creator guides you through:



-   Clarifying questions

-   Structure planning

-   Description writing

-   SKILL.md generation

-   Testing guidance



## Your Workflow



1.  **Identify** â€” What procedure do you repeat?

2.  **Draft** â€” Write SKILL.md with frontmatter + instructions

3.  **Test** â€” Try with real tasks

4.  **Iterate** â€” Use co-learning to refine

5.  **Share** â€” Skills are portable across Claude surfaces



## Common Mistakes



-   **Vague descriptions** â€” "Helps with notes" won't trigger; be specific

-   **Too narrow** â€” "Summarizes Zoom calls from marketing" misses edge cases

-   **Over-explaining** â€” Claude is smart; trust it

-   **No examples** â€” Concrete input/output pairs teach better than rules



## Skills Lab Reference



1.  Go to [github.com/panaversity/claude-code-skills-lab](https://github.com/panaversity/claude-code-skills-lab)

2.  Click green **Code** button â†’ **Download ZIP**

3.  Extract and open folder in terminal



**8 ready-to-use skills**: docx, pptx, xlsx, pdf, doc-coauthoring, internal-comms, theme-factory, skill-creator



## Connections



-   **Lesson 04**: WHY skills exist (conceptual foundation)

-   **Lesson 05**: WHERE skills fit (platform architecture)

-   **Lesson 06 (this)**: HOW to build skills (hands-on creation)

-   **Lesson 07**: CLAUDE.md for project context

-   **Lesson 08**: MCP for external connectivity



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/AI-Tool-Landscape/claude-code-features-and-workflows/agent-skills

</build-own-skills>











also adding "The Skill Factory Pattern" just if need:

<skill-factory-pattern>

-   [](/)

-   [Part 2: AI Tool Landscape](/docs/AI-Tool-Landscape)

-   [Chapter 5: Claude Code: Your First General Agent](/docs/AI-Tool-Landscape/claude-code-features-and-workflows)

-   The Skill Factory Pattern



# The Skill Factory Pattern



You've just spent the last lesson building skills manuallyâ€”writing SKILL.md files, testing them, iterating based on results. That's valuable. You understand the mechanics.



But here's the friction point: **What if you need to create ten skills for a complex project? Twenty skills for an enterprise system? Do you manually write twenty SKILL.md files?**



This is where the **Skill Factory** concept changes everything.



Imagine you're a chef who teaches other chefs. You *could* explain your cooking technique to each apprentice individually. Or you could write down your teaching systemâ€”then use that system to train hundreds of apprentices consistently, at scale, with quality guaranteed.



That's the Skill Factory pattern.



In this lesson, you'll learn:



1.  What a Skill Factory is (and why it matters)

2.  The `skill-creator` meta-skill that generates other skills

3.  When to use factory approaches vs manual crafting

4.  How to build integrated skill systems (hands-on: 3-skill study system)

5.  The ultimate pattern: skill-maker (a skill that creates meta-skills)



* * *



## What Is a Skill Factory?



A **Skill Factory** is a meta-skillâ€”a skill that creates other skills.



### The Pattern



Manual Approach



Factory Approach



You write SKILL.md by hand



skill-creator generates SKILL.md



Consistency depends on your care



Factory ensures consistency



Fast for one skill



Fast for many skills



Hard to reproduce your decisions



Easy to reuse your patterns



~30 minutes per skill



~10 minutes per skill (estimated 3x faster)\*



### Why This Matters



In Lesson 06, you learned to create individual skills. But notice the pattern: **every skill you create follows the same structure:**



1.  YAML frontmatter (name, description, version)

2.  "When to Use" section (activation triggers)

3.  Procedure section (step-by-step instructions)

4.  Output Format section (expected results)

5.  Quality Criteria section (standards)

6.  Example Input/Output



If you could encode this pattern once, then use it to generate skills repeatedly, you'd:



-   Create skills significantly faster (users report ~3x improvement)

-   Ensure consistency across your skill ecosystem

-   Share best practices with your team

-   Make skill creation teachable to others



That's the Skill Factory.



* * *



## Introducing skill-creator: The Meta-Skill



The **skill-creator** is a pre-built skill available in the Claude Code Skills Lab. It's a meta-skill designed to do exactly one thing: guide you through creating new skills efficiently.



### How It Works



When you invoke skill-creator, it:



1.  **Asks clarifying questions** about your desired skill

    

    -   What problem does it solve?

    -   When should it activate?

    -   What makes it distinctive?

2.  **Plans the structure**

    

    -   Determines what goes in SKILL.md vs supporting files

    -   Identifies quality criteria

    -   Defines activation triggers

3.  **Generates the complete SKILL.md**

    

    -   YAML frontmatter (name, description, version)

    -   All required sections with your specifications

    -   Example input/output matching your domain

4.  **Suggests test prompts**

    

    -   How to verify the skill works

    -   What to watch for in Claude's response



### The Meta-Skill Advantage



Think of it like this:



```

Without skill-creator:You â†’ Write SKILL.md (30 min) â†’ Test â†’ Iterate â†’ SkillWith skill-creator:You â†’ Describe your procedure (5 min) â†’ skill-creator â†’ SKILL.md â†’ Skill

```



The skill-creator encodes best practices for SKILL.md creation. It knows:



-   What makes descriptions activate properly

-   How to structure procedures for maximum clarity

-   What examples Claude learns from best

-   Quality criteria that actually matter



* * *



## Factory vs Manual: Decision Criteria



When should you use skill-creator vs writing SKILL.md by hand?



Situation



Approach



Reason



**First skill ever**



Manual



Learn the mechanics



**Simple, single-purpose skill**



Manual



Faster to write than explain



**Complex skill with subtleties**



Manual



Your expertise needs direct expression



**Creating 2+ similar skills**



Factory



Consistency and speed pay off



**Building a skill suite**



Factory



Ensures coherence across skills



**Need to iterate quickly**



Factory



Factory includes test guidance



**Teaching others to create skills**



Factory



System is more teachable



**Skill has supporting files/scripts**



Factory



Factory handles complex structures



**Uncertain about structure**



Factory



Ask clarifying questions



### Decision Framework



```

IF you're creating ONE specific skill you know well  â†’ Write SKILL.md manually (faster)IF you're creating 2+ skills in a domain  â†’ Use skill-creator (consistency + speed)IF you're uncertain about structure or approach  â†’ Use skill-creator (guided questions)IF you're building a system of interdependent skills  â†’ Use skill-creator + orchestration (this lesson's focus)

```



* * *



## The skill-creator Meta-Skill



Located in `.claude/skills/skill-creator/SKILL.md`, the skill-creator provides guided creation. Here's what it includes:



### Part 1: Initial Questions



The skill-creator asks:



```

1. What problem does your skill solve?2. When should Claude activate it?3. What procedure does it encode?4. What makes YOUR approach distinctive?5. What should the output look like?

```



Your answers shape the generated skill.



### Part 2: Structure Planning



The skill-creator decides:



```

- Will this be simple (SKILL.md only) or complex (with supporting files)?- What goes in YAML frontmatter?- How many sections does the procedure need?- What examples best demonstrate the skill?

```



### Part 3: SKILL.md Generation



The skill-creator generates the complete file with:



-   Your problem-specific activation trigger

-   Clear step-by-step procedure

-   Output format tailored to your domain

-   Quality criteria matching your standards

-   Example input/output demonstrating success



### Part 4: Test Guidance



The skill-creator suggests:



```

Test prompt 1: [basic use case]Test prompt 2: [edge case]Test prompt 3: [integration with other skills]

```



* * *



## Hands-On: Create a 3-Skill Study System



Now let's build something real: a **study system** with three coordinated skills that work together.



```

ðŸ“– Study Notes Skill    â†“ðŸŽ´ Flashcard Skill    â†“ðŸ“ Quiz Skill

```



The flow: Student creates study notes â†’ System generates flashcards â†’ System creates quiz for self-assessment.



### Setup



First, ensure you have access to skill-creator. If you haven't downloaded the Skills Lab:



1.  Visit [github.com/panaversity/claude-code-skills-lab](https://github.com/panaversity/claude-code-skills-lab)

2.  Click **Code â†’ Download ZIP**

3.  Extract to your projects folder

4.  Open in Claude Code: `claude` from the extracted directory



### Step 1: Create study-notes Skill



In your Claude Code session with the Skills Lab open:



```

I want to create a skill that transforms study topics intostructured study notes. The procedure should:1. Accept a topic or concept2. Break it into 5-7 key sections3. For each section: definition, key points (3-4 bullets),   practical examples4. Add a summary that connects all sections5. Flag critical concepts for flashcard creationUse skill-creator to generate this skill. Call it "study-notes".

```



**What you're learning**: How skill-creator asks clarifying questions, and how your answers shape the generated SKILL.md.



The output will be a complete SKILL.md file with:



-   Activation trigger: "when user asks to study X" or "help me learn"

-   Procedure: 5 clearly numbered steps

-   Output Format: Study notes template

-   Quality Criteria: Specificity, connection between sections, flashcard readiness

-   Example: Input "Photosynthesis" â†’ Output complete study notes



### Step 2: Create flashcards Skill



Building on the previous skill:



```

Create a skill that transforms study notes into flashcards.The procedure should:1. Analyze the study notes for key concepts2. Generate 5 flashcards total:   - 2 Easy (definition-level)   - 2 Medium (application-level)   - 1 Hard (synthesis-level)3. For each flashcard: front (question) + back (answer)4. Format for spaced repetition tools5. Flag which concepts appear in multiple cards (critical knowledge)Use skill-creator. Call it "flashcards".

```



**What you're learning**: How skill-creator handles interdependencies (this skill depends on study-notes output), and how to ensure outputs from one skill feed into the next.



### Step 3: Create quiz Skill



Completing the system:



```

Create a skill that generates assessment quizzes from study notes.The procedure should:1. Extract key concepts from study notes2. Generate 5 quiz questions:   - 2 Multiple choice (concepts)   - 2 Short answer (application)   - 1 Essay (synthesis)3. Provide answer key with explanations4. Suggest which answers indicate gaps in understanding5. Recommend which study-notes sections to review based on performanceUse skill-creator. Call it "quiz".

```



**What you're learning**: How to create assessment skills that close the feedback loop in a learning system.



### Step 4: Test the Integrated System



Now test all three together:



```

I have three skills: study-notes, flashcards, and quiz.Test the full flow:1. Use study-notes to create notes for "Machine Learning Fundamentals"2. Use the output to create flashcards3. Use the flashcard output to create a quiz4. Show me how the three skills chain togetherReport on:- Did the output from skill 1 work as input for skill 2?- Did skill 3 properly consume skill 2's output?- What integration points were smooth? Rough?

```



**What you're learning**: How to verify skill systems actually work together, and identify where integration fails.



### Integration Observations



You'll notice:



-   **Smooth points**: Flashcards format works perfectly for quiz generation

-   **Rough points**: Study notes might reference topics not captured in flashcards

-   **Opportunities**: Quiz could suggest which flashcards to review



This is normal. Real systems need iteration.



* * *



## The Ultimate Pattern: skill-maker



We've built a factory (skill-creator) that creates skills.



Now imagine: **A skill that creates skill-creators.**



Enter **skill-maker**â€”the ultimate meta-skill.



### What Is skill-maker?



```

Typical skill:     Solves a problemskill-creator:    Creates skills that solve problemsskill-maker:      Creates skill-creators that create skills

```



### The Recursive Pattern



Here's where it gets interesting:



```

You â†’ skill-maker â†’ skill-creator-for-domain-X      â†’ skill-creator-for-domain-Y â†’ Generates skills for Y

```



Instead of having ONE skill-creator that's generic, skill-maker could generate **specialized skill-creators** for specific domains:



-   **study-system-creator**: Builds all the skills needed for a study system

-   **blog-creator**: Builds all skills for blog writing and publishing

-   **code-review-creator**: Builds all skills for code review across languages

-   **meeting-system-creator**: Builds all skills for meeting management



### How skill-maker Would Work



```

Prompt: Create a specialized skill-creator for buildingcustomer support systems.skill-maker:1. Analyzes customer support domain2. Identifies needed skills:   - support-ticket-classifier   - response-generator   - escalation-detector   - satisfaction-tracker3. Generates skill-creator specialized for customer support4. That creator can now generate all 4 skillsResult: domain-specific skill factory

```



### Why This Matters



Level



What It Does



Estimated Time



Manual



Create one SKILL.md



~30 minutes



Factory



Create many skills



~10 minutes each



Meta-Factory



Create specialized factories



~60 minutes (then ~10 min per skill)



*Time estimates based on user experience; actual duration varies by skill complexity and familiarity.*



For large systems (20+ skills), the meta-factory approach:



-   Ensures consistency across the domain

-   Encodes domain-specific best practices

-   Makes skill creation teachable to the whole team



This is the power of recursive abstraction.



* * *



## Designing Skill Systems



When you have multiple skills working together, design matters.



### System Design Pattern



```

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚        User Request (Large Goal)        â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚                     â”‚    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”‚ Skill1 â”‚          â”‚ Skill2  â”‚    â”‚ (Parse)â”‚          â”‚(Analyze)â”‚    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â”‚                    â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚              â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”              â”‚ Skill3  â”‚              â”‚(Combine)â”‚              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                   â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚   User Gets Result  â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```



### Design Questions



**Before building a skill system, ask:**



1.  **What's the user goal?** (the big thing they want to accomplish)

2.  **What substeps exist?** (can be decomposed into smaller skills?)

3.  **What's the natural order?** (skill A must run before skill B?)

4.  **What integration points exist?** (where does output from one skill become input to another?)

5.  **What falls back gracefully?** (if one skill fails, can others continue?)



### Example: Blog Publishing System



```

Goal: User writes and publishes a blog postSubsteps:1. blog-planner: Plan the post (outline, headlines)2. content-generator: Write sections based on plan3. seo-optimizer: Check for SEO compliance4. formatter: Convert to blog markdown with frontmatter5. publisher: Push to blog platformIntegration:- Planner output â†’ Generator input- Generator output â†’ SEO input- SEO output â†’ Formatter input- Formatter output â†’ Publisher inputEach skill activates in sequence, with clear handoffs.

```



### Building Composable Skills



When designing skills for a system:



```

# Good: Single responsibility- name: "seo-optimizer"  does: "Check SEO compliance only"  input: "Draft content"  output: "SEO suggestions"# Bad: Too broad- name: "content-processor"  does: "Plan, write, optimize, format, and publish everything"  (This isn't a skill, it's a workflow)

```



**Key principle**: Each skill does ONE thing well. Systems orchestrate skills.



* * *



## When to Use What



### Use Manual Skill Creation When:



-   You're creating your first skill (learn the mechanics)

-   The skill is simple and you know it well

-   You need fine-grained control over every detail

-   The skill is domain-specific with unique requirements



**Example**: Your company's specific meeting note format (you have precise requirements)



### Use skill-creator When:



-   You're creating multiple skills

-   You want guided questions to clarify scope

-   You're uncertain about structure

-   You want integrated test guidance

-   You're building a skill system with interdependencies



**Example**: Creating study-notes, flashcards, quiz (this lesson's system)



### Use skill-maker (Future Concept) When:



-   You're building 10+ skills in a domain

-   Your team needs to create skills with consistency

-   You want to encode domain-specific patterns

-   You're building products with reusable skill components



**Example**: Enterprise customer support system with 15+ specialized skills



* * *



## Try With AI



**Note**: The following prompts assume you have access to skill-creator in your Claude Code environment. If you haven't set it up, download the Skills Lab first (see setup section).



### Prompt 1: Build Your First Factory Skill



**Setup**: In your Claude Code session with the Skills Lab



```

I want to create a skill for [your domain:technical writing / project planning / code review / research summaries].My procedure is:1. [Step 1]2. [Step 2]3. [Step 3]4. [Step 4]5. [Step 5]What makes my approach distinctive:- [Your unique element 1]- [Your unique element 2]Use skill-creator to generate this skill. Ask clarifying questionsif anything is unclear, then create the complete SKILL.md.

```



**What you're learning**: How skill-creator asks questions to clarify your procedure, and how your answers shape the generated skill. You'll experience the factory's intelligenceâ€”it doesn't just copy templates, it adapts to your domain.



### Prompt 2: Design a 3-Skill System



**Setup**: After creating individual skills



```

I've created three skills: [skill1], [skill2], [skill3].Help me design them as a system:1. What's the natural flow/sequence?2. What's the user goal that ties them together?3. Where should they hand off to each other?4. What breaks the chain, and what's the fallback?5. Should I create a 4th skill to orchestrate the others?Show me the system diagram and suggest any improvements.

```



**What you're learning**: Systems thinkingâ€”understanding how individual skills compose into larger capabilities. You're learning to see beyond individual skills to skill ecosystems.



### Prompt 3: Envision Your Domain's skill-maker



**Setup**: Imagining future capabilities



```

Imagine a skill-maker specialized for [your domain].This meta-skill creates domain-specific skill-creators.For [your domain], what would that specialized creator build?1. What are the core skills it would generate?2. What domain-specific patterns would it enforce?3. How would it ensure skills in this domain work together?4. What would be different from a generic skill-creator?Help me design this hypothetical skill-maker specification.

```



**What you're learning**: Recursive abstraction and long-term thinking. You're practicing how to think about systems that generate systemsâ€”a key AI-native development skill.



Safety Tips for Skill Systems



-   **Test integration points**: When Skill A outputs data for Skill B, verify the format works

-   **Start small**: Test with 2-skill systems before building 5-skill chains

-   **Document interfaces**: Specify what input/output formats each skill expects

-   **Keep skills simple**: "Many simple skills" beats "one complex skill"



# Lesson Summary: The Skill Factory Pattern



## Key Concepts



### Skill Factory: Meta-Skill That Creates Skills



-   A **Skill Factory** is a meta-skillâ€”a skill that creates other skills

-   Encodes repeated patterns in SKILL.md creation into a reusable system

-   Achieves consistency and speed: approximately 3x faster than manual creation (~10 minutes per skill vs ~30 minutes)

-   Pattern recurs in every skill: YAML frontmatter â†’ "When to Use" â†’ Procedure â†’ Output Format â†’ Quality Criteria â†’ Example Input/Output



### skill-creator: The Pre-Built Meta-Skill



-   Available in Claude Code Skills Lab at `.claude/skills/skill-creator/SKILL.md`

-   Automates the process: Clarifying questions â†’ Structure planning â†’ SKILL.md generation â†’ Test guidance

-   Understands what makes effective descriptions, procedures, and examples for Claude learning

-   Provides integrated test guidance to verify generated skills work correctly



### Decision Framework: Factory vs Manual



-   **Use Manual**: First skill ever (learn mechanics), simple/single-purpose skill, complex skill with unique requirements, need fine-grained control

-   **Use Factory**: Creating 2+ similar skills, building a skill suite, need rapid iteration, teaching others to create skills, uncertain about structure, skill has supporting files

-   **Use skill-maker** (future): 10+ skills in a domain, team consistency needed, encoding domain-specific patterns, building products with reusable skill components



### Skill Systems: Multiple Skills Working Together



-   Systems need **design**: User goal â†’ Substeps (decomposed into skills) â†’ Natural order â†’ Integration points â†’ Graceful fallbacks

-   Each skill should have **single responsibility**: One thing done well, systems orchestrate skills

-   **Clear handoffs**: Output from one skill becomes input to next

-   Integration points must be tested to verify format compatibility



### skill-maker: The Ultimate Abstraction



-   Recursive pattern: Model creates code creates skills creates specialized skill-creators

-   Could generate domain-specific skill-creators:

    -   study-system-creator (for educational systems)

    -   blog-creator (for publishing workflows)

    -   code-review-creator (for quality assurance)

    -   meeting-system-creator (for organizational processes)

-   Enables teams to encode domain expertise into teachable systems



## Mental Models



### Chef Teaching Apprentices



Manual approach: Explain technique to each apprentice individually. Factory approach: Write down teaching system, use it to train hundreds consistently. Factory encodes and scales your decisions.



### Computing Stack Analogy



You don't build processors (models) or operating systems (runtimes). You build applications (skills). skill-creator is your application factory.



### Assembly Line vs Handcraft



-   Manual creation = handcrafted artisan work (best for unique, complex items)

-   Factory creation = assembly line efficiency (best for 2+ similar items)

-   Choose the right approach based on volume and consistency needs



### Layers of Abstraction



1.  **Manual skill creation**: Write SKILL.md by hand

2.  **skill-creator**: Factory that generates SKILL.md files

3.  **skill-maker**: Meta-factory that generates specialized skill-creators

4.  **Full automation**: Teams self-service skill creation using domain-specific factories



## Critical Patterns



### System Design Pattern



```

User Request (Large Goal)

â”œâ”€â”€ Skill 1 (Parse/Analyze)

â”œâ”€â”€ Skill 2 (Process/Transform)

â””â”€â”€ Skill 3 (Combine/Output)



```



Proper flow requires:



-   Defined sequence (natural order of operations)

-   Explicit integration points (output format from Skill A matches input expectations of Skill B)

-   Fallback strategies (what happens if one skill fails)



### skill-creator Workflow



1.  **Clarification phase**: Asks 5 key questions about problem, activation, procedure, distinctiveness, output

2.  **Planning phase**: Decides complexity level, YAML structure, procedure sections, example quality

3.  **Generation phase**: Creates complete SKILL.md with all sections tailored to your domain

4.  **Testing phase**: Suggests three test prompts for validation



### Three-Skill Study System Example



-   **study-notes skill**: Transforms topics into structured notes (5-7 sections with definitions, key points, examples)

-   **flashcards skill**: Converts study notes into spaced-repetition flashcards (2 easy, 2 medium, 1 hard)

-   **quiz skill**: Generates assessment from study notes (multiple choice, short answer, essay with answer key)

-   Integration: study-notes output â†’ flashcards input â†’ quiz input â†’ assessment result



## Common Mistakes



### Factory Overuse



-   Using skill-creator for first skill (you need to learn SKILL.md mechanics first)

-   Using factory for single, unique skill (faster to write manually)

-   Not testing integration points (assuming Skill A output works with Skill B input without verification)



### System Design Errors



-   Creating skills that are too broad (one skill doing everything, not a system)

-   Unclear handoffs between skills (output format doesn't match input expectations)

-   Ignoring fallback paths (system breaks if any single skill fails)

-   Tight coupling instead of loose integration (skills depend on internal details of other skills)



### Activation Mismatch



-   skill-creator generates activation triggers that don't match actual use cases

-   Generated skills activated by vague descriptions instead of clear problem statements

-   Not testing whether skill activates at the right times



### Complexity Misconception



-   Assuming skill-maker is just skill-creator applied to skill-creator (missing the domain specialization aspect)

-   Trying to build skill-maker before mastering skill-creator

-   Thinking recursion complexity is simple (it requires clear interface contracts between layers)



## Key Insight



The bottleneck in building large AI systems isn't intelligence (Claude has that) or code execution (Claude Code provides that). **The bottleneck is consistency and speed of encoding domain expertise at scale.** skill-creator solves this by making expert decision-making teachable and reproducible. skill-maker solves this at organizational scale by letting teams create specialized factories for their domains.



## Questions to Test Understanding



1.  When should you use manual skill creation vs skill-creator?

2.  How does skill-creator ask questions to shape the generated SKILL.md?

3.  What makes a good skill system design, and what breaks it?

4.  How would a domain-specific skill-maker (e.g., for customer support) differ from generic skill-creator?

5.  Why is "single responsibility" important when designing skills that work together?

6.  What are the three integration points in the study system example, and how could they fail?



## Connection to Other Lessons



-   **Lesson 06**: HOW to create skills manually â†’ foundation for understanding what skill-creator automates

-   **Lesson 07 (this)**: WHERE factory patterns emerge â†’ meta-skills that scale manual processes

-   **Lesson 08+**: WHEN to apply factories in real projects â†’ Pattern recognition for scalability needs

-   **Part 4 (Spec-Driven Development)**: How specifications encode the "questions" that skill-creator asks â†’ Skills as automated implementation

-   **Part 6 (Advanced Development)**: How to build skill-maker â†’ Meta-meta-skills and recursive abstractions



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/AI-Tool-Landscape/claude-code-features-and-workflows/skill-factory

</skill-factory-pattern>









<compiling-MCP-to-skills>

-   [](/)

-   [Part 2: AI Tool Landscape](/docs/AI-Tool-Landscape)

-   [Chapter 5: Claude Code: Your First General Agent](/docs/AI-Tool-Landscape/claude-code-features-and-workflows)

-   Compiling MCP to Skills



# Compiling MCP to Skills



You've learned MCP servers connect Claude Code to external systems. You've mastered skills as encoded expertise. Now: what happens when you want both, but MCP's token consumption makes it expensive?



![skills-mcp](https://pub-80f166e40b854371ac7b05053b435162.r2.dev/books/ai-native-dev/static/images/part-2/chapter-05/skills-mcp.png)



This lesson shows you a powerful pattern: **compile high-token MCP servers into lean skills**, reducing context consumption by up to 98.7% while maintaining full functionality.



Industry Standard



Skills format is now supported by Claude Code, OpenAI Codex (beta), and Goose. Skills you compile here work across all three agents.



* * *



## The Problem: MCP Token Bloat



When Claude Code loads an MCP server, it eagerly loads ALL tool definitions upfront. Here's the real impact from Anthropic's engineering blog:



> "Tool descriptions occupy more context window space, increasing response time and costs. For agents with thousands of tools, this means processing hundreds of thousands of tokens before reading a request." â€” Anthropic, [Code Execution with MCP](https://www.anthropic.com/engineering/code-execution-with-mcp)



**Concrete examples from the blog:**



-   **Agent with 1,000 tools**: **150,000 tokens** loaded before your first request

-   **2-hour meeting workflow**: Fetching transcript from Google Drive and attaching to Salesforce = **50,000 additional tokens** for repeated data processing

-   **Compiled skill approach**: Reduces to **~2,000 tokens (98.7% reduction)**



**The math for a single MCP server**: Playwright MCP loads approximately 5,000-8,000 tokens of tool definitions. Use it 3 times in a session? That's 15,000-24,000 tokens of overheadâ€”before you've accomplished anything.



#### ðŸ’¬ AI Colearning Prompt



> "I have 3 MCP servers installed. Help me estimate my token overhead: For each server, how many tokens does it load at startup? What's my total context cost before I've even asked a question?"



* * *



## The Solution: Code Execution Pattern



Instead of calling MCP tools directly through Claude's context, compile them into **skills with executable scripts**:



### The Architecture



**SKILL.md + Scripts Model:**



```

.claude/skills/browsing-with-playwright/â”œâ”€â”€ SKILL.md                    # High-level procedures (~150 tokens)â”œâ”€â”€ references/                 # Cached tool documentationâ””â”€â”€ scripts/    â”œâ”€â”€ mcp-client.py          # Universal MCP client (HTTP/stdio transport)    â”œâ”€â”€ start-server.sh        # Starts Playwright MCP on localhost:8808    â””â”€â”€ stop-server.sh         # Stops server gracefully

```



**How It Works:**



1.  **SKILL.md** provides high-level procedures (loaded once at startup, ~150 tokens)

2.  **Claude executes bash commands** calling `mcp-client.py` (runs locally, outside context)

3.  **mcp-client.py** connects to Playwright MCP server via HTTP transport

4.  **Server performs browser operations** (navigate, extract, screenshot)

5.  **Only filtered results** returned to Claude's conversation



**Token Comparison:**



-   Direct MCP: ~5,000-8,000 tokens Ã— 3 calls = ~15,000-24,000 tokens

-   Compiled skill: ~150 tokens (SKILL.md) + 0 tokens (local scripts) = ~150 tokens

-   **Savings: ~98-99%**



* * *



### Progressive Disclosure: 3-Stage Loading



Skills use **three-level loading** (covered in Lesson 6) to minimize token consumption:



1.  **Discovery** (startup): Load only `description` field (~30 tokens)

2.  **Activation** (when relevant): Load full SKILL.md (~150 tokens)

3.  **Execution** (when needed): Run `scripts/` locally (0 tokens in context)



**Key for compiled skills:** Stage 3 executes MCP tools outside Claude's context, so heavy operations consume zero tokens.



**Example:**



```

User: "Extract product prices from Amazon"â†’ Stage 1: Match description "browser automation"â†’ Stage 2: Load SKILL.md proceduresâ†’ Stage 3: Execute script locally (filter 1000 â†’ 20 products)â†’ Return only 20 results to Claude

```



* * *



### Why Scripts Save Tokens



**Without scripts (direct MCP):**



```

User request â†’ Claude calls browser_navigate (2,000 tokens loaded)            â†’ Claude calls browser_evaluate (3,000 tokens loaded)            â†’ Returns 1,000 product objects (10,000 tokens in context)            â†’ Claude filters to 20 relevant (10,000 tokens wasted)Total: 15,000 tokens

```



**With scripts (code execution):**



```

User request â†’ Claude runs bash command (0 tokens - executes locally)            â†’ bash: python mcp-client.py call -t browser_navigate ...            â†’ bash: python mcp-client.py call -t browser_evaluate ...            â†’ Server filters/processes data locally            â†’ Returns only final result to Claude (~200 tokens)Total: ~200 tokens

```



**Savings: ~98-99%**



* * *



## Hands-On: Use browsing-with-playwright Skill from Skills Lab



You'll experience the power of compiled skills by using the pre-built **browsing-with-playwright skill** from Skills Lab, then comparing its token efficiency against direct MCP usage.



### Step 1: Download Skills Lab



If you haven't already downloaded Skills Lab from Lesson 4, do so now:



1.  Go to [github.com/panaversity/claude-code-skills-lab](https://github.com/panaversity/claude-code-skills-lab)

2.  Click the green **Code** button

3.  Select **Download ZIP**

4.  Extract the ZIP file

5.  Open the extracted folder in your terminal



**If you already downloaded Skills Lab in Lesson 4**, navigate to that folder.



### Step 2: Baseline - Try Playwright MCP Directly



First, let's see the token overhead WITHOUT compilation. If you have Playwright MCP configured in Claude Code, start Claude:



```

claude

```



Ask Claude to use Playwright MCP directly:



```

Use the Playwright MCP server to navigate to https://example.comand extract the main heading text.

```



**What happens:**



-   Claude loads ALL Playwright MCP tool definitions (~5,000-8,000 tokens)

-   Calls `browser_navigate` tool through context

-   Calls `browser_evaluate` or `browser_snapshot` through context

-   Full tool schemas processed for each call

-   Returns result



**Observe**: This works, but notice the initial loading overhead when Claude loads tool definitions.



### Step 3: Now Use browsing-with-playwright Skill



Exit Claude (Ctrl+C) and restart for a fresh session in the Skills Lab folder:



```

claude

```



Now ask using the compiled skill:



```

Use browsing-with-playwright skill to navigate to https://example.comand extract the main heading text.

```



**What happens:**



```

âº Skill(browsing-with-playwright)  âŽ¿  Loadingâ€¦â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Use skill "browsing-with-playwright"? Claude may use instructions, code, or files from this Skill.   Browser automation using Playwright MCP. Navigate websites, fill forms,   click elements, take screenshots, and extract data. Do you want to proceed? â¯ 1. Yes  2. Yes, and don't ask again for browsing-with-playwright in this directory  3. No

```



Select **1\. Yes**.



**What happens now:**



1.  Claude loads browsing-with-playwright SKILL.md (~150 tokens only)

2.  Skill tells Claude to run: `bash scripts/start-server.sh` (starts Playwright MCP on localhost:8808)

3.  Claude executes bash commands:

    

    ```

    python mcp-client.py call -u http://localhost:8808 -t browser_navigate \  -p '{"url": "https://example.com"}'

    ```

    

4.  **Script runs OUTSIDE Claude's context** (0 tokens consumed)

5.  Only the result comes back: page snapshot showing heading "Example Domain"

6.  Claude runs another command to extract heading:

    

    ```

    python mcp-client.py call -u http://localhost:8808 -t browser_evaluate \  -p '{"function": "() => document.querySelector('h1').textContent"}'

    ```

    

7.  Result returned: `"Example Domain"` (~100 tokens)

8.  Claude stops server: `bash scripts/stop-server.sh`



**Key Difference**: Tool definitions NEVER loaded into Claude's context. All browser operations happen locally via HTTP calls.



### Step 4: Compare Token Usage



Ask Claude:



```

Compare the token usage between:1. Using Playwright MCP directly (what we did in Step 2)2. Using browsing-with-playwright skill (what we just did in Step 3)Estimate the tokens consumed in each approach andshow me the percentage reduction.

```



**Expected explanation:**



```

Direct Playwright MCP:- Tool definitions loaded at startup: ~5,000-8,000 tokens- browser_navigate call in context: ~2,000 tokens- browser_evaluate call in context: ~2,000 tokens- Result processing: ~500 tokens- Total: ~9,500-12,500 tokensbrowsing-with-playwright skill:- SKILL.md loaded: ~150 tokens- Bash commands executed: 0 tokens (runs locally)- Only final results returned: ~100 tokens- Total: ~250 tokensReduction: ~97-98% token savings

```



### Step 5: Test Different Browser Operations



Try various automation tasks to see the skill in action:



```

1. Use browsing-with-playwright to take a screenshot of https://github.com and save it2. Use browsing-with-playwright to extract the page title from https://news.ycombinator.com3. Use browsing-with-playwright to check if example.com contains the text "documentation"

```



**Observe:**



-   Each operation runs bash commands locally

-   You see: `python mcp-client.py call -t <tool_name> ...`

-   Server starts once, handles multiple operations

-   Only filtered results come back to conversation

-   Claude doesn't reload tool definitions



### Step 6: Explore How browsing-with-playwright Works Internally



After trying the skill, explore its structure:



```

# Look at the skill structurels -la .claude/skills/browsing-with-playwright/

```



You'll see:



```

SKILL.md           # Procedures Claude follows (~150 tokens)references/        # Cached tool documentationscripts/           # Scripts Claude executes locally

```



**This is the code execution pattern**: Heavy operations happen in local HTTP server, outside Claude's token-counted context.



#### ðŸ’¬ AI Colearning Prompt



> "I've used browsing-with-playwright skill. Explain: (1) How does running mcp-client.py via bash save tokens vs calling MCP tools through Claude's context? (2) What happens when Claude executes 'python mcp-client.py call -t browser\_navigate'? (3) If I performed 10 browser operations, what would be the token difference between direct MCP vs browsing-with-playwright skill?"



* * *



## Hands-On 2: Use fetch-library-docs Skill



Let's try a second compiled skill that demonstrates a different use case: **fetching library documentation with intelligent filtering**. This skill wraps the Context7 MCP server and reduces tokens by 60-90% through content-type filtering.



No Programming Required



You don't need to understand the code in the documentationâ€”you're learning how **token reduction** works, not React or Next.js. Focus on the numbers: how many tokens before vs after.



### What fetch-library-docs Does



When developers need documentation, they typically ask questions like:



-   "How do I install Next.js?"

-   "Show me examples of useState"

-   "What's the API for fetch in JavaScript?"



**Without the skill**: Context7 MCP returns everythingâ€”examples, explanations, API references, troubleshootingâ€”consuming thousands of tokens.



**With the skill**: You specify what you need (`setup`, `examples`, `api-ref`), and the skill filters locally, returning only relevant content.



### Step 1: Try fetch-library-docs Skill



In your Skills Lab folder, start Claude:



```

claude

```



Ask Claude to fetch installation instructions:



```

Use fetch-library-docs skill to look up "getting started" for Next.js.I only need setup instructions, not code examples.

```



**What happens:**



1.  Claude loads fetch-library-docs SKILL.md (~150 tokens)

2.  Skill executes bash command locally:

    

    ```

    bash scripts/fetch-docs.sh --library nextjs --topic "getting started" --content-type setup

    ```

    

3.  Script calls Context7 MCP via subprocess (outside Claude's context)

4.  **Filters response locally** to extract only terminal commands and setup instructions

5.  Returns filtered content (~50-100 tokens instead of ~500-800)



**You'll see output like:**



```

## Setup Instructions### Installationnpm create next-app@latest my-app### Run Development Servercd my-appnpm run dev[Token savings: 81%]

```



### Step 2: Compare Content Types



Now try the same library but requesting code examples:



```

Use fetch-library-docs skill to look up "data fetching" for Next.js.I want code examples only.

```



**Different content type = different filtering:**



```

bash scripts/fetch-docs.sh --library-id /vercel/next.js --topic "data fetching" --content-type examples

```



**You'll see:**



```

## Code Examples### Example 1export default async function Page() {  let data = await fetch('https://api.vercel.app/blog')  let posts = await data.json()  return (    <ul>      {posts.map((post) => (        <li key={post.id}>{post.title}</li>      ))}    </ul>  )}[Token savings: 92%]

```



### Step 3: Understand the Token Savings



Ask Claude to explain what just happened:



```

Compare the token usage between:1. Calling Context7 MCP directly for "Next.js data fetching"2. Using fetch-library-docs skill with --content-type examplesShow me the savings breakdown.

```



### Step 4: Explore the Skill Structure



Ask Claude to show you the internal structure of fetch-library-docs skill:



You'll see:



```

SKILL.md           # Decision logic for when/how to fetch docsreferences/        # Library IDs, patterns, API detailsscripts/           # Shell scripts that call Context7 MCP locally  â”œâ”€â”€ fetch-docs.sh      # Main orchestrator  â”œâ”€â”€ fetch-raw.sh       # Calls Context7 MCP  â”œâ”€â”€ filter-by-type.sh  # Routes to content extractors  â””â”€â”€ extract-*.sh       # Content-type specific filters

```



**Key insight**: The filtering happens in shell scripts (local execution), not in Claude's context. This is why tokens are savedâ€”heavy processing stays outside the conversation.



#### ðŸ’¬ AI Colearning Prompt



> "I used fetch-library-docs skill with two different content types: setup and examples. Explain: (1) Why does 'setup' have higher token savings than 'examples'? (2) What happens locally when the skill filters content? (3) If I needed both examples AND API reference, how would I request that?"



* * *



## When to Compile MCP Servers



Not every MCP server benefits from compilation. Use this decision framework:



### Compile to Skill When:



âœ… **High token overhead** (>5,000 tokens per query)



-   Example: Playwright, Google Drive, Database MCP, Context7 (documentation)



âœ… **Frequent use** (3+ times per session or across projects)



-   Repeated calls multiply token waste



âœ… **Large datasets returned** (need filtering/transformation)



-   Processing 1,000 items â†’ returning 20 relevant ones



âœ… **Multi-step workflows** (chaining operations)



-   Navigate â†’ extract â†’ transform â†’ filter



### Use Direct MCP When:



âŒ **Low token overhead** (<1,500 tokens per query)



-   MCP already efficient, compilation overhead not worth it



âŒ **Infrequent use** (once per month or less)



-   Setup cost > token savings



âŒ **Small, well-formatted results** (no transformation needed)



-   Results already optimal for Claude



âŒ **Rapidly changing API** (MCP tools frequently updated)



-   Skill scripts would need constant maintenance



* * *



## Decision Framework: When to Compile vs. Use Direct MCP



Not every MCP server needs compilation. Use this matrix to decide:



Scenario



Recommendation



Reasoning



**One-off query**



Use MCP directly



Compilation overhead not worth it for single use



**Repeated workflow** (3+ times)



Compile to skill



Amortizes compilation cost across multiple uses



**High token definitions** (5,000+ tokens)



Compile to skill



Token savings justify upfront work



**Low token definitions** (<500 tokens)



Use MCP directly



Compilation provides minimal benefit



**Rapidly changing API**



Use MCP directly



Compiled skill becomes stale quickly



**Stable tool set**



Compile to skill



Skill remains accurate over time



**Privacy-sensitive data**



Compile to skill



Local script execution avoids context logging



**Integration with other skills**



Compile to skill



Composability improves with skill format



**Team workflow**



Compile to skill



Shareable SKILL.md vs proprietary MCP setup



**Decision shortcut**:



-   Calling MCP 1-2 times? â†’ Use direct MCP

-   Calling MCP 3+ times in same session? â†’ Compile to skill

-   Working with high-token server? â†’ Compile to skill

-   Building production workflow? â†’ Compile to skill



* * *



## What's Ahead



You've experienced compiled skills and their massive token reductionâ€”up to 98% savings while preserving full functionality. You understand the code execution pattern and why it works. But what happens when you need multiple skills working together on complex tasks?



**Lesson 10: Subagents and Orchestration** introduces the next level: specialized AI assistants that handle specific types of work with isolated context. Where compiled skills give you *efficiency*, subagents give you *coordination*â€”a team of focused specialists instead of one overloaded generalist.



**In advanced lessons**, you'll learn to create your own compiled skills using skill-creator, compiling other MCP servers (Google Drive, Database, etc.) and designing custom workflows. The skills you use from Skills Lab now become templates for creating your own later.



* * *



## Sources



Research and tools supporting this lesson:



-   [Anthropic: Code Execution with MCP](https://www.anthropic.com/engineering/code-execution-with-mcp) â€” Architecture for local execution + context reduction

-   [Armin Ronacher: Skills vs Dynamic MCP Loadouts](https://lucumr.pocoo.org/2025/12/13/skills-vs-mcp/) â€” Token efficiency analysis and pattern recommendations

-   [SmartScope: MCP Code Execution Deep Dive](https://smartscope.blog/en/blog/mcp-code-execution-agent-design/) â€” Detailed compilation workflow examples

-   [Claude Code Documentation: MCP Integration](https://docs.anthropic.com/claude-code/mcp) â€” Official MCP protocol reference



* * *



## Try With AI



### browsing-with-playwright Skill



> "I've downloaded the Skills Lab. Guide me through using the browsing-with-playwright skill to extract product names from an e-commerce site. Show me the token savings compared to direct MCP."



**Measure Token Reduction:**



> "I used browsing-with-playwright skill for 3 browser operations. Calculate the token savings: (1) Estimate tokens if I used Playwright MCP directly, (2) Estimate tokens with browsing-with-playwright skill, (3) Show percentage reduction with explanation."



### fetch-library-docs Skill



**Fetch Different Content Types:**



> "Use fetch-library-docs skill to look up React useState. First fetch with --content-type examples, then with --content-type api-ref. Compare the outputs and explain why they're different sizes."



**Compare Token Savings:**



> "I need to look up 'routing' for Next.js. Show me the token difference between: (1) Using fetch-library-docs with --content-type setup, (2) Using fetch-library-docs with --content-type all (no filtering). Calculate the percentage saved."



**Decide When to Use:**



> "I have these MCP servers installed: \[list\]. For each, should I look for a compiled skill or use direct MCP? Use the decision framework to recommend."



**Compare Direct MCP vs Compiled Skill:**



> "I want to understand the difference: (1) Run a browser automation task using Playwright MCP directly, (2) Run the same task using browsing-with-playwright skill, (3) Show me the exact token difference and explain where savings come from."



### Core Concept



MCP servers load ALL tool definitions at startup (5,000-8,000+ tokens each), creating token bloat. Pre-compiled skills reduce context consumption by 97-99% while preserving full functionalityâ€”SKILL.md (~150 tokens) + local script execution (0 tokens in context) replaces repeated tool definition loading.



### Key Mental Models



-   **Token Bloat**: MCP eagerly loads everything; you're paying for tools you're not using. Agent with 1,000 tools = 150,000 tokens before first request. 2-hour workflow might waste 50,000+ tokens.

-   **Code Execution Pattern**: Instead of calling MCP tools directly through Claude's context, compiled skills run scripts locally via bashâ€”Claude orchestrates, `mcp-client.py` executes outside context.

-   **Progressive Disclosure (3-Stage Loading)**: (1) Discoveryâ€”load description only (~30 tokens), (2) Activationâ€”load full SKILL.md (~150 tokens), (3) Executionâ€”run scripts locally (0 tokens).

-   **Content-Type Filtering**: Skills like fetch-library-docs filter responses locally (setup, examples, api-ref), returning only relevant content for 60-90% additional savings.



### Critical Patterns



-   Use pre-compiled skills from Skills Lab: `browsing-with-playwright`, `fetch-library-docs`

-   Skills execute bash commands: `python mcp-client.py call -t browser_navigate ...`

-   Scripts run OUTSIDE Claude's contextâ€”heavy operations consume zero tokens

-   Content-type filtering: `--content-type setup` vs `--content-type examples` returns different filtered results

-   Compare token usage: Direct MCP (~15,000-24,000 tokens) vs compiled skill (~150-250 tokens)



### Common Mistakes



-   Using direct MCP for repeated workflowsâ€”token waste multiplies with each call

-   Not specifying content-type when fetching docsâ€”returns everything instead of filtered content

-   Expecting to create skills in this lessonâ€”skill creation is covered in advanced lessons

-   Forgetting to start/stop the MCP server when using browsing-with-playwright



### Decision Framework



-   **Use direct MCP**: One-off queries, low-token servers (<1,500 tokens), rapidly-changing APIs, small well-formatted results

-   **Use compiled skill**: Repeated workflows (3+ calls), high-token servers (5,000+), large datasets needing filtering, multi-step workflows, team sharing, privacy-sensitive data



### Connections



-   **Builds on**: Skills (Lessons 05-06) for SKILL.md format and Skills Lab; MCP Integration (Lesson 08) for understanding MCP servers

-   **Leads to**: Subagents and Orchestration (Lesson 11) for composing skills into multi-agent workflows; Advanced lessons for creating your own compiled skills



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/AI-Tool-Landscape/claude-code-features-and-workflows/compiling-mcp-to-skills

</compiling-MCP-to-skills>









<subagents-and-orchestration>

-   [](/)

-   [Part 2: AI Tool Landscape](/docs/AI-Tool-Landscape)

-   [Chapter 5: Claude Code: Your First General Agent](/docs/AI-Tool-Landscape/claude-code-features-and-workflows)

-   Subagents and Orchestration



# Subagents and Orchestration



You've installed Claude Code and tried basic commands. Now let's understand **subagents**â€”specialized AI assistants that help Claude Code handle complex tasks more effectively.



* * *



## The Problem: Context Clutter



Imagine you're working on a startup idea. You ask Claude Code to:



1.  Research competitors in the market

2.  Explain business model options

3.  Find relevant industry reports

4.  Draft a pitch deck outline



By request #4, Claude Code's conversation context is cluttered with research notes, explanations, and report summaries. The context is messy.



**Solution**: Instead of one AI trying to do everything, **specialized assistants (subagents)** handle focused tasks with clean, isolated context.



* * *



## What Are Subagents?



**Definition**: A subagent is a specialized AI assistant with its own instructions and isolated context window. Each subagent is an expert at one type of task.



Think of Claude Code as a project manager with a team of specialists:



-   **Claude Code (main)**: Coordinates overall work

-   **Plan subagent**: Researches your codebase and creates multi-step plans

-   **Custom subagents**: You can create specialists for your team's specific needs (content planning, research synthesis, document structuring, etc.)



![Three-tier hierarchy tree showing Claude Code (orchestrator) at top, Subagents (specialized agents) in middle tier, and Skills (reusable capabilities) at bottom, with delegation arrows and example instances](https://pub-80f166e40b854371ac7b05053b435162.r2.dev/books/ai-native-dev/static/images/part-2/chapter-05/skills-subagents-hierarchy-tree.png)



**Key benefit**: Each subagent has **clean context** (no clutter from other conversations) and **focused expertise** (specialized instructions for its task).



#### ðŸ’¬ AI Colearning Prompt



> "Explain why subagents use isolated context windows instead of sharing the main conversation. What problems does context isolation solve?"



* * *



## Why This Matters: Specialized Expertise



**Workflow Impact**: Subagents let you delegate complex, multi-step tasks to specialists. Instead of a single AI trying to do everything, you have a teamâ€”one expert for research, one for code review, one for testing, one for documentation. Each maintains cleaner context, produces better results.



**Paradigm Connection**: This is the orchestration pattern in AI-driven development. Like a software architect delegating to specialists (frontend dev, backend dev, DBA), you orchestrate AI specialists. The main Claude Code session is your architect; subagents are your specialists.



**Real-World Context**: In production work, you'll create subagents for:



-   Research-heavy tasks (gather requirements, analyze documentation)

-   Specialized code reviews (security audits, performance optimization)

-   Domain-specific work (database design, API integration, testing strategies)



Subagents are how you scale AI collaboration beyond single tasks.



* * *



## The Plan Subagent (Built-In)



Claude Code includes a **Plan subagent** that automatically activates for complex, multi-step tasks.



### When Plan Activates



When you ask for complex work, Claude Code delegates to the Plan subagent:



**You ask**: "Add user authentication to this project"



**Plan subagent does**:



1.  **Researches your codebase** to understand current structure

2.  **Creates a multi-phase plan**:

    -   Phase 1: Database schema (users table, sessions)

    -   Phase 2: Auth logic (password hashing, login/logout)

    -   Phase 3: Integration (middleware, protect routes)

    -   Phase 4: Testing (unit tests, flow validation)

3.  **Presents plan for your approval** before any changes



### Why This Matters



**Without Plan subagent**: Claude might jump straight to code without understanding your project structure, missing dependencies or creating conflicts.



**With Plan subagent**: Research happens first, then a strategic plan, then executionâ€”step by step.



#### ðŸŽ“ Expert Insight



> In AI-native development, orchestration is a design skill, not a technical one. You don't memorize subagent commandsâ€”you recognize WHEN a task needs specialized focus vs general assistance. The `/agents` syntax is cheap; knowing when to delegate is gold.



* * *



## How Subagents Work



### The Execution Model: One Task, One Completion



**Critical concept**: A subagent is invoked **once** for a specific goal, completes its work, and **returns results to main Claude Code**.



**The flow**:



1.  Main Claude Code recognizes a task that needs a specialist

2.  Launches the subagent with a specific goal

3.  Subagent works independently in isolated context

4.  Subagent completes its task and returns results

5.  **Control returns to main Claude Code**

6.  You interact with main Claude Code to proceed



**Think of it like this**: You send a specialist to research something. They go off, do their work, come back with a report, and then you continue the conversation with your main assistant.



### Automatic Delegation



You don't command "use the Plan subagent." Claude Code decides when to delegate based on:



-   Task complexity (multi-step tasks trigger Plan)

-   Your request type (code review request might trigger a review subagent if you have one)

-   Subagent descriptions (Claude matches task to specialist)



**Built-in automatic triggers**:



-   **Explore subagent**: Automatically triggers for open-ended searches and codebase exploration

-   **Plan subagent**: Activates for complex, multi-step tasks that need research and strategy

-   **Specialized subagents**: Match based on how their description aligns with your request



**Example**:



```

You: "Help me create a content marketing strategy for my startup"Claude Code: *recognizes complexity, delegates to Plan subagent*Plan subagent: *researches market, creates phase breakdown, returns plan*Claude Code: *receives plan, presents it to you*You: *approve or modify the plan*Claude Code: *proceeds with execution*

```



### Explicit Invocation



You can also request a specific subagent directly:



```

You: "Use the startup-planner subagent to analyze my business model"Claude Code: *invokes startup-planner, waits for results, presents findings*

```



* * *



## More Subagent Ideas



Once you understand the pattern, you can create subagents for any specialized task:



-   **Research Subagents**: Deep-dive into documentation, gather requirements, analyze existing solutions

-   **Code Review Subagents**: Security audits, performance analysis, pattern compliance checks

-   **Testing Subagents**: Test strategy design, edge case generation, coverage analysis

-   **Documentation Subagents**: API documentation, architecture diagrams, onboarding guides



**The pattern is always**:



1.  Define the specialist's focus (what domain expertise does this subagent have?)

2.  Identify key decisions (what autonomy does it need?)

3.  Specify output format (how should results be delivered?)



You learned this pattern in Step 2.5 (Co-learn Subagent Design). Apply it to create any subagent your workflow needs.



For detailed subagent architecture, see `.claude/agents/` directory for real examples from this book's production system.



* * *



## Understanding Orchestration



**Orchestration** = AI coordinating multiple specialists toward a goal



When you ask for a complex task:



1.  **Claude Code (orchestrator)** analyzes your request

2.  **Launches a specialist subagent** (e.g., Plan subagent) for a specific goal

3.  **Subagent completes its task** (researches, creates strategy) and **returns results**

4.  **Control returns to main Claude Code**, which presents the results to you

5.  **You approve/modify** the strategy

6.  **Main Claude Code executes** step-by-step with your oversight



**Key insight**: Subagents don't stick around. They're invoked for one task, complete it, return results, and hand control back to main Claude Code.



**This is orchestration in action**: One AI managing a team of specialist AIs to accomplish complex workâ€”each specialist does their job and returns the baton.



* * *



## Hands-On: Create Your First Custom Subagent



Creating a subagent is **easy**â€”Claude Code has a built-in workflow. Let's create a simple startup planning subagent together.



### Step 1: Launch the Agent Creation Workflow



In Claude Code, type:



```

/agents

```



**What you'll see**:



```

â”‚ Agents                                                                        â”‚â”‚ 9 agents                                                                      â”‚â”‚                                                                               â”‚â”‚ â¯ Create new agent                                                            â”‚â”‚                                                                               â”‚â”‚   Built-in agents (always available)                                         â”‚â”‚   general-purpose Â· sonnet                                                   â”‚â”‚   Explore Â· haiku                                                            â”‚â”‚   Plan Â· sonnet                                                              â”‚

```



Select **"Create new agent"**



### Step 2: Choose Location



```

â”‚ Choose location                                                               â”‚â”‚                                                                               â”‚â”‚ â¯ 1. Project (.claude/agents/)                                                â”‚â”‚   2. Personal (~/.claude/agents/)                                             â”‚

```



**Choose 1** (Project) â€” this makes the agent available only in this project.



### Step 3: Choose Creation Method



```

â”‚ Creation method                                                               â”‚â”‚                                                                               â”‚â”‚ â¯ 1. Generate with Claude (recommended)                                       â”‚â”‚   2. Manual configuration                                                     â”‚

```



**Choose 1** â€” Let Claude generate the agent based on your description.



### Step 4: Describe Your Agent



```

â”‚ Describe what this agent should do and when it should be used                â”‚â”‚                                                                               â”‚â”‚ e.g., Help me plan a content marketing strategy...                           â”‚

```



**Type this**:



```

Help me plan startup ideas: research competitors, analyze business models,identify market gaps, and suggest go-to-market strategies.Use this when I ask "help me plan a startup" or "analyze this business idea."

```



Press **Enter**.



### Step 5: Claude Generates the Agent



```

â”‚ âœ½  Generating agent from description...                                      â”‚

```



Claude Code creates:



-   Agent name: `startup-planner`

-   Instructions based on your description

-   Tool permissions (WebSearch, Read)

-   Saves to `.claude/agents/startup-planner.md`



### Step 6: Test Your New Subagent



Now test it! In Claude Code, say:



```

Use the startup-planner subagent to analyze this business idea:"A platform that connects freelance writers with small businesses"

```



**What happens**:



1.  Main Claude Code launches your `startup-planner` subagent

2.  Subagent researches competitors, analyzes the market

3.  Subagent completes its analysis and returns findings

4.  Control returns to main Claude Code

5.  Main Claude presents the business plan to you



**Key insight**: You just created a reusable specialist that you can invoke anytime with a simple request.



* * *



## Where Subagents Live



Subagents are stored as Markdown files with YAML frontmatter:



**Project-level**: `.claude/agents/` (specific to this project) **User-level**: `~/.claude/agents/` (available across all your projects)



**Example subagent file structure**:



```

---name: startup-plannerdescription: Analyzes business ideas and creates go-to-market strategiesmodel: sonnetcolor: purple---# Startup Planning InstructionsWhen analyzing a business idea:1. Research competitors and market size2. Identify unique value propositions3. Suggest business model options4. Create go-to-market strategy...

```



* * *



## What's Ahead



Subagents delegate to specialists. But who controls the rules of engagement? What happens when team members need different configurations, or when you want to experiment locally without affecting project settings?



Lesson 10 introduces **settings hierarchy**â€”three levels of configuration that enable personal customization, team standards, and local experimentation to coexist without conflict.



* * *



## Try With AI



Let's build your first custom subagent and explore how subagent orchestration works in practice.



**ðŸŽ¯ Practice Creating a Custom Subagent:**



> "Walk me through creating a custom subagent for \[describe a repeated task you do: code reviews / blog planning / meeting notes / test design\]. Use the `/agents` command. Help me think through: What should this subagent do autonomously? What questions should it ask me to gather context? What should the output format be?"



**ðŸ” Explore the Plan Subagent:**



> "I want to create a 6-month content marketing plan for my startup. Create a strategy showing the phases and what each phase includes. Help me understand: When does the Plan subagent activate? What research does it do? How does control return to you after it completes?"



**ðŸ§ª Test Your Custom Subagent:**



> "I just created a \[your subagent name\] subagent. Help me test it with a real scenario: \[describe your specific use case\]. Invoke the subagent explicitly and walk me through what happens step-by-step. Show me the one-task, one-completion pattern in action."



**ðŸš€ Design an Advanced Workflow:**



> "I have complex projects that could benefit from multiple specialized subagents (research, code review, documentation, testing). Help me design a workflow where I orchestrate several subagents for \[describe your project type\]. Which subagents should I create? When should each be invoked? How do I coordinate their outputs?"



### Core Concept



Subagents are specialized AI assistants with isolated context windows. Each subagent handles one focused task, completes it, and returns results to the main Claude Code sessionâ€”enabling complex work through orchestrated delegation rather than cluttered single-context conversations.



### Key Mental Models



-   **Orchestrator/Specialist**: Claude Code acts as a project manager coordinating a team of specialist subagentsâ€”one for research, one for code review, one for testingâ€”each expert in their domain

-   **One Task, One Completion**: Subagents don't persist. They receive a goal, work independently, return results, and hand control back. Like sending a specialist to research somethingâ€”they return with a report, then you continue with your main assistant

-   **Context Isolation = Clean Context**: Each subagent starts fresh without clutter from other conversations, producing more focused, higher-quality results



### Critical Patterns



-   Use `/agents` â†’ "Create new agent" to build custom subagents through Claude's guided workflow

-   Describe what the subagent should do and when it should be used; Claude generates the configuration

-   Subagents can activate automatically (Claude recognizes task complexity) or explicitly ("Use the startup-planner subagent to...")

-   Subagent files live in `.claude/agents/` (project) or `~/.claude/agents/` (personal)

-   The built-in Plan subagent automatically activates for complex multi-step tasks, researching your codebase before proposing changes



### Common Mistakes



-   Expecting subagents to remember previous conversationsâ€”each invocation starts with clean context

-   Trying to continue dialogue with a subagent after it completesâ€”control returns to main Claude Code

-   Overusing subagents for simple tasks that don't need specialist focus

-   Creating overly broad subagents instead of focused specialists (defeats the purpose of context isolation)



### Connections



-   **Builds on**: Claude Code installation, CLAUDE.md project memory, understanding of AI collaboration patterns

-   **Leads to**: Multi-subagent orchestration workflows, advanced delegation patterns, building domain-specific subagent suites



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/AI-Tool-Landscape/claude-code-features-and-workflows/subagents-and-orchestration

</subagents-and-orchestration>









<settings-hierarchy>

-   [](/)

-   [Part 2: AI Tool Landscape](/docs/AI-Tool-Landscape)

-   [Chapter 5: Claude Code: Your First General Agent](/docs/AI-Tool-Landscape/claude-code-features-and-workflows)

-   Settings Hierarchy



# Settings Hierarchy



## What Are Settings?



Claude Code has a **settings system** that lets you customize how it behaves. These settings control things like:



-   Permission modes (whether Claude asks before edits)

-   Output preferences (how Claude formats responses)

-   Project-specific defaults (which tools Claude prioritizes)

-   Team standards (shared rules for collaborative work)



Instead of having one global settings file, Claude Code uses a **three-level hierarchy**. This design lets you have **personal preferences, project standards, and temporary overrides** all at the same time.



#### ðŸ’¬ AI Colearning Prompt



> "Explain why applications use hierarchical configuration systems instead of a single global settings file. What problems does hierarchy solve?"



* * *



## The Three Settings Levels



![Pyramid showing three configuration levelsâ€”Global settings (base, system-wide defaults), Project settings (middle, .claude/config for repo), File settings (top, frontmatter overrides)â€”with precedence arrows showing file > project > global](https://pub-80f166e40b854371ac7b05053b435162.r2.dev/books/ai-native-dev/static/images/part-2/chapter-05/settings-hierarchy-global-project-file.png)



Claude Code settings exist at three levels, from general to specific:



### Level 1: User Settings (Most General)



**Location**: `~/.claude/settings.json`



**Scope**: Applies to **all your Claude Code projects** on your machine



**Applies to**: Every project you work on, across your entire system



**When to use this level**:



-   Your personal preferences (always use dark mode, prefer verbose output)

-   Your coding style (consistent formatting choices)

-   Your workflow defaults (always prefer plan mode for safety)



**Example content**:



```

{  "model": "claude-sonnet-4-5-20250929",  "outputStyle": "Concise",  "includeCoAuthoredBy": true}

```



### Level 2: Project Settings (Middle)



**Location**: `.claude/settings.json` (inside your project directory)



**Scope**: Applies to **just this project**



**Applies to**: Only when you're working in this specific project



**When to use this level**:



-   Team standards (your team agrees on permission settings)

-   Project-specific customizations (this project uses a different framework)

-   Temporary standards (during alpha testing, use stricter permissions)



**Example content**:



```

{  "permissions": {    "defaultMode": "acceptEdits",    "allow": ["Bash(npm run test:*)"],    "deny": ["Read(./.env)"]  },  "env": {    "PROJECT_ENV": "development"  }}

```



### Level 3: Local Settings (Most Specific)



**Location**: `.claude/settings.local.json` (inside your project directory)



**Scope**: Applies to **this project only, on your machine only**



**Applies to**: Just your local work in this project (not shared with team)



**When to use this level**:



-   Temporary overrides (you need different settings just for today)

-   Personal experiments (testing a new workflow locally)

-   Machine-specific settings (your laptop needs different settings than your desktop)



#### ðŸ’¬ AI Colearning Prompt



> "Why would you use three levels of settings (user/project/local) instead of just one global settings file? What flexibility does this hierarchy provide?"



**Example content**:



```

{  "outputStyle": "Verbose",  "sandbox": {    "enabled": true  }}

```



* * *



## Why Settings Hierarchy Matters



### The Organizational Intelligence Framework



**Team Collaboration Without Conflicts**: Settings hierarchy enables teams to share standards while allowing personal customization and local experimentationâ€”all without stepping on each other's toes.



**Three Layers of Context** (from general to specific):



-   **User settings** (`~/.claude/settings.json`): Your personal AI preferences that follow you across ALL projects

-   **Project settings** (`.claude/settings.json`): Team-agreed standards that everyone on the project shares (shared via git)

-   **Local settings** (`.claude/settings.local.json`): Your private testing and experiments (gitignored, never committed)



This connects to earlier lessons:



-   **CLAUDE.md (Lesson 07)** provides content context at the project level

-   **Skills (Lesson 06)** can be enabled at any hierarchy level

-   **Plugins (Lesson 12)** will use this same hierarchy to configure bundled capabilities



### Practical Applications



#### 1\. Cross-Project Preferences (User Level)



You always prefer verbose output and specific model settings. â†’ Set these in `~/.claude/settings.json` â†’ These preferences follow you across ALL projects â†’ Project or local settings can override for specific needs



**Example**:



```

{  "model": "opus",  "outputStyle": "Verbose",  "includeCoAuthoredBy": true}

```



#### 2\. Team Standards (Project Level)



Your team decides: "All projects should deny access to .env files for security." â†’ Set `permissions.deny: ["Read(./.env)"]` at **project level** (`.claude/settings.json`) â†’ Everyone on the team gets this standard automatically â†’ Overrides user-level settings for this project



**Example**:



```

{  "permissions": {    "defaultMode": "acceptEdits",    "deny": ["Read(./.env)"]  },  "env": {    "PROJECT_ENV": "development"  }}

```



#### 3\. Personal Workflow Experiments (Local Level)



You want to test a new workflow without affecting the team. â†’ Create `.claude/settings.local.json` with your experimental settings â†’ Your changes stay private, invisible to the team â†’ Local overrides take precedence over both project and user settings â†’ Delete the file to revert to project/user standards



**Example**:



```

{  "outputStyle": "Concise",  "sandbox": {    "enabled": true  }}

```



### Real-World Impact



Without this hierarchy, teams either enforce rigid standards (no personal customization) or descend into chaos (everyone's setup is different). The three-level system gives you both consistency AND flexibility.



#### ðŸŽ“ Expert Insight



> In AI-native development, configuration hierarchy mirrors organizational intelligence. User settings = your personal defaults. Project settings = team agreements. Local settings = safe experimentation space. Understanding WHEN to configure at each level is more valuable than memorizing settings syntax.



* * *



## Precedence: Which Settings Win?



When the same setting exists at multiple levels, Claude Code follows this **precedence order** (most specific wins):



**Local > Project > User**



This means:



-   **Local settings** override both project and user settings

-   **Project settings** override user settings

-   **User settings** are the fallback when nothing more specific exists



### Visual Hierarchy



```

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚   LOCAL SETTINGS                â”‚â”‚   .claude/settings.local.json    â”‚  â† Most Specific (Highest Priority)â”‚   (just your machine, temporary) â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â†‘ Overridesâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚   PROJECT SETTINGS              â”‚â”‚   .claude/settings.json          â”‚  â† Team/Project Levelâ”‚   (shared with team)             â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â†‘ Overridesâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚   USER SETTINGS                 â”‚â”‚   ~/.claude/settings.json        â”‚  â† Most General (Fallback)â”‚   (all projects on this machine) â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```



* * *



## Example: Settings Precedence in Action



Let's say you have:



**User level** (`~/.claude/settings.json`):



```

{  "outputStyle": "Concise"}

```



**Project level** (`.claude/settings.json` in your project):



```

{  "outputStyle": "Explanatory"}

```



**Local level** (`.claude/settings.local.json` in your project):



```

{  // Empty or not set}

```



**Result**: Claude Code uses `outputStyle: "Explanatory"` (from project level, since it overrides user level)



* * *



### What If Local Level Is Set?



Now you add a temporary local override:



**Local level** (`.claude/settings.local.json`):



```

{  "outputStyle": "Verbose"}

```



**New Result**: Claude Code uses `outputStyle: "Verbose"` (from local level, which overrides both project and user)



**Why this matters**: You can temporarily change your workflow for this one session without affecting your project's standards or your personal preferences. Tomorrow, when you delete the local settings file, you're back to `"Explanatory"` (project level).



#### ðŸ¤ Practice Exercise



> **Ask your AI**: "I have outputStyle set to 'Concise' at user level and 'Explanatory' at project level. I'm working in this project. Which style is active? If I create a .claude/settings.local.json file with outputStyle: 'Verbose', what happens?"



**Expected Outcome**: AI explains that project level is active (Explanatory), and creating a local override would switch to Verboseâ€”helping you understand how to temporarily override settings without changing team standards.



* * *



## The .claude/ Directory: Don't Delete It



You might see a `.claude/` directory in your project and wonder: "Is this important? Can I delete it?"



**Short answer**: Don't delete it.



**What it contains**:



-   `settings.json` â€” Project-level settings

-   `settings.local.json` â€” Your local, temporary overrides

-   Other configuration files Claude Code needs



The `.claude/` directory is how Claude Code stores project customization. Deleting it would reset all your project settings to defaults.



**What you should do**: Treat `.claude/settings.json` like your `.gitignore` or `package.json`â€”it's part of your project configuration. Include it in version control (share with team). But `.claude/settings.local.json` should probably be in your `.gitignore` (keep it personal).



#### ðŸ’¬ AI Colearning Prompt



> "Explain the difference between .claude/settings.json and .claude/settings.local.json. Which one should be in .gitignore? Why?"



* * *



## Not Configuring Yetâ€”This Is Part 5 Content



This lesson teaches you that **settings exist and how the hierarchy works**. You don't need to configure them yet. Basic Claude Code usage works perfectly fine with defaults.



**Detailed settings configuration** (what specific settings do, how to change them, team policies) is **Part 5 content** (Spec-Driven Development, team workflows). For now, just know:



âœ… Settings exist at three levels âœ… Precedence is: local > project > user âœ… This hierarchy enables team collaboration + personal customization



That's enough to understand when you encounter `.claude/settings.json` references in documentation.



* * *



## Try With AI



Let's understand how Claude Code's three-level settings hierarchy enables both team collaboration and personal customization.



**ðŸ’¡ Understand the Hierarchy:**



> "Claude Code has settings at three levels: user (~/.claude/settings.json), project (.claude/settings.json), and local (.claude/settings.local.json). Explain what each level is for and why having three levels is better than one global settings file. Give me concrete examples of what I'd put at each level."



**ðŸ” Verify Your Current Configuration:**



> "Help me check which settings files exist on my system. Walk me through the commands to check each level (user, project, local). Then, based on what exists, explain which settings are actually controlling my current Claude Code session and why."



**ðŸ§ª Test Precedence Rules:**



> "Let's test precedence with a scenario: User level has outputStyle='Concise', Project level has outputStyle='Explanatory', and Local level is not set. Which outputStyle is active and why? Then, if I create a .claude/settings.local.json file with outputStyle='Verbose', what happens? Walk me through the precedence logic."



**ðŸš€ Plan for Team Workflows:**



> "When I learn team workflows in Part 5, help me understand which settings level to use for different scenarios: personal preferences (my editor style, my default verbosity), team standards (shared coding conventions, security policies), and temporary experiments (testing new configurations). Explain the decision framework for choosing the right level."



### Core Concept



Claude Code uses a three-level settings hierarchy (user â†’ project â†’ local) where more specific settings override more general ones, enabling both team standards and personal customization without conflicts.



### Key Mental Models



-   **Specificity Wins**: More specific context always overrides more generalâ€”local overrides project overrides user

-   **Three Layers of Context**: User settings = personal defaults across all projects; Project settings = team-shared standards committed to git; Local settings = private experimentation that stays on your machine

-   **Configuration as Organizational Intelligence**: The hierarchy mirrors how teams organize knowledgeâ€”individual preferences, shared agreements, and safe experimentation spaces



### Critical Patterns



-   Three file locations: `~/.claude/settings.json` (user), `.claude/settings.json` (project), `.claude/settings.local.json` (local)

-   Precedence order: local > project > user (most specific wins)

-   Commit `.claude/settings.json` to version control; add `.claude/settings.local.json` to `.gitignore`

-   Use user level for personal preferences, project level for team standards, local level for temporary experiments



### Common Mistakes



-   Deleting the `.claude/` directory thinking it's unimportantâ€”this resets all project configuration

-   Committing `.claude/settings.local.json` to git, exposing personal/temporary settings to the team

-   Using project level for personal experiments instead of local level (pollutes team standards)



### Connections



-   **Builds on**: CLAUDE.md for project context (Lesson 7), Skills for capability customization (Lesson 6)

-   **Leads to**: Part 5 detailed settings configuration and team workflow policies



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/AI-Tool-Landscape/claude-code-features-and-workflows/settings-hierarchy

</settings-hierarchy>









<hooks-and-extensibility>

-   [](/)

-   [Part 2: AI Tool Landscape](/docs/AI-Tool-Landscape)

-   [Chapter 5: Claude Code: Your First General Agent](/docs/AI-Tool-Landscape/claude-code-features-and-workflows)

-   Hooks and Extensibility



# Hooks and Extensibility



You're working on a project. Every time you start a new Claude Code session, you manually:



1.  Explain the project structure

2.  Remind Claude about your naming conventions

3.  Load environment variables

4.  Set up project context



By the third session today, you're frustrated. **Why can't Claude Code just do this automatically when a session starts?**



**That's what hooks solve.**



* * *



## What Are Hooks?



**Definition**: Hooks are automated scripts that run when specific events occur in Claude Codeâ€”like when a session starts, a file is edited, or a tool runs.



**Think of hooks as**: Event listeners that trigger actions automatically.



A hook has three parts:



1.  **Event**: What triggers the hook (like "SessionStart")

2.  **Condition** (optional): Which tools or files match

3.  **Action**: What command runs automatically



**Key benefit**: Automate repetitive tasks so you focus on creative work, not setup.



#### ðŸ’¬ AI Colearning Prompt



> "Explain what hooks are in Claude Code. Give 2-3 real-world examples where a hook would save time by automating a repetitive task."



* * *



## Hook Events: When Hooks Trigger



Claude Code recognizes four main event types that can trigger hooks:



### PreToolUse



Fires **before** Claude Code runs a command or tool.



**Example use case**: Validation hook that checks requirements before running a build script.



### PostToolUse



Fires **after** Claude Code completes a command or tool.



**Example use case**: Format code immediately after edit, or run tests after saving.



### SessionStart



Fires **when you open a Claude Code session**.



**Example use case**: Load environment variables, initialize project context, or run startup checks.



### SessionEnd



Fires **when you close a Claude Code session**.



**Example use case**: Cleanup tasks, save session logs, or sync project state.



#### ðŸŽ“ Expert Insight



> In AI-driven development, hooks automate the routine follow-ups that developers used to do manually. Your value isn't in running `npm test` after every editâ€”it's in understanding WHEN testing matters and what the results tell you about your design.



* * *



## Real-World Hook Examples



Here are three practical scenarios showing hooks in action:



**Scenario**



**Hook Type**



**Event Trigger**



**Action**



**Benefit**



**Code Formatting**



PostToolUse



After Claude edits a `.py` file



Run `black --line-length 88 file.py`



Consistent code style without manual intervention



**Test Validation**



PostToolUse



After any changes to `src/` directory



Run `pytest` and report results



Catch bugs immediately instead of later



**Environment Setup**



SessionStart



When new Claude Code session opens



Load variables from `.env` and run `source setup.sh`



Project context always ready without manual setup



Each hook saves time by automating what you would otherwise do manually after specific events.



#### ðŸ’¬ AI Colearning Prompt



> "Explain the difference between PreToolUse and PostToolUse hooks. Give 2 examples where each would be more appropriate than the other."



* * *



## Why Hooks Matter for Professional Workflows



Imagine you're part of a development team. Without hooks:



-   After every edit, someone manually runs formatters: **repetitive, error-prone**

-   Tests run on a schedule instead of immediately: **bugs discovered late**

-   Setup requires manual steps: **onboarding takes longer**



With hooks:



-   Code automatically formatted **immediately after edit**

-   Tests run **automatically whenever code changes**

-   Project environment **auto-loads on session start**



Hooks are about **automating the predictable parts of your workflow** so you focus on the strategic thinkingâ€”the parts only humans can do.



#### ðŸ¤ Practice Exercise



> **Think of a repetitive task** you've done recently (running tests, formatting code, deploying, checking linting). **Ask your AI**: "In Claude Code, how would a hook help automate this repetitive task? What event would trigger it?"



**Expected Outcome**: You'll understand how hooks map to your real workflows and why automation matters for productivity.



* * *



## Hands-On: Create Your First Hook



Let's create a simple SessionStart hook that displays a welcome message when you open Claude Code.



### Step 1: Create Settings File



Hooks are configured in `.claude/settings.json`. Create this file in your project:



```

mkdir -p .claudetouch .claude/settings.json

```



### Step 2: Add a SessionStart Hook



Edit `.claude/settings.json` and add:



```

{  "hooks": {    "SessionStart": [      {        "matcher": "startup",        "hooks": [          {            "type": "command",            "command": "echo 'Welcome to your project! Claude Code session started.'"          }        ]      }    ]  }}

```



**What this does**:



-   **"hooks"**: Top-level wrapper for all hook configurations

-   **"SessionStart"**: Event that triggers when session starts

-   **"matcher"**: "startup" filters when this hook should run

-   **"type"**: "command" means run a bash command

-   **"command"**: The actual command to run (echo a welcome message)



### Step 3: Test Your Hook



Close and restart Claude Code in this project:



```

exit  # if already in a sessionclaude

```



**What you should see**:



```

Welcome to your project! Claude Code session started.

```



The hook ran automatically when the session started!



### Step 4: Make It More Useful



Update `.claude/settings.json` to show project info:



```

{  "hooks": {    "SessionStart": [      {        "matcher": "startup",        "hooks": [          {            "type": "command",            "command": "echo \"Project: $(basename $(pwd)) | Files: $(ls -1 | wc -l) | Last modified: $(ls -lt | head -2 | tail -1 | awk '{print \\$6, \\$7, \\$8}')\""          }        ]      }    ]  }}

```



Restart Claude Code:



```

exitclaude

```



**Now you see**:



```

Project: my-project | Files: 15 | Last modified: Nov 14 10:23

```



Useful project context **automatically** every session!



### Step 5: Co-Design a Hook for Your Workflow (Optional but Recommended)



Now that you understand hooks, let's collaborate with Claude Code to design one for YOUR specific needs.



**ðŸ¤ Practice Exercise: Three Roles Co-Design**



Ask Claude Code:



```

"I want to create a hook that automates [YOUR SPECIFIC TASK].What event should trigger it? What command should run?What should I watch out for?"

```



**What happens in this collaboration**:



1.  **AI as Teacher**: Claude suggests appropriate hook design patterns you might not have considered

2.  **AI as Student**: You provide context about your specific workflow that Claude doesn't know

3.  **AI as Co-Worker**: Together you refine the design, converging on a hook that actually solves your problem



This is the Three Roles Framework in actionâ€”not just "asking AI to do something," but genuine collaboration where both you and Claude Code learn from each other.



**Try it now** with a real automation you need in your current project.



* * *



## Common Questions About Hooks



### "Aren't hooks the same as Skills or Plugins?"



Noâ€”each serves a different purpose:



-   **Hooks**: Automate Claude Code's behavior in response to events (like "format code after edit")

-   **Skills**: Extend Claude's knowledge and capabilities with new functions or custom commands (like "extract PDF forms")

-   **MCP**: Connect external tools to Claude Code (like "access GitHub repositories")

-   **Plugins**: Bundle all of the above together as complete customizations (marketplace packages)



Think of it this way: Skills and MCP add **new capabilities**. Hooks add **automation**. They're complementary, not overlapping.



### "Will I encounter hook errors?"



Possibly, but they're usually non-blocking. If a hook fails:



-   Claude Code continues working (hook errors don't stop your session)

-   The error is logged, not shown as a blocker

-   You can investigate later



Debugging hooks is advanced content (Part 5). For now, just know hook errors won't stop you from working.



### "Should I enable hooks right now?"



Not necessary. Claude Code works perfectly fine without custom hooks. You can:



-   Use Claude Code productively without any hooks

-   Learn about hooks conceptually now

-   Build hooks later when you're ready to optimize your workflow



**No pressure to customize right nowâ€”focus on mastering basic workflows first.**



* * *



## Try With AI



Let's explore how hooks can automate repetitive tasks in your development workflow.



**ðŸ’¡ Understand Hook Fundamentals:**



> "Explain hooks in Claude Code. Give me 3 concrete examples where a hook would save time by automating a repetitive task I currently do manually. For each example, explain: what event triggers the hook, what the hook does automatically, and how much time it saves."



**ðŸ” Identify Automation Opportunities:**



> "I frequently \[describe your repetitive task: run tests before committing / format code after editing / check linting / load environment variables / deploy to staging\]. Could a Claude Code hook automate this? If so, what would the hook do? Which event would trigger it? Walk me through what the automation would look like."



**ðŸŽ¯ Design Your First Hook:**



> "Based on my workflow, help me design a simple hook to automate \[your most annoying repetitive task\]. What would the hook configuration look like? What command would it run? What event should trigger it? What could go wrong and how do I troubleshoot it?"



**ðŸš€ Plan Your Learning Path:**



> "I want to learn how to build custom hooks. What are the prerequisites? When in this book will I learn this? How hard is it compared to what I've learned so far (CLAUDE.md, MCP servers, subagents, skills)? Give me a roadmap for mastering Claude Code extensibility."



### Core Concept



Hooks are automated scripts that run when specific events occur in Claude Codeâ€”eliminating repetitive setup and follow-up tasks so you focus on creative work instead of manual routines.



### Key Mental Models



-   **Event-Trigger-Action**: When X happens (session starts, file edited, tool runs) â†’ Y command executes automatically

-   **Automation vs. Capabilities**: Hooks add *automation* to workflows; Skills and MCP add *new capabilities*â€”they're complementary, not overlapping

-   **Predictable â†’ Strategic**: Automate the predictable parts of your workflow to free attention for the thinking only humans can do



### Critical Patterns



-   Four hook events: `PreToolUse` (before tool runs), `PostToolUse` (after tool completes), `SessionStart` (session opens), `SessionEnd` (session closes)

-   Hooks live in `.claude/settings.json` with structure: event type â†’ matcher (optional) â†’ command action

-   Start simple: a SessionStart hook that echoes project context gives immediate value

-   Co-design custom hooks through AI collaboration: you provide workflow context, AI suggests patterns, iterate together



### Common Mistakes



-   Confusing hooks with skills/MCP/pluginsâ€”hooks automate behavior, they don't add new capabilities

-   Feeling pressure to customize immediatelyâ€”Claude Code works perfectly without hooks; master basic workflows first

-   Worrying that hook errors will break your sessionâ€”they're non-blocking and logged for later investigation



### Connections



-   **Builds on**: CLAUDE.md configuration, MCP external connections, Skills custom commands

-   **Leads to**: Advanced hook building and debugging (Part 5)



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/AI-Tool-Landscape/claude-code-features-and-workflows/hooks-and-extensibility

</hooks-and-extensibility>









<claude-code-plugins>

-   [](/)

-   [Part 2: AI Tool Landscape](/docs/AI-Tool-Landscape)

-   [Chapter 5: Claude Code: Your First General Agent](/docs/AI-Tool-Landscape/claude-code-features-and-workflows)

-   Discovering and Using Claude Code Plugins



# Discovering and Using Claude Code Plugins



You've learned to create skills, use subagents, and connect MCP servers. But what if someone has already built exactly what you need?



**Scenario**: You want Claude to help with:



-   Creating visual designs (canvas art, diagrams)

-   Building web apps with React components

-   Testing web applications automatically

-   Following your company's brand guidelines



**Question**: Should you build these capabilities from scratch, or use what already exists?



**Answer**: Use existing plugins from marketplaces.



* * *



## What Are Plugins?



**Definition**: A plugin is a **bundled package** that can include:



1.  **Skills** (autonomous capabilities Claude discovers)

2.  **Subagents** (specialized AI assistants)

3.  **Hooks** (event-driven automation)

4.  **Commands** (slash commands like `/design`)

5.  **MCP configuration** (external integrations)



**Think of plugins as**: Pre-built toolkits that extend Claude's capabilities in specific domains (design, testing, development, enterprise workflows).



**Key insight**: You don't need to build everything yourself. Marketplaces provide ready-to-use plugins created by Anthropic and the community.



#### ðŸ’¬ AI Colearning Prompt



> "Explain how plugins bundle multiple Claude Code components (skills, agents, hooks, MCP). Why is bundling better than installing each component separately?"



* * *



## Discovering Plugins: Anthropic's Skills Marketplace



The easiest way to extend Claude Code is using **Anthropic's official skills repository**â€”a curated collection of pre-built capabilities.



### What's Available in the Skills Repository?



The Anthropic skills marketplace provides **two main plugin bundles**:



**1\. example-skills** - Creative and development capabilities (canvas design, web testing, communications, and more)



**2\. document-skills** - Document processing suite (Word, PDF, PowerPoint, Excel)



### How to Add the Skills Marketplace



In Claude Code, run:



```

/plugin marketplace add anthropics/skills

```



**What happens**:



1.  Claude Code connects to [https://github.com/anthropics/skills](https://github.com/anthropics/skills)

2.  Downloads the marketplace configuration

3.  Marketplace appears as **"anthropic-agent-skills"** in your plugin list



### Browse and Install Plugins



After adding the marketplace, run `/plugin` to see an interactive menu that guides you through browsing and installing available plugins.



### Install a Plugin Bundle



Use the `/plugin` UI to browse and install, or run directly:



```

/plugin install example-skills@anthropic-agent-skills

```



Claude Code downloads the skills bundle and installs it to `.claude/skills/`, making all skills immediately available.



### Test Your Installed Skills



After installing `example-skills`, try asking Claude to create a visual diagram. The canvas-design skill will activate automatically when it detects visual/design requests.



#### ðŸ¤ Practice Exercise



> **Ask your AI**: "I just installed the example-skills plugin from Anthropic. List what capabilities this plugin provides. Then help me create a simple visual diagram showing how plugins, skills, and subagents relate to each other."



**Expected Outcome**: You'll understand what the plugin bundle includes and see a skill in action (canvas-design creating a visual).



* * *



## Plugin Marketplaces: Beyond Anthropic



### Adding Custom Marketplaces



You can add marketplaces from:



**GitHub repositories**:



```

/plugin marketplace add owner/repo

```



**GitLab or other git services**:



```

/plugin marketplace add https://gitlab.com/company/plugins.git

```



**Local paths** (for development):



```

/plugin marketplace add ./my-marketplace

```



**Direct URLs**:



```

/plugin marketplace add https://url.of/marketplace.json

```



### List Installed Marketplaces



```

/plugin marketplace list

```



**What you'll see**:



-   All marketplaces you've added

-   Source URLs

-   Number of plugins available from each



* * *



## When to Use Existing Plugins vs. Create Custom



**Use existing plugins** when standard capabilities exist (design, testing, document processing). **Create custom** when your workflow is team-specific or no existing plugin matches your needs. Check marketplaces first.



#### ðŸŽ“ Expert Insight



> In AI-native development, knowing what already exists is more valuable than building from scratch. Check marketplaces firstâ€”leverage community intelligence before creating custom solutions. Reuse is strategic; reinvention is waste.



* * *



## Hands-On: Set Up the Anthropic Skills Marketplace



### Step 1: Add the Marketplace



```

claude/plugin marketplace add anthropics/skills

```



### Step 2: Install a Plugin Bundle



```

/plugin install example-skills@anthropic-agent-skills

```



### Step 3: Test the Installation



Ask Claude: "What skills do you have available?" You should see the installed skills listed.



**What you just did**: Added Anthropic's marketplace and installed production-quality skills that are now available across all Claude Code sessions.



* * *



## Official Resources



**Anthropic Skills Repository**:



-   [https://github.com/anthropics/skills](https://github.com/anthropics/skills)

-   Browse all available skills

-   View skill source code and documentation

-   Learn from professional skill examples



**Plugin Marketplaces Documentation**:



-   [https://code.claude.com/docs/en/plugin-marketplaces](https://code.claude.com/docs/en/plugin-marketplaces)

-   How to create custom marketplaces

-   Team distribution strategies

-   Advanced marketplace configuration



**When You're Ready to Build**: After using existing plugins, you can learn to create custom ones in advanced content (Part 5+).



* * *



## Why This Matters: Composition as Organizational Practice



**Workflow Impact**: Plugins bundle everything you've learned (CLAUDE.md, MCP, Subagents, Skills, Hooks) into shareable, reusable packages.



**Paradigm Connection**: This is L4â€”spec-driven composition. You define WHAT you need (design tools, testing automation, document processing), install the plugin, and orchestrate the capabilities.



**Real-World Context**: Teams can distribute organizational intelligence as plugins. Your company's code review process, brand guidelines, deployment workflowsâ€”all packaged as plugins that work the same way for every team member.



**From scattered to unified**: Before plugins, skills lived in `.claude/skills/`, agents in `.claude/agents/`, MCP configs elsewhere. Plugins bundle everything into one installable package.



* * *



## Plugin Architecture: The Manifest



Every plugin has a `.claude-plugin/plugin.json` manifest listing its components:



```

{  "name": "feature-dev",  "version": "1.0.0",  "description": "Feature development workflow",  "components": {    "skills": [{ "path": "skills/code-quality-checker" }],    "commands": [{ "name": "code-review", "path": "commands/code-review.md" }],    "agents": [{ "name": "test-orchestrator", "path": "agents/test-orchestrator.md" }],    "hooks": [{ "trigger": "PreToolUse", "path": "hooks/pre-tool-validation.md" }],    "mcp_config": "mcp-servers.json"  }}

```



**A single plugin can bundle skills, commands, agents, hooks, and MCPâ€”everything you've learned in Lessons 1-8.**



* * *



## How Components Work Together



**Skills**: Claude discovers and uses autonomously **Commands**: You invoke explicitly with `/command-name` **Agents**: Specialized assistants for focused tasks **Hooks**: Event-triggered automation **MCP**: External integrations



**All bundled in one plugin** for easy sharing.



* * *



## Skill Best Practices Reference



**When you're ready to create custom skills** (covered in Lesson 6, advanced implementation in Part 5+), follow these patterns:



### 1\. Write Clear Descriptions



Include activation triggers and examples in your SKILL.md:



```

# DescriptionThis skill [what it does] by [how it does it].Use this skill when:- [Specific trigger scenario]- [Another trigger scenario]Examples:- "Help me [example request]"

```



### 2\. Keep Skills Focused



One skill, one workflow. Don't create "mega-skills"â€”Claude can compose multiple focused skills together.



### 3\. Start Simple, Iterate



V1: Basic description + happy path V2: Add error handling from real usage V3: Production-grade with validation



### 4\. Avoid Common Pitfalls



-   âŒ Vague descriptions ("helps with tasks")

-   âŒ Technology lock-in (hardcoded frameworks)

-   âŒ No examples (abstract triggers)



### 5\. Learn from Anthropic's Repository



Study production examples at [https://github.com/anthropics/skills](https://github.com/anthropics/skills) (canvas-design, web-app-testing, skill-creator)



* * *



## Try With AI



Let's integrate everything you've learned about Claude Code's extensibility (CLAUDE.md, MCP, subagents, skills, hooks, settings) into a cohesive workflow.



**ðŸ” Explore the Plugin Ecosystem:**



> "Show me how to install and use a skill plugin from the Anthropic skills repository (anthropics/skills). Walk me through: finding available plugins, installing one that seems useful, and testing it with a real task. How do I verify the plugin is active and working correctly?"



**ðŸ’¡ Understand the Decision Framework:**



> "I need Claude to help with \[describe your specific task\]. Should I: (a) use an existing plugin, (b) create a custom skill, (c) create a custom subagent, or (d) just ask Claude directly without customization? Walk me through the decision tree. What are the tradeoffs for each approach?"



**ðŸŽ¯ Design Your Customization Stack:**



> "Based on my workflow \[describe what you do: web development / data analysis / content creation / etc.\], help me design a complete Claude Code customization stack. What should I put in CLAUDE.md? Which MCP servers should I enable? Should I create custom skills or subagents? What hooks would save me time? Give me a prioritized action plan."



**ðŸš€ Build Integration Workflow:**



> "I want to build a workflow that combines multiple Claude Code capabilities: use CLAUDE.md for project context, enable Playwright MCP for testing, create a custom 'test-generator' subagent, and add a pre-commit hook for linting. Help me design how these pieces work together. What's the setup order? How do they interact? What could go wrong?"



### Core Concept



Plugins are bundled packages that combine multiple Claude Code capabilitiesâ€”skills, agents, hooks, commands, and MCPâ€”into single installable units. Check marketplaces before building custom solutions; reuse is strategic, reinvention is waste.



### Key Mental Models



-   **Composition Over Creation**: Existing plugins solve common workflow needs (design, testing, document processing)â€”leverage community intelligence before building custom

-   **Bundled Intelligence**: A plugin's manifest declares its components, allowing one install to add skills, agents, hooks, commands, and MCP together

-   **Marketplace-First Discovery**: Start with Anthropic's skills repository (`anthropics/skills`) and custom marketplaces before considering custom development



### Critical Patterns



-   Add marketplaces: `/plugin marketplace add anthropics/skills`

-   Install bundles: `/plugin install example-skills@anthropic-agent-skills`

-   List available: `/plugin marketplace list`

-   Plugin manifest (`plugin.json`) declares all components with paths

-   Skills install to `.claude/skills/` and activate automatically when Claude detects matching requests



### Common Mistakes



-   Building capabilities from scratch without checking what already exists in marketplaces

-   Creating "mega-skills" that do too much instead of focused, composable skills Claude can combine

-   Writing vague skill descriptions without specific trigger scenarios and examples

-   Treating components as separate (skills here, agents there) instead of bundling related capabilities together



### Connections



-   **Builds on**: All Claude Code features (CLAUDE.md, MCP, subagents, skills, hooks, settings) from Lessons 1-8

-   **Leads to**: Custom plugin creation and distribution strategies (Part 5+)



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/AI-Tool-Landscape/claude-code-features-and-workflows/plugins-putting-it-all-together

</claude-code-plugins>











<monetizing-agent-expertise>

-   [](/)

-   [Part 2: AI Tool Landscape](/docs/AI-Tool-Landscape)

-   [Chapter 5: Claude Code: Your First General Agent](/docs/AI-Tool-Landscape/claude-code-features-and-workflows)

-   From Skills to Business: Monetizing Agent Expertise



# From Skills to Business: Monetizing Agent Expertise



You've spent this chapter learning to build powerful things:



-   **Skills** that encode your expertise

-   **MCP integrations** that connect to real data

-   **Subagents** that coordinate complex work

-   **Hooks and plugins** that extend functionality



But here's what most tutorials never tell you: **these aren't just productivity tools. They're products.**



Every Skill you create is intellectual property. Every MCP integration is a connection to real business value. The question isn't whether your work has valueâ€”it's how you capture that value.



* * *



## Skills Are Intellectual Property



Think about what a Skill actually is:



-   **Encoded expertise**: Your knowledge of a domain, written as instructions

-   **Tested workflows**: Patterns that work, refined through use

-   **Reusable logic**: Something that solves a problem repeatedly



Traditional consulting: You sell your time. You explain things to clients. They pay per hour. When you stop working, the income stops.



**Skill-based business**: You encode your expertise once. You sell it repeatedly. You improve it over time. It works while you sleep.



A Skill that automates financial audits isn't just a file. It's a product that can serve thousands of customers simultaneously.



* * *



## The Digital FTE Model



Here's the concept that changes how you price AI agents: the **Digital Full-Time Equivalent (FTE)**.



### What's an FTE?



In business, an FTE is a unit of work: one full-time employee working 40 hours per week. Companies budget in FTEs. "We need 3 FTEs for this project" means they need 3 full-time people.



### What's a Digital FTE?



An AI agent packaged and sold as if it were an employee. But with different economics:



Metric



Human Employee



Digital FTE



Hours per week



40



168 (24/7)



Monthly cost



$4,000-8,000+



$500-2,000



Ramp-up time



3-6 months



Instant



Sick days



Yes



No



Turnover



~20% annual



Never quits



Scaling



Hire and train more



Clone instantly



Consistency



Variable



99%+ reliable



### The CEO Pitch



Here's how you sell it:



> "For $1,500/month, you get a Digital Sales Development Rep that works 24/7, never takes breaks, and handles 10x the volume of a junior hire. A human SDR costs $5,000/month plus benefits, takes 3 months to ramp up, and works 40 hours a week."



The math is simple. The savings are obvious. CEOs approve this without long debates.



### Cost Per Task



The real advantage shows in cost-per-task:



Task



Human Cost



Digital FTE Cost



Savings



Qualify a lead



$3-5



$0.25-0.50



85-90%



Review a contract



$50-100



$2-5



95%



Generate a report



$20-40



$1-2



95%



Answer a support ticket



$5-10



$0.50-1



90%



This is why companies adopt AI agents. Not because they're new and shinyâ€”because they save money on work that needs to get done.



* * *



## Four Revenue Models



You have four main ways to make money from AI agents:



### 1\. Digital FTE Subscription ($500-2,000/month)



**How it works**: You host and manage the agent. The client pays monthly for access. They get a "Digital Accountant" or "Digital SDR" that handles their workflows.



**You provide**:



-   The agent (your Skills + MCP integrations)

-   Hosting and infrastructure

-   Maintenance and updates

-   Support when things break



**Best for**:



-   Clients who want hands-off solutions

-   Workflows that need ongoing operation

-   When you want recurring revenue



**Example**: A "Digital Contract Reviewer" that monitors a law firm's incoming contracts, extracts key terms, and flags risks. $1,200/month.



### 2\. Success Fee (Pay per Result)



**How it works**: You charge based on outcomes, not access. $5 per qualified lead. 2% of cost savings. $50 per document processed.



**You provide**:



-   The agent doing the work

-   Tracking and reporting on results

-   Proof that value was delivered



**Best for**:



-   When outcomes are measurable

-   High-trust client relationships

-   When you're confident in the agent's performance



**Example**: A lead qualification agent that charges $5 for every lead it identifies as sales-ready. High-volume clients pay more; you're incentive-aligned.



### 3\. License (Sell the Recipe)



**How it works**: You sell the Skills and agent code. The client runs it themselves. You get annual license fees.



**You provide**:



-   The Skills (SKILL.md files)

-   Documentation and setup guides

-   Maybe some initial training

-   Annual updates (for maintenance fees)



**Best for**:



-   Enterprise clients who need data to stay in-house

-   Healthcare, finance, defense (regulated industries)

-   When clients have their own infrastructure



**Example**: A compliance-checking Skill licensed to banks for $50,000/year. They run it on their servers with their data. You never see their sensitive information.



### 4\. Skill Marketplace (Volume Play)



**How it works**: Publish your Skills to platforms like OpenAI Apps. Users discover and adopt them. You earn from usage or subscriptions.



**You provide**:



-   The Skill/agent on the platform

-   Documentation and examples

-   Updates to stay competitive



**Best for**:



-   Reaching large audiences without sales teams

-   Building a brand in a niche

-   Testing product-market fit quickly



**Example**: A "Meeting Notes Summarizer" Skill published to the OpenAI Apps marketplace. 10,000 users at $10/month = $100,000/month revenue.



* * *



## The Agent Factory Business Model



Remember from Lesson 1: General Agents build Custom Agents. This creates a powerful business model.



### The Factory Workflow



```

Your Expertise (domain knowledge)        â†“Specification (clear description of what the agent should do)        â†“Claude Code (General Agent as builder)        â†“Custom Agent + Skills (your product)        â†“Revenue (subscription, license, marketplace, or success fee)

```



### Why This Works



1.  **Low cost to create**: Claude Code does most of the implementation work

2.  **High value to sell**: Agents that save hours are worth thousands

3.  **Infinitely scalable**: Digital products don't get tired

4.  **Compounding returns**: Each Skill you build can be reused and combined



### Case Study: Digital SDR



A startup built a Digital SDR (Sales Development Rep) using this model:



**Before**:



-   5-person sales team reaching 50 prospects/day

-   Cost: $40,000/month in salaries

-   Response time: 4-6 hours



**After**:



-   1 Digital SDR agent + 2 humans for complex deals

-   Cost: $8,000/month total

-   Response time: less than 2 minutes

-   Volume: 1,000+ prospects/day



**How they built it**:



1.  Wrote a spec for lead qualification (Lesson 4: CLAUDE.md)

2.  Used Claude Code to build the Skills (Lessons 5-6)

3.  Connected MCP servers to their CRM and email (Lesson 8)

4.  Ran it themselves initially, then deployed as Custom Agent later (Part 6)



**Result**: 80% cost reduction, 20x volume increase.



**Note**: The first three steps use skills from this chapter. The fourth step (Custom Agent deployment) comes from Part 6. But they started making money at step 3â€”running the Skills themselves and delivering results to clients.



* * *



## The OpenAI Apps Opportunity



Traditional enterprise sales requires:



-   6-month sales cycles

-   Large sales teams

-   Expensive marketing

-   Relationship building



The OpenAI Apps marketplace (chatgpt.com/apps) changes this:



-   **800+ million users** already on the platform

-   **1+ million businesses** actively looking for AI solutions

-   **Single-click adoption**â€”no procurement process

-   **Built-in trust**â€”OpenAI's brand does the heavy lifting



You don't need a sales team. You need a great agent, clear positioning, and the platform handles distribution.



This is the "App Store moment" for AI agents. Just as mobile apps created millionaires who understood the new distribution, AI agent marketplaces will create the next wave.



* * *



## What You Can Build Today (After Chapter 5)



You've finished Chapter 5. Here's exactly what you can do RIGHT NOW:



### Your Current Toolkit



What You Learned



What It Enables



**Claude Code**



A General Agent that can execute any digital task



**SKILL.md files**



Reusable expertise you can share or sell



**MCP integrations**



Connections to databases, APIs, file systems



**Compiled Skills**



Token-efficient expertise packs



**Subagents**



Delegation for complex multi-step work



**Hooks**



Custom automation triggered by events



### Three Things You Can Sell Today



**1\. Skill Packs (License Model)**



You can create a SKILL.md that solves a specific problem and license it to others.



Example: A "Financial Report Analyzer" Skill that:



-   Reads PDF financial statements

-   Extracts key metrics

-   Generates summary reports

-   Flags anomalies



Package it, document it, sell it for $500-5,000 as a license.



**2\. Done-For-You Services (Success Fee)**



Use Claude Code + your Skills to do work for clients. You run it; they get results.



Example: "I'll analyze your last 12 months of customer support tickets and give you a report on the top 10 issues, patterns by time of day, and suggested FAQ additions. $500 flat fee."



You use Claude Code with MCP connections to their data. They get a deliverable.



**3\. Consulting + Skill Handoff (Hybrid)**



Help a client set up Claude Code, create custom Skills for their workflow, train their teamâ€”then hand it off.



Example: "I'll spend 2 weeks building custom Skills for your sales process, connect them to your CRM via MCP, and train your team. $5,000."



They get a working system. You move on to the next client.



### What You CAN'T Do Yet (Until Part 6)



-   **Customer-facing agents**: Public apps that customers use directly

-   **Hosted agent APIs**: Agents running 24/7 on servers

-   **Production guardrails**: Safety controls for autonomous operation

-   **SDK-based agents**: OpenAI Agents SDK, Claude SDK, Google ADK



That's what Part 6 teaches. But you don't need Part 6 to start making money. The Skills and MCP integrations you have now are enough for consulting, licensing, and done-for-you services.



* * *



## The 30-Day Roadmap



Here's how to go from Skills to first revenue in 30 days:



### Week 1: Identify Your Opportunity



-   What do you know that others find difficult?

-   What tasks do people pay consultants to do?

-   What repetitive work wastes hours every week?



**Good candidates for Chapter 5 skills**:



-   Document analysis and summarization

-   Data extraction and formatting

-   Research and report generation

-   Content creation and editing

-   Process documentation

-   Competitive analysis



### Week 2: Build Your First Monetizable Skill



-   Write a SKILL.md for your chosen task

-   Add MCP connections if needed (database, API, files)

-   Test it on real examples

-   Refine until it's reliable



### Week 3: Package and Price



-   Document what the Skill does

-   Create before/after examples

-   Decide: License, service, or hybrid?

-   Set your price (start with what feels too high)



### Week 4: Find Your First Customer



-   Start with people you know

-   Offer a pilot at reduced rate

-   Get testimonials

-   Iterate based on feedback



* * *



## When NOT to Build Agent Businesses



Not every problem should be an AI agent product. Avoid:



**Irreversible high-stakes decisions**



-   Medical diagnoses that affect treatment

-   Legal advice that could cause harm

-   Financial decisions with major consequences



**Undefined success criteria**



-   "Make our customers happier" (too vague)

-   Problems where you can't measure if the agent worked



**Unstable data environments**



-   When the data sources change constantly

-   When integrations break frequently



**Relationship-critical interactions**



-   Negotiations requiring emotional intelligence

-   Situations where trust is the product



**Rule of thumb**: If a mistake costs more than the agent saves, use humans.



* * *



## Try With AI



Practice applying these concepts:



**ðŸ” Identify Your Expertise:**



> "What domain do I know well enough to encode into a Skill? Think about: What questions do people ask me repeatedly? What tasks do I do that others find difficult? What knowledge have I accumulated that's valuable?"



**ðŸ’° Calculate the Value:**



> "Pick one task I could automate. How much does it cost in human time today? If an agent could do it for 10% of that cost, what would the savings be per month? Per year?"



**ðŸ—ï¸ Design the Product:**



> "For my chosen task, which revenue model fits best? Why? What would I need to build (Skills, MCP connections, hosting) to make it work?"



**ðŸš€ Find the Customer:**



> "Who would pay for this? What's their job title? What budget do they control? How would I explain the value in one sentence?"



* * *



## Summary



The Skills and MCP integrations you've learned in this chapter aren't just technical exercises. They're the building blocks of a business:



1.  **Skills are intellectual property** that can be sold repeatedly

2.  **Digital FTEs** reframe agents as employees with better economics

3.  **Four revenue models** (subscription, success fee, license, marketplace) fit different situations

4.  **The Agent Factory** model lets you create products faster than traditional development

5.  **Distribution platforms** like OpenAI Apps eliminate the need for large sales teams



The question isn't whether AI agents will transform work. The question is: will you be selling them, or competing against them?



* * *



## What's Next



In Part 6 (Chapter 33 and beyond), you'll learn to build production Custom Agents using SDKs like OpenAI Agents SDK and Google ADK. You'll go from Skills (expertise packs) to full agent applications with guardrails, orchestration, and deployment.



The business models you learned here apply directly to those agents. The difference: Custom Agents give you more control, reliability, and scalability for customer-facing products.



Your journey from learner to builder to business owner is just beginning.



---

Source: https://agentfactory.panaversity.org/docs/AI-Tool-Landscape/claude-code-features-and-workflows/from-skills-to-business

</monetizing-agent-expertise>









<team-needs-shared-rules>

-   [](/)

-   [Part 4: SDD-RI Fundamentals](/docs/SDD-RI-Fundamentals)

-   [Chapter 13: Understanding Spec-Driven Development with Reusable Intelligence](/docs/SDD-RI-Fundamentals/specification-driven-development-fundamentals)

-   Your Team Needs Shared Rules: Memory Banks and Constitutions



# Your Team Needs Shared Rules: Memory Banks and Constitutions



## The Problem You Haven't Faced Yet



Imagine you're on a team of 5 developers. Each person writes their own password reset feature for different projects.



Developer A uses bcrypt (secure) Developer B uses MD5 (insecure) Developer C doesn't hash at all (catastrophic)



Each developer has a spec for their password reset system. Each spec says "use secure hashing." But they interpreted it differently.



**Result**: Your system is inconsistent and insecure.



* * *



## The Solution: Shared Rules That Apply to Everything



Instead of assuming everyone knows "password reset should be secure," you write rules that apply to EVERY feature.



These rules have names: **Memory Banks** (Kiro framework) or **Constitutions** (Spec-Kit framework).



**They're basically: the rules that govern your entire system.**



### How to Scale Specs Across Teams



**The Challenge**: You're on a team of 5 developers. Each person writes different features. How do you ensure everyone follows the same security, architecture, and quality standards without constant meetings?



**The Solution**: Create a **Memory Bank** (Kiro framework) or **Constitution** (Spec-Kit framework). This document lists rules that apply to **every feature**:



-   ALL passwords use bcrypt

-   ALL APIs are rate-limited

-   ALL code has 80%+ test coverage

-   ALL data is encrypted in transit



Every developer reads this before writing code. Every AI agent follows these rules. **Consistency becomes automatic.**



![Flow diagram showing how constitution principles cascade through specifications to ensure quality and consistency across team development](https://pub-80f166e40b854371ac7b05053b435162.r2.dev/books/ai-native-dev/static/images/part-4/chapter-13/constitution-cascade-quality-flow.png)



* * *



### Constitutions Enforce Quality at Scale



Beyond consistency, Constitutions ensure **every spec meets minimum quality standards**.



**Without Constitution**: Each developer decides what "good spec" means



-   Some specs define edge cases, others don't

-   Some specify error handling, others assume happy path

-   Quality varies across team (inconsistent, unpredictable)



**With Constitution**: Global quality rules apply to ALL specs



-   "All API specs MUST define error response format"

-   "All authentication specs MUST specify token expiry and rotation policy"

-   "All data validation specs MUST list edge cases and rejection responses"

-   Quality becomes predictable and measurable



* * *



## What Goes in a Memory Bank / Constitution?



### 1\. Product Vision



```

We're building a healthcare scheduling platform. Our core promise:"Scheduling in under 30 seconds. No phone calls, no back-and-forth."

```



**Why**: Developers know what problem they're solving. Decisions align with vision.



### 2\. Architecture Patterns



```

- All endpoints follow FastAPI patterns- All services use repository pattern for data- All databases accessed through SQLAlchemy ORM- All errors follow standard error response format

```



**Why**: New developers don't reinvent wheels. Consistency.



### 3\. Technology Stack



```

Backend: Python 3.13+, FastAPIDatabase: PostgreSQL (primary), Redis (cache)Testing: PytestDeployment: Docker + Kubernetes

```



**Why**: Developers know what tools they're using. No tool debates.



### 4\. Security Rules (Non-Negotiable)



```

- ALL user data encrypted at rest (AES-256)- ALL data in transit over TLS 1.3+- ALL passwords hashed with bcrypt (cost 12+)- NEVER log passwords, tokens, or sensitive data- ALL endpoints require authentication (JWT)

```



**Why**: Security is default, not an afterthought. No vulnerable implementations.



### 5\. Quality Standards



```

CODE QUALITY:- Minimum test coverage: 80% per file- All functions have docstrings- All code formatted with Black (automatic)- Type hints on all functions (mypy strict mode)SPECIFICATION QUALITY (prevents quality gaps):- All API specs MUST define error response format (not just happy path)- All authentication specs MUST specify token expiry and rotation- All data validation specs MUST document edge cases and rejections- All feature specs MUST list non-functional requirements (performance, security)

```



**Why**: Quality is measurable at both spec AND code level. Bugs prevented at spec time, not caught in testing.



### 6\. Common Patterns and Anti-Patterns



```

DO THIS: Use service + repository pattern  service calls â†’ repository calls â†’ databaseDON'T DO THIS: Database calls scattered through endpoints  (makes code hard to maintain)

```



**Why**: Developers learn patterns by example.



* * *



## How It Works in Practice



### Scenario: Developer writes password reset feature



**Step 1: Read the Constitution**



Developer reads:



```

- Passwords MUST use bcrypt (cost 12+)- NEVER log sensitive data- ALL endpoints require rate limiting- ALL code must have 80%+ tests

```



**Step 2: Write spec aligned with Constitution**



Developer writes password reset spec:



```

## Non-Functional Requirements- Password hashing: bcrypt cost 12+ (per Constitution)- Rate limiting: 5 attempts per hour (per Constitution)- No logging of tokens (per Constitution)- Test coverage: 80%+ (per Constitution)

```



**Step 3: Generate code**



Code is generated. It automatically:



-   âœ… Uses bcrypt (no option to use MD5)

-   âœ… Implements rate limiting (required by Constitution)

-   âœ… Never logs tokens (Constitution rule enforced)

-   âœ… Has tests for 80%+ coverage (Constitutional requirement)



**Step 4: Code review**



Reviewer checks: "Does code follow Constitution?" Always yes, because Constitution was enforced in step 1-3.



* * *



### Encoding Lessons Learned: Bug â†’ Rule



But Constitutions aren't written once and then forgotten. They evolve as your team learns.



**Pattern**: Production bug discovered â†’ Root cause: vague specification â†’ Add constitutional rule â†’ Future specs prevent the same bug



**Example: The Forgotten Token Expiry**



```

INCIDENT: Leaked password reset token doesn't expire  Users found: Reset tokens from weeks ago still work (critical security gap)ROOT CAUSE: Original spec didn't specify token lifetime  Spec said: "Create password reset token"  Never said: "Token expires after X minutes"CONSEQUENCE: Generated code has no expiry logic. Tests don't check expiry.CONSTITUTIONAL FIX:+ Add to Constitution, under Authentication:  "All temporary access tokens (password reset, email verification, etc.)   MUST specify time-based expiry. Default: 30 minutes unless otherwise justified."RESULT: Future password reset specs automatically include token expiry.         Generated code includes expiry logic.         Team learns from mistake: quality rule prevents recurrence.

```



This is **organizational learning**. Your team encounters a gap once, encodes it as a rule, and prevents that gap from happening again in 50 other features across 10 teams.



* * *



## The Power at Scale



**Without Constitution:**



-   5 developers

-   Each makes security decisions independently

-   Some use bcrypt, some use MD5, some use nothing

-   Security is chaotic

-   Code review has to check everything



**With Constitution:**



-   5 developers

-   Constitution says "bcrypt always"

-   All developers implement bcrypt (no debate)

-   Security is consistent

-   Code review can focus on logic, not security basics



Now imagine 50 developers, or 500 developers. Constitution doesn't scale linearly. It scales exponentially: **More developers â†’ more need for shared rules.**



#### ðŸ’¬ AI Colearning Prompt



> "Why do teams with Constitutions (shared rules) scale better than teams without them? What breaks down in large teams when everyone makes independent security or architecture decisions?"



* * *



## The Key Insight



**Specifications are feature-specific. Constitutions are system-wide.**



-   **Spec**: "How should password reset work?"

-   **Constitution**: "How should ALL code handle security, quality, testing?"



Specs drive individual features. Constitutions ensure consistency across ALL features.



* * *



## Beyond Constitution: Capturing the Journey and Decisions



A Constitution defines **what rules we follow**. But as your team develops software, two other critical questions emerge:



1.  **How did we learn what works?** (The journey of discovery)

2.  **Why did we choose this approach?** (The rationale behind mutable decisions)



Two additional artifacts address these questions: **Prompt History Records (PHR)** and **Architectural Decision Records (ADR)**.



* * *



### Prompt History Records (PHR): Capturing the Journey



**The Problem**: When AI-generated code fails, how do you debug it? When a spec works perfectly, how do you learn why it succeeded? Without a record of your AI interactions, knowledge is lost.



**The Solution**: PHR = Structured log of all AI interactions during development.



#### Why PHRs Matter



**1\. Debugging**: When generated code doesn't work as expected



```

Problem: Password reset emails not sendingâ†“Check PHR: What did we ask AI to generate?â†“PHR shows: "Generate password reset with email notification"â†“Insight: We never specified SMTP configuration in the spec!â†“Solution: Update spec with email configuration details

```



**2\. Learning**: Patterns emerge showing which prompts produce better results



```

PHR Analysis after 3 months:- Prompts with explicit error handling: 95% success rate- Prompts without error handling: 60% success rateâ†’ Team learns: Always specify error handling in specs

```



**3\. Collaboration**: Team members understand reasoning path, not just final code



```

New developer joins team:"Why did we use JWT instead of sessions for password reset?"â†“Check PHR from that feature:â†“Shows full discussion: Security Subagent recommended JWT,team discussed trade-offs, chose JWT for stateless scalingâ†“New developer understands context without meeting

```



* * *



### Architectural Decision Records (ADR): Documenting Mutable Decisions



**Constitution = Immutable**: "ALL passwords use bcrypt" (never changes)  

**ADR = Mutable**: "For this feature, we chose JWT over sessions because..." (might change)



#### Understanding the Difference



**Constitution**: System-wide principles that rarely change



-   "All passwords use bcrypt cost 12+"

-   "Test coverage must exceed 80%"

-   "All data encrypted in transit"



**ADR**: Feature-specific decisions that might evolve



-   "Use JWT for password reset tokens" (might switch to sessions later)

-   "30-minute token expiry" (might adjust based on user feedback)

-   "Email-only reset" (might add SMS backup in future)



#### When to Write an ADR



Write an ADR for:



-   âœ… Significant architecture decisions (database choice, auth pattern, API design)

-   âœ… Trade-offs between competing approaches (performance vs simplicity)

-   âœ… Deviations from obvious/standard patterns (why we didn't use the common approach)

-   âœ… Decisions that future developers will question ("Why did they choose this?")



Don't write an ADR for:



-   âŒ Decisions covered by Constitution (those are already documented)

-   âŒ Trivial implementation details (variable naming, minor refactoring)

-   âŒ Temporary workarounds (document in code comments instead)



## The Power at Scale Revisited



**Without SDD**:



-   5 developers make independent decisions

-   Some use JWT, some use sessions, some use both

-   No one knows why decisions were made

-   New developers ask same questions repeatedly

-   AI agents make inconsistent choices



**With SDD and an Opinionated Tools:**



-   Constitution: "Auth pattern is JWT" (everyone follows)

-   ADR: "Why JWT? Because..." (everyone understands)

-   Spec: "This feature uses JWT per ADR-001" (consistent implementation)

-   PHR: "AI generated this based on ADR-001" (traceable)

-   New developers: Read ADRs, understand context, continue pattern



**Result**: Consistency emerges not just from rules (Constitution), but from shared understanding (ADRs) and institutional learning (PHRs).



#### ðŸŽ“ Expert Insight



> In AI-native development, your Constitution isn't just documentationâ€”it's executable memory. Every AI agent reads it before generating code. Every human reads it before writing specs. The Constitution becomes your organization's shared brain, preventing the same security mistake from happening in 50 different features across 10 different teams.



* * *



## Your Reflection



**Questions:**



1.  **What rules would YOUR Constitution include?**

    

    -   Security rules for your domain?

    -   Architecture patterns you want everyone to follow?

    -   Quality standards?

    -   Technology choices?

2.  **How would a Constitution change your team's work?**

    

    -   Less debate about "should we use bcrypt or MD5?" (Constitution says bcrypt)

    -   Faster code review (Constitution compliance checked automatically)

    -   More consistent codebase

    -   Easier onboarding (new devs read Constitution, understand rules)

3.  **Where do specs and Constitution meet?**

    

    -   Specs implement the Constitution

    -   Constitution enforces quality across all specs

    -   No conflict: they work together



#### ðŸ¤ Practice Exercise: Encode Quality Rules



> **Ask your AI**: "Help me draft a Constitution for a \[describe your domain: e-commerce, fintech, healthcare, etc.\] application. What security rules should apply to every feature? What architecture patterns should be mandatory? **What quality gaps has our team experienced repeatedly?** What specification quality rules would prevent those gaps?"



**Two-part exercise**:



1.  **Domain-Specific Rules**: Ask AI to suggest security (PCI-DSS for payments, HIPAA for healthcare, GDPR for EU data) and architecture rules.

    

2.  **Quality-at-Scale Rules**: Ask AI to help you identify and encode specification quality standards:

    

    -   "What should every API spec specify?" (error handling, status codes, rate limits)

    -   "What should every authentication spec require?" (token expiry, refresh rotation, session management)

    -   "What should every data validation spec include?" (edge cases, rejection responses, format validation)



**Expected Outcome**: Constitution that prevents common quality gaps before they become production bugs. Your team learns from past mistakes by encoding them as rules.



* * *



**Professional teams don't debate fundamentals every project.**



They write down the rules (Constitution / Memory Bank). Everyone follows them. Consistency emerges.



This is how teams scale without chaos.



* * *



## What Happens Next: From Patterns to Reusable Intelligence



You now understand how Constitutions enforce quality across team specs. But you're noticing something:



**The same patterns repeat**:



-   Every API spec defines endpoints, error handling, authentication

-   Every authentication spec specifies tokens, expiry, rotation

-   Every data validation spec documents edge cases, rejections

-   Every feature spec includes the same quality checklists



**Question**: If the same patterns show up in 30 different specs, could those patterns become **reusable**?



**Lesson 6 introduces Reusable Intelligence (RI)**â€”where specification patterns become Skills and Subagents that your team and AI agents use to write better specs faster.



Constitution + Reusable Intelligence = Teams that scale without losing quality.



* * *



## Try With AI



Ready to apply constitutional thinking to your work? Explore these prompts:



**ðŸ” Explore Constitution Design:**



> "What's the difference between a rule that belongs in a Constitution (applies to everything) vs. a rule that belongs in an ADR (applies to one decision)? Give me 5 examples of each for a web application."



**ðŸŽ¯ Practice Drafting Rules:**



> "I'm building a social media platform. Help me draft constitutional rules for: (1) Data privacy, (2) Content moderation, (3) API rate limiting, (4) Testing standards. What should be mandatory across ALL features?"



**ðŸ§ª Test Constitutional Enforcement:**



> "Here's a feature spec for user authentication \[paste or describe\]. Does it comply with this Constitution rule: 'ALL passwords use bcrypt cost 12+'? If not, what's missing?"



**ðŸš€ Apply to Your Organization:**



> "My team of \[X people\] is building \[describe project\]. We keep having debates about \[describe recurring issue\]. Should this be a Constitutional rule or handled case-by-case? Help me decide."



* * *



### Core Concept



A Constitution (or Memory Bank) is a set of system-wide rules that apply to ALL specifications, ensuring consistency across teams without constant meetings or debates.



### Key Mental Models



-   **System-wide vs Feature-specific**: Specifications define how individual features work; Constitutions define universal rules applying to everything

-   **Consistency at scale**: As team size grows from 5 to 50 to 500 developers, shared rules prevent security/architecture/quality chaos

-   **Organizational learning**: Production bugs â†’ root cause analysis â†’ constitutional rule â†’ future specs automatically prevent recurrence

-   **Triple documentation layer**: Constitution (immutable global rules) + ADRs (why we chose this decision) + PHRs (how we learned it)



### Critical Patterns



-   **Constitution structure**: Product vision, architecture patterns, technology stack, security rules, quality standards, common patterns

-   **Spec compliance to Constitution**: Every spec references constitutional rules as "per Constitution" rather than redefining basics

-   **Bug-to-rule encoding**: When security/quality gaps appear in production, convert to constitutional rule preventing same gap across 50 features

-   **Code review simplification**: When Constitution is enforced in specs, code review focuses on logic, not security/architecture basics



### AI Collaboration Keys



-   AI agents read Constitution before generating code (ensures consistent implementations)

-   Team members reference Constitution when writing specs (prevents ad-hoc decisions)

-   Constitutions encode expert knowledge that humans might forget or vary



### Common Mistakes



-   Writing Constitution too specific (should apply broadly, not lock teams into single pattern)

-   Treating Constitution as static (must evolve as team learns from production incidents)

-   Forgetting that Constitution is enforcement mechanism, not documentation (specs must reference rules explicitly)



### Connections



-   **Builds on**: SDD fundamentals (clear specifications as foundation)

-   **Leads to**: Reusable Intelligence (patterns that repeat across specs become Skills/Subagents), ADRs (documenting why constitutional decisions were made), PHRs (tracking how team learned each constitutional lesson)



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/SDD-RI-Fundamentals/specification-driven-development-fundamentals/your-team-needs-shared-rules

</team-needs-shared-rules>











<reusable-intelligence>

-   [](/)

-   [Part 4: SDD-RI Fundamentals](/docs/SDD-RI-Fundamentals)

-   [Chapter 13: Understanding Spec-Driven Development with Reusable Intelligence](/docs/SDD-RI-Fundamentals/specification-driven-development-fundamentals)

-   Introduction to Reusable Intelligence



# Introduction to Reusable Intelligence



In Lessons 1-5, you've mastered specification-driven development fundamentals:



-   **L1-3**: Write clear specifications (intent, requirements, constraints)

-   **L4**: Collaborate with AI to refine specifications

-   **L5**: Create Constitutions to ensure consistency across all specs



But specifications and constitutions alone don't capture the full value of SDD-RI. **The "RI" in SDD-RI stands for Reusable Intelligence**â€”and it represents a fundamental shift in what we consider valuable in software development.



This lesson introduces the three components of Reusable Intelligence and explains why they've become strategic assets in AI-native development.



* * *



## From Reusable Code to Reusable Intelligence



### The Traditional Model: Code as Primary Asset



For decades, software engineering organized around **human-authored source code** as the canonical representation of system behavior. Reusability meant:



-   **Modular libraries and frameworks** (import React, use Stripe SDK)

-   **Design patterns** that encode proven solutions (Singleton, Factory, Observer)

-   **Abstraction hierarchies** that minimize duplication (base classes, interfaces)

-   **Components designed for composition** (plug-and-play modules)



The goal: **Write code once, reuse everywhere**. The value: Code was the primary asset.



**What shaped this model**:



-   Version control systems (Git tracks code changes)

-   Software architecture patterns (MVC, microservices organize code)

-   Career paths (junior developer â†’ senior developer â†’ architect)

-   Educational curricula (learn to write better code)



* * *



### The Shift: AI Commoditizes Implementation



The maturation of Large Language Models and AI coding agents introduces capabilities that challenge code-centric assumptions:



**What AI can now do**:



-   Generate code on-demand from natural language descriptions

-   Refactor automatically while preserving behavior

-   Synchronize implementations continuously as requirements evolve

-   Migrate across frameworks with reduced manual intervention



**The bottleneck shifts**:



-   **Old bottleneck**: "Writing code" (skilled human labor, time-intensive)

-   **New bottleneck**: "Expressing intent with precision" (clear specifications, decision guidance)



**What this means**:



```

When AI can reliably generate idiomatic implementations from clear specifications,the bottleneck moves from "writing code" to "expressing intent with precision."

```



AI tools excel at producing implementations that match specified patterns, but **human judgment remains essential** for:



-   System architecture decisions

-   Performance optimization strategies

-   Security requirements and threat modeling

-   Domain-specific logic and business rules



* * *



### The New Paradigm: Intelligence as Strategic Asset



Instead of focusing exclusively on reusable code, organizations must now cultivate **reusable intelligence**â€”structured knowledge and decision-making capabilities that can be applied consistently across projects.



**The competitive advantage shifts**:



```

Two teams using similar AI models and programming languages may achievevastly different productivity based on how well they've structured theirspecifications and intelligence libraries.

```



**Consider the historical evolution of programming**:



Era



Abstraction



What Humans Write



What's Generated



**1950s-60s**



Machine code â†’ Assembly



Assembly instructions



Machine code (via assembler)



**1970s-90s**



Assembly â†’ High-level languages



C/Java/Python



Assembly (via compiler)



**2020s+**



High-level code â†’ Specifications + AI



Specs + Intelligence



Code (via AI agents)



In this emerging paradigm, **languages like Python and TypeScript serve increasingly as intermediate representations**â€”analogous to how assembly functions in compiled language workflows. The "source" shifts upward to specifications, constraints, and architectural decisions that guide AI-powered implementation.



**Critical qualification**: This transformation applies most strongly to certain categories:



-   âœ… Infrastructure code, API implementations, data pipelines, testing frameworks

-   âŒ Novel algorithms, performance-critical systems, domains requiring deep optimization



The spec-driven approach **complements** rather than replaces traditional development.



* * *



## What Is Reusable Intelligence?



**Reusable Intelligence manifests as three primary components:**



### Component 1: Skills (Horizontal Expertise)



**Definition**: Packaged expertise that can be broadly applied across many features and domains.



**Characteristics**:



-   **Horizontal applicability**: Works across different projects, domains, technologies

-   **Broadly useful**: "Every API needs error handling" (applies to all APIs)

-   **Guidance-based**: Provides decision frameworks, not rigid implementations

-   **Examples**: Logging patterns, error handling, input validation, API pagination



**What a Skill bundles**:



-   Custom instructions (how to think about this pattern)

-   Decision frameworks (when to choose option A vs B)

-   Templates and conventions (standard structures)

-   Reference documentation (why these decisions matter)



**Example Skills**:



-   **Error Handling Skill**: How to structure error responses, status codes, logging

-   **Input Validation Skill**: How to validate data at boundaries, handle invalid input

-   **API Pagination Skill**: How to design paginated endpoints, cursor vs offset



**Why "horizontal"?** These patterns apply broadlyâ€”error handling is relevant whether you're building authentication APIs, payment processors, or search endpoints.



* * *



### Component 2: Subagents (Vertical Specialization)



**Definition**: Specialized agents with focused expertise in specific domains, invokable from primary coding agents.



**Characteristics**:



-   **Vertical applicability**: Deep expertise in specific domain (security, performance, accessibility)

-   **Specialized focus**: Each subagent maintains domain expertise while accessing shared context

-   **Autonomous reasoning**: Makes independent judgments, not just checklist verification

-   **Examples**: Security auditors, performance analyzers, test generators, documentation curators



**What a Subagent provides**:



-   **Persona and behavioral profile**: Identity that shapes how it interprets tasks (e.g., "security auditor evaluating threat surfaces")

-   **Tooling and environment access**: Integration with relevant systems (code repos, build systems, testing frameworks)

-   **Domain expertise**: Specialized knowledge (OWASP Top 10 for security, Big-O analysis for performance)



**Example Subagents**:



-   **@security**: Reviews code for vulnerabilities, evaluates threat models, suggests defenses

-   **@performance**: Analyzes scalability, identifies N+1 queries, recommends caching strategies

-   **@tests**: Generates test suites, ensures coverage, identifies edge cases

-   **@docs**: Maintains documentation, ensures clarity, updates on code changes



**Why "vertical"?** These agents specialize deeply in one domainâ€”a security auditor focuses exclusively on security concerns across all features.



* * *



### Component 3: Orchestration Patterns (Multi-Agent Collaboration)



**Definition**: Workflows that coordinate multiple Skills and Subagents to solve complex problems.



**Characteristics**:



-   **Multi-agent coordination**: Combines horizontal Skills + vertical Subagents

-   **Workflow definition**: Defines sequence, dependencies, failure handling

-   **Systematic quality**: Ensures every feature goes through same validation pipeline

-   **Repeatable process**: Organizational memory of "how we build quality software"



**What an Orchestration Pattern defines**:



-   **Stage sequence**: Specification â†’ Design â†’ Implementation â†’ Validation

-   **Agent roles**: Which Subagents review which aspects

-   **Collaboration protocol**: How agents share context and findings

-   **Failure handling**: What happens when validation fails (iterate? block? escalate?)



**Example Orchestration**:



```

Feature Development WorkflowStage 1: Specification Design- Human writes initial spec- Apply Skills: Error Handling, Input Validation, AuthenticationStage 2: Architecture Review- @security subagent: Reviews for vulnerabilities- @performance subagent: Analyzes scalability- @accessibility subagent: Checks inclusive designStage 3: Implementation- AI generates code guided by spec + Skills + Subagent feedbackStage 4: Validation- @tests subagent: Generates test suite- Run tests â†’ If pass, proceed; If fail, refine and retryStage 5: Documentation- @docs subagent: Generates documentation from spec + codeResult: Every feature gets security review, performance analysis,accessibility check, comprehensive tests, and clear documentationâ€”systematically.

```



**Why "orchestration"?** Complex features need coordination across multiple expertise areas. Orchestration ensures no step is skipped.



* * *



## The Microservices Analogy



Designing agent systems parallels designing distributed systems:



**Microservices Architecture**:



-   Decompose application into manageable units

-   Each service has specific responsibility

-   Services communicate via API contracts

-   Compose services to solve complex problems



**Agent Architecture**:



-   Decompose problem-solving into modular intelligence units

-   Each agent/skill has specialized persona and expertise

-   Agents share context and findings

-   Compose agents to deliver comprehensive solutions



**Just as you wouldn't put all logic in one monolithic service, you don't put all intelligence in one generic "helpful AI."** You design specialized components that work together.



* * *



## Skills vs Subagents: Understanding the Distinction



Both Skills and Subagents are reusable intelligence, but they serve different purposes:



### When to Create a Skill (Horizontal Expertise)



**Recognition signals**:



-   Pattern repeats across many different features

-   Applies broadly without major customization

-   Provides guidance for common decisions

-   Answers recurring questions (2-4 decision points)



**Example**: Error Handling Skill



-   **Applies to**: Every API, every service, every endpoint

-   **Provides**: Decision framework for status codes, error format, logging

-   **Reusable across**: Authentication APIs, payment APIs, search APIs, analytics APIs

-   **Horizontal because**: Every feature needs error handling



* * *



### When to Create a Subagent (Vertical Specialization)



**Recognition signals**:



-   Requires deep domain expertise (security, performance, compliance)

-   Makes complex judgments (5+ interconnected decisions)

-   Needs autonomous reasoning (not just checklist)

-   Adapts analysis to context (different features need different scrutiny)



**Example**: Security Auditor Subagent



-   **Specializes in**: Security threat modeling, vulnerability detection

-   **Analyzes**: Threat actors, data sensitivity, compliance requirements, attack vectors

-   **Reasoning**: "This file upload endpoint â†’ HIGH RCE risk, requires sandboxing"

-   **Vertical because**: Deep security expertise, not needed for every aspect



* * *



### Comparison Table



Characteristic



Skills (Horizontal)



Subagents (Vertical)



**Scope**



Broad (many features)



Deep (one domain)



**Expertise**



Generalist patterns



Specialist knowledge



**Decision Points**



2-4 questions



5+ complex questions



**Autonomy**



Guidance (human decides)



Autonomous (agent decides)



**Application**



"All APIs need this"



"Security review needed here"



**Examples**



Error handling, Pagination, Validation



Security audit, Performance analysis, Accessibility review



* * *



## Why This Matters: The Strategic Value



### For Individuals



**Traditional career path**: Junior Developer â†’ Senior Developer â†’ Architect



-   Value: Write better code faster

-   Bottleneck: Coding skill and time



**AI-native career path**: Junior Developer â†’ Intelligence Designer â†’ Spec Architect



-   Value: Design better specifications and intelligence

-   Multiplier: AI executes at scale



**Emerging roles**:



-   **AI Systems Designer**: Designs agent architectures and workflows

-   **Intelligence Engineer**: Creates Skills and Subagents that encode expertise

-   **Spec Architect**: Designs specifications that guide AI generation effectively

-   **Agent Orchestrator**: Coordinates multi-agent systems for complex projects



* * *



### For Teams and Organizations



**Traditional engineering practices**:



-   Version control for code (Git)

-   CI/CD for code deployment

-   Code review for quality

-   Documentation of implementations



**AI-native engineering practices**:



-   Version control for **specifications and agent configurations**

-   CI/CD with **specification validation and agent orchestration**

-   Review for **specification quality and alignment**

-   Documentation of **intent, constraints, and decision frameworks**



**The shift in investment priorities**:



```

Old focus: Building code assets (libraries, frameworks)New focus: Curating intelligence libraries (Skills, Subagents, Orchestrations)Old asset: Python authentication library (works only in Python)New asset: Authentication Intelligence (generates Python, TypeScript, Go, Rust)

```



**Why this matters**: When new frameworks emerge, code becomes legacy. Intelligence persists.



* * *



### For the Industry



**The competitive advantage shifts**:



**Team A** (Traditional):



-   Uses AI to generate code from vague descriptions

-   Every developer makes ad-hoc decisions

-   Inconsistent quality across features

-   Knowledge lost when developers leave



**Team B** (SDD-RI):



-   Uses AI guided by comprehensive specifications + intelligence library

-   Skills ensure consistent patterns (error handling, validation)

-   Subagents provide expert review (security, performance)

-   Orchestration ensures systematic quality

-   Intelligence library accumulates and compounds over time



**Result**: Team B builds higher-quality software faster, despite using similar AI models and languages. **The differentiator is intelligence architecture.**



* * *



## The Three-Layer Knowledge Stack (Revisited)



Now you understand all three layers of SDD-RI:



### Layer 1: Specifications (WHAT to Build)



```

Purpose: Define system boundaries and requirementsExample: "Build user authentication with email/password"Captures: Intent, requirements, constraints, non-goalsMissing: Universal rules, decision frameworks, expert review

```



### Layer 2: Constitutions (UNIVERSAL RULES)



```

Purpose: Ensure consistency across ALL specificationsExample: "All passwords must use bcrypt hashing"Captures: Non-negotiable standards, governance rulesMissing: Context-specific decision guidance, domain expertise

```



### Layer 3: Reusable Intelligence (DECISION-MAKING EXPERTISE)



```

Purpose: Provide decision frameworks and expert reviewComponents:  - Skills (horizontal patterns)  - Subagents (vertical expertise)  - Orchestration (multi-agent workflows)Captures: How to make good decisions, expert reasoning, quality validation

```



**How they work together**:



```

Specification defines: "Build authentication"Constitution requires: "Use bcrypt hashing"Skills provide: Error handling pattern, Input validation patternSubagents review: @security (threat model), @performance (scalability)Orchestration ensures: All reviews happen, nothing skippedâ†’ Result: High-quality, expert-level authentication system

```



* * *



## Platform Generalization: Beyond Claude Code



While this lesson uses terminology from Claude Code (Skills, Subagents), the **underlying pattern is universal**:



**All major coding agents are converging toward**:



-   Separation of agent roles and capabilities

-   Reusable configuration rather than repeated prompting

-   Tool/skill composition for complex workflows

-   Specification-driven approaches



**Platform-specific terminology**:



-   **Anthropic Claude Code**: Skills + Subagents (most explicit)

-   **OpenAI**: Custom GPTs + Assistants + Function calling

-   **Google Gemini**: Extensions + Tools + Multi-agent patterns

-   **Microsoft**: Agent configurations + Semantic Kernel skills



**Universal concepts** (regardless of platform):



-   âœ… Specialized agent personas with focused expertise

-   âœ… Packaged capabilities that bundle knowledge and tools

-   âœ… Orchestration patterns for multi-agent collaboration

-   âœ… Reusable configuration files (not repeated prompts)



**The good news**: Even platforms without explicit "Subagent" constructs can achieve similar outcomes by designing **MCP servers** that act as specialized agents with domain skills.



* * *



## What You've Learned



You now understand:



âœ… **The paradigm shift**: From reusable code â†’ reusable intelligence âœ… **The three components**:



-   Skills (horizontal expertise, 2-4 decisions, guidance)

-   Subagents (vertical specialization, 5+ decisions, autonomous)

-   Orchestration (multi-agent workflows, systematic quality) âœ… **The microservices analogy**: Modular intelligence units with specialized roles âœ… **The strategic value**: Intelligence becomes organizational competitive advantage âœ… **Platform generalization**: Concepts apply across all AI coding platforms



* * *



## Try With AI



**Setup**: Open your AI coding assistant and explore the concept of reusable intelligence.



**Prompt Set**:



**Prompt 1 (Understanding the Shift)**:



```

I'm learning about the shift from reusable code to reusable intelligence.Help me understand:- In traditional development, I'd write an authentication library (code)- In AI-native development, I'd create authentication intelligence (specs + skills)What's the difference? Why is intelligence more valuable than code in the AI era?

```



**Prompt 2 (Skills vs Subagents)**:



```

I'm trying to understand the difference between Skills and Subagents.Skills (horizontal): Apply broadly, provide guidance (e.g., error handling)Subagents (vertical): Deep domain expertise (e.g., security auditor)Give me 3 examples of each:- 3 Skills that would apply across many features- 3 Subagents that would provide specialized expertise

```



**Prompt 3 (Recognizing Patterns)**:



```

I've worked on these features: [describe 2-3 recent features]What patterns repeated across these features?Which patterns would be good Skills (horizontal)?Which patterns would benefit from Subagent expertise (vertical)?

```



**Expected Outcomes**:



-   Clear understanding of why intelligence is the new strategic asset

-   Ability to distinguish Skills (horizontal) from Subagents (vertical)

-   Recognition of patterns in your own work worth capturing as RI



**Optional Stretch**:



```

Imagine I'm building a healthcare application with HIPAA compliance requirements.What Skills would I need? (horizontal patterns)What Subagents would I need? (specialized expertise)How would Orchestration ensure compliance?

```



* * *



### Core Concept



When AI generates code reliably from specifications, the bottleneck shifts from "writing code" to "expressing intent clearly," making reusable intelligence (specifications, decision frameworks, domain expertise) the new strategic asset instead of reusable code.



### Key Mental Models



-   **Paradigm shift**: From code-centric (libraries, frameworks) to intelligence-centric (specifications, decision guidance, expertise)

-   **Historical evolution**: Assemblyâ†’High-level codeâ†’Specifications+AI (where specs become "source" as AI becomes "compiler")

-   **Three components stack**: Specifications (WHAT) + Constitutions (universal rules) + Reusable Intelligence (HOW to decide)

-   **Microservices analogy for agents**: Just as you decompose monolithic services into focused microservices, you decompose problem-solving into specialized intelligence units (Skills for horizontal patterns, Subagents for vertical expertise)



### Critical Patterns



-   **Skills (Horizontal expertise)**: Broad patterns (error handling, input validation, pagination) applicable across all projects and domains

-   **Subagents (Vertical specialization)**: Deep domain expertise (security auditor, performance engineer) with 5+ autonomous decisions per feature

-   **Orchestration patterns**: Workflows coordinating Skills+Subagents ensuring systematic quality (specification â†’ design review â†’ implementation â†’ validation)

-   **Intelligence library as asset**: Organization accumulates Skills/Subagents over time, compounding productivity multiplier as library grows



### AI Collaboration Keys



-   AI agents access Skill library to apply consistent patterns across features

-   Subagents provide expert review (security, performance, accessibility) automatically

-   Specifications+Intelligence enable AI to produce production-grade code (not just functional code)



### Common Mistakes



-   Confusing Skills (guidance, 2-4 decisions) with Subagents (autonomous, 5+ decisions)

-   Treating intelligence as technology-specific (Python-only) instead of platform-agnostic (spec â†’ any language)

-   Building intelligence library without organizational discipline (fragments instead of library)



### Connections



-   **Builds on**: Constitution (global rules) establishes governance context

-   **Leads to**: Designing Skills/Subagents with P+Q+P (how to actually create intelligence components), framework selection (choosing between Kiro/Spec-Kit/Spec-Kit Plus/Tesel)



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/SDD-RI-Fundamentals/specification-driven-development-fundamentals/introduction-to-reusable-intelligence

</reusable-intelligence>











<p+q+p>

-   [](/)

-   [Part 4: SDD-RI Fundamentals](/docs/SDD-RI-Fundamentals)

-   [Chapter 13: Understanding Spec-Driven Development with Reusable Intelligence](/docs/SDD-RI-Fundamentals/specification-driven-development-fundamentals)

-   Designing Skills and Subagents with P+Q+P



# Designing Skills and Subagents with P+Q+P



You've explored specifications. You've identified recurring patterns that become Skills and Subagents. But **how do you actually design reusable intelligence?**



The answer is **Persona + Questions + Principles (P+Q+P)**: a pattern-activation framework that moves beyond templates toward reasoning-driven design.



* * *



## Understanding P+Q+P: Why This Pattern Works



Before we design anything, let's understand **why P+Q+P activates reasoning instead of just pattern matching**.



### The Problem with Traditional Guidance



When AI systems follow generic instructions like "make it secure," they pattern-match against their training data. They produce reasonable outputs that ignore your actual constraints.



Compare two approaches:



**Without P+Q+P** (Prediction Mode â€” pattern matching):



```

Instruction: "Design a security system"AI response: [Generates OAuth with MFA, using standard NIST guidelines]Problem: Generic solution that ignores YOUR threat model, compliance requirements, and MVP scope

```



**With P+Q+P** (Reasoning Mode â€” context analysis):



```

Persona: "You are a security auditor reviewing systems protecting customer payment data"Questions:  - What are our actual threat actors? (script kiddies, targeted attackers, nation-states?)  - What's our compliance requirement? (PCI-DSS, HIPAA, GDPR, none?)  - What's our deployment timeline? (6 months, 2 weeks, 2 days?)Principles:  - Defend against ACTUAL threats, not theoretical ones  - Compliance is non-negotiable, but scope is negotiable  - Fail secure: Errors deny accessAI response: [Contextual security design matching YOUR situation]

```



**The difference**: Persona sets cognitive stance. Questions force context-specific analysis. Principles ensure consistent decision-making.



### The Three Components of P+Q+P



#### 1\. Persona: Cognitive Stance



**Purpose**: Activate domain expertise by establishing how the AI should think about problems.



**Good Persona** (specific expertise):



```

You are a defensive programming specialist focused on preventinginvalid input bugs. Think about data validation the way a securityauditor thinks about attack surfaces: What could an attacker provideto break this system?

```



**Weak Persona** (too generic):



```

You are an expert in programming

```



The strong version activates a **specific thinking pattern** (defensive-against-invalid-input) while the weak version just says "be good."



![Decision matrix comparing Skills (2-4 decision points) versus Subagents (5+ decision points), showing when to use each based on complexity and autonomy needs](https://pub-80f166e40b854371ac7b05053b435162.r2.dev/books/ai-native-dev/static/images/part-4/chapter-13/skill-vs-subagent-decision-matrix.png)



#### 2\. Questions: Reasoning Activation



**Purpose**: Force context-specific analysis instead of generic pattern retrieval.



**Good Questions** (force reasoning):



```

1. What are valid input types for THIS feature?2. What value ranges would break THESE calculations?3. What happens if invalid input arrives at THIS boundary?4. Should validation fail-closed (reject everything by default) or   fail-open (accept by default)?

```



**Weak Questions** (pattern matching):



```

1. Is this secure?2. Is this efficient?

```



The strong questions demand specific analysis of **your situation**. The weak questions encourage generic pattern matching.



#### 3\. Principles: Decision Framework



**Purpose**: Provide consistent guidance when context demands tradeoffs.



**Good Principles** (decision frameworks):



```

- Validate at system boundaries (where user data enters)- Fail fast with clear error messages (immediate feedback)- Use type hints + runtime validation (defense in depth)- Document rules in spec.md (clarity for AI and future readers)

```



**Weak Principles** (vague aspirations):



```

- Use best practices- Make it secure- Be efficient

```



The strong principles guide **specific decisions** with clear reasoning. The weak principles offer no guidance.



* * *



## Design Walkthrough â€” Input Validation Skill



Let's design a real, reusable skill: **Input Validation at System Boundaries**.



### Why This Pattern Recurs



In Lessons 1-6, you've written 10+ specifications. How many had invalid input problems?



-   âœ… API endpoints accepting user input

-   âœ… Database queries with user-provided filters

-   âœ… File upload features accepting file types

-   âœ… Authentication systems validating passwords

-   âœ… Payment processing accepting amounts



**Pattern recurrence**: This problem appears in 80% of features. It's worth encoding as reusable intelligence.



### Step 1: Define the Persona



**What type of thinking does this pattern need?**



```

Persona: You are a defensive programming specialist focused onpreventing invalid input bugs. Think about input validation the waya security auditor thinks about attack surfaces:- What could an attacker provide to break this system?- What could a careless developer miss?- Where do assumptions about input fail?

```



**Why this persona?**



-   "Defensive" activates security mindset (not just "validate")

-   "Attack surface" analogy forces thinking about adversaries

-   "Careless developer" accounts for honest mistakes

-   Security-first perspective prevents under-validation



### Step 2: Articulate Reasoning Questions



**What context-specific analysis must happen?**



```

Questions to Ask:1. What are valid input types for THIS feature?   (Examples: string, integer, float, list, dict)   (Reasoning: Different types need different validation)2. What are acceptable value ranges?   (Examples: username 3-50 chars, age 0-150, email must contain @)   (Reasoning: Catches both typos and malicious input)3. What happens with invalid input?   (Options: raise ValueError, return None, use default value,    coerce type)   (Reasoning: Error handling strategy depends on context)4. Should validation be strict (reject by default) or lenient   (accept by default)?   (Examples: Authentication: strict. Search filters: lenient)   (Reasoning: Depends on consequences of rejecting valid input)5. Where does validation happen?   (Examples: API endpoint, database layer, UI validation)   (Reasoning: Boundary validation is most effective)

```



**Why these questions?**



-   Q1-2 define the validation **scope** (what counts as valid)

-   Q3-4 define the validation **strategy** (how to handle invalids)

-   Q5 defines the validation **location** (where in system)

-   Together, they force context analysis specific to your feature



### Step 3: Establish Consistent Principles



**What decision frameworks guide application?**



```

Principles:1. Validate at system boundaries first   - Prevent bad data from ever entering your system   - API endpoints, user inputs, file uploads are boundaries   - Example: Validate email format when received, not when stored2. Fail fast with clear error messages   - Detect invalid input immediately   - Tell users (or developers) EXACTLY what was invalid   - Bad: "Invalid input"   - Good: "Email must contain @ symbol and valid domain"3. Use type hints + runtime validation   - Type hints document expectations   - Runtime validation enforces them   - Together: Defense in depth (redundant safety)4. Document validation rules in your spec.md   - Future developers (and AI agents) need to understand rules   - Example: In your Constraints section, list all input validations   - Clarity prevents reimplementing the same logic twice

```



**Why these principles?**



-   Principle 1 prevents buggy data

-   Principle 2 provides user-friendly feedback

-   Principle 3 combines static + runtime checks

-   Principle 4 makes the pattern reusable (not ad-hoc)



### Complete Input Validation Skill Template



Here's what this skill looks like as a complete P+Q+P document:



```

# Input Validation Skill## PersonaYou are a defensive programming specialist focused on preventinginvalid input bugs. Think about input validation the way a securityauditor thinks about attack surfaces: What could break this system?## Questions to Ask1. What are valid input types? (string, int, float, list, dict)2. What are acceptable value ranges? (min/max, allowed values)3. What happens with invalid input? (raise, return None, coerce)4. Should validation be strict (reject) or lenient (accept)?5. Where is the system boundary? (API endpoint, database, UI)## Principles- Validate at system boundaries (prevent bad data from entering)- Fail fast with clear error messages (immediate feedback)- Use type hints + runtime validation (defense in depth)- Document validation rules in spec.md (clarity for AI and future readers)## When to Apply This SkillUse this skill when your feature:- Accepts user input (forms, API endpoints, file uploads)- Makes assumptions about data format or range- Has consequences for invalid input (crashes, security issues, data loss)## Example Application**Feature**: User registration API endpoint**Applying P+Q+P**:1. Persona analysis:   - What could users (maliciously or accidentally) provide?   - Weakness: Assume email is always valid? No.2. Questions answered:   - Valid types: email (string), password (string), age (int)   - Ranges: email <256 chars, password 8-100 chars, age 13-120   - On invalid: raise ValueError with descriptive message   - Strategy: Strict (registration must succeed with valid input)   - Boundary: API endpoint (before database storage)3. Principles applied:   - Boundary validation: Check formats at endpoint entry   - Fast failure: Return 400 Bad Request immediately   - Type hints: def register(email: str, password: str, age: int)   - Runtime check: assert 13 <= age <= 120, "Age must be 13-120"   - Documentation: spec.md includes validation rules

```



* * *



## Design Walkthrough â€” Performance Optimization Subagent



Now let's design something more complex: a **Performance Optimization Subagent**.



### Why Subagent Instead of Skill?



A skill gives guidance (2-4 decisions). A subagent makes autonomous decisions (5+ decision points).



**Performance optimization requires 5+ independent decisions:**



1.  What data volumes matter? (10 records vs 1M vs 1B)

2.  What latency constraints exist? (real-time vs batch vs offline)

3.  What query patterns cause N+1 problems?

4.  Should we cache results? (frequency analysis)

5.  What memory constraints apply? (streaming vs loading all data)

6.  What data structures are optimal? (hash tables vs indexes)



This complexity justifies an **autonomous subagent** that analyzes your specification and makes performance recommendations.



### Step 1: Define the Persona



**What type of thinking does performance analysis need?**



```

Persona: You are a performance engineer who reviews systems forefficiency concerns. Think about performance the way a systemsarchitect thinks about scalability:- What happens when data grows 1000x?- What's the true constraint? (CPU? Memory? Network?)- Is optimization valuable (actual bottleneck) or waste (premature)?

```



**Why this persona?**



-   "Performance engineer" activates scalability mindset

-   "1000x growth" forces thinking beyond current data

-   "True constraint" prevents false optimizations

-   "Actual bottleneck" prioritizes valuable work



### Step 2: Articulate Reasoning Questions



**What deep analysis must happen?**



```

Questions to Ask:1. What are expected data volumes?   (Examples: 10 user records, 1M transactions, 1B events)   (Reasoning: Optimization depends on scale)2. What are latency requirements?   (Examples: Real-time <100ms, batch <1min, overnight <1hr)   (Reasoning: Different requirements need different approaches)3. Are there N+1 query risks?   (Examples: Loading 100 users + querying posts for each = 101 queries)   (Reasoning: Single worst performance problem in data-driven systems)4. Should results be cached?   (Examples: If same 10 users searched 1000x/day, cache valuable)   (Reasoning: Cache only helps if queries repeat)5. What are memory constraints?   (Examples: Can we load all 1B records? No. Stream them? Yes.)   (Reasoning: Memory vs latency tradeoff)6. What data structures are optimal?   (Examples: O(1) lookup needs hash, sorted data needs tree)   (Reasoning: Data structure choice determines complexity)

```



**Why these questions?**



-   Q1 defines the **scale** of the problem

-   Q2 defines the **performance targets**

-   Q3-6 identify specific optimization opportunities

-   Together, they prevent wasted effort on non-problems



### Step 3: Establish Consistent Principles



**What decision frameworks guide optimization?**



```

Principles:1. Optimize for actual use case, not theoretical worst case   - Premature optimization is waste   - Profile first, then optimize based on real bottlenecks   - Example: If system handles 10 users, optimizing for 1B is wasted effort2. Measure before optimizing   - Profile your code, don't guess   - "It's fast enough" is valid if profiling proves it   - Optimization without data is cargo cult engineering3. Document performance requirements in Constraints section   - Make latency requirements explicit in spec.md   - Example: "API response must be <100ms for user list endpoint"   - Clarity lets AI implement with performance in mind from start4. Choose data structures intentionally   - O(1) lookup needs hash tables (memory cost)   - O(log n) lookup needs sorted trees (query cost)   - O(n) scan needs optimization elsewhere   - Tradeoff explicitly, don't default to "fastest"

```



**Why these principles?**



-   Principle 1 prevents wasted optimization effort

-   Principle 2 replaces guessing with measurement

-   Principle 3 makes performance a first-class spec concern

-   Principle 4 provides concrete optimization guidance



### Complete Performance Optimization Subagent Template



Here's what this subagent looks like as a complete P+Q+P document:



```

# Performance Optimization Subagent## PersonaYou are a performance engineer who reviews systems for efficiencyconcerns. Think about performance the way a systems architect thinksabout scalability: What happens when data grows 1000x? What's thetrue bottleneck? Is this optimization valuable or waste?## Questions to Ask1. What are expected data volumes? (10, 1M, 1B records?)2. What are latency requirements? (real-time, batch, offline?)3. Are there N+1 query risks? (query-per-item problems?)4. Should results be cached? (how often do identical queries repeat?)5. What are memory constraints? (load all data vs stream?)6. What data structures would be optimal? (O(1) vs O(log n) vs O(n)?)## Principles- Optimize for actual use case (prevent premature optimization waste)- Measure before optimizing (profile, don't guess)- Document performance requirements in Constraints (make targets explicit)- Choose data structures intentionally (explicit tradeoffs, not defaults)## When to Activate This SubagentUse this subagent when reviewing specifications for:- Features handling large datasets- APIs with strict latency requirements (<100ms)- Database operations that might have N+1 query risks- Systems needing caching strategies- Memory-constrained environments## Example Application**Feature**: "Fetch all posts by a user and their comments"**Activating Performance Subagent**:Persona analysis:- As performance engineer, what's the risk? N+1 queries (user posts,  then comment per post)- 1000x growth: 10 users becomes 10,000? 100,000?Questions answered:1. Data volumes: ~50,000 users, each with ~20 posts, each with ~5 comments2. Latency: API response <200ms3. N+1 risk: YES (query user, query posts, query comments) = 1000+ queries4. Cache: YES (users viewing same user's posts repeatedly)5. Memory: Can load all posts + comments for single user (<1MB)6. Structures: Post index by user ID (hash), comments by post ID (hash)Subagent recommendations:- Principle 1: This IS a real bottleneck (1000+ queries), worth optimizing- Principle 2: Profile current implementation to measure impact- Principle 3: Add to spec.md: "API response <200ms, including comments"- Principle 4: Use hash indexes for userâ†’posts and postâ†’comments (O(1) lookup)

```



* * *



## Hands-On Practice â€” Design Your Components



Now it's your turn. You'll design **two reusable intelligence components** using P+Q+P.



### Exercise 1: Design a Security Review Subagent



**Scenario**: In Lessons 1-6, you've written multiple specs for features handling sensitive data (user authentication, payment processing, data storage). A **Security Review Subagent** would evaluate every spec for security gaps.



**Your task**: Design (don't implement) a Security Review Subagent using P+Q+P.



**Template to complete**:



```

## Security Review Subagent### Persona[What thinking pattern activates security analysis?][Specific expertise, not generic "expert"]### Questions to Ask[List 5+ questions that force context-specific security analysis][Think: What attacks are possible? What's the threat model? What standards apply?]### Principles[List 3-4 decision frameworks guiding security recommendations][Think: Defense in depth? Fail secure? Least privilege?]### When to Apply[What feature types benefit from this subagent?]

```



**Guidance**:



-   Persona should activate threat-modeling mindset

-   Questions should cover: threat actors, data sensitivity, compliance, attack vectors

-   Principles should guide real security decisions (not generic "be secure")

-   Consider: Is this 5+ decision points? (If <5, it's a Skill instead)



### Exercise 2: Design an API Design Skill



**Scenario**: Throughout the book, students write APIs with inconsistent endpoint patterns. A **API Design Skill** would provide guidance for consistent REST design.



**Your task**: Design (don't implement) an API Design Skill using P+Q+P.



**Template to complete**:



```

## API Design Skill### Persona[What thinking pattern ensures consistent API design?][Specific expertise: What does good API design mean?]### Questions to Ask[List 2-4 questions that guide API decisions][Think: Naming conventions? Versioning? Status codes? Resource structure?]### Principles[List 3-4 decision frameworks for API consistency][Think: REST conventions? Semantic meaning? Developer experience?]### When to Apply[What features benefit from this skill?]

```



**Guidance**:



-   Persona should activate API-design thinking (not generic "expert")

-   Questions should cover: naming, resource structure, status codes, versioning

-   Principles should guide specific choices (not vague "best practices")

-   Consider: Is this 2-4 decision points? (If 5+, it's a Subagent instead)



### Self-Critique: Comparing Your Designs



Once you've designed both components, **evaluate them**:



**For the Security Review Subagent:**



-   Does the Persona activate threat-modeling thinking?

-   Do the Questions cover: threat actors, data sensitivity, compliance, attack vectors?

-   Do the Principles guide real security decisions?

-   Are there 5+ decision points? (Justifying Subagent, not Skill)



**For the API Design Skill:**



-   Does the Persona activate API-design thinking?

-   Do the Questions cover key API decisions (naming, structure, codes)?

-   Do the Principles guide specific choices?

-   Are there 2-4 decision points? (Justifying Skill, not Subagent)



**Comparison**:



-   Which design is more specific and actionable?

-   Which design would produce more consistent AI outputs?

-   Would you use either of these in future features?



* * *



## Skill vs Subagent: The Decision Framework



You've now designed both a Skill and a Subagent. How do you know which to create?



### Decision Matrix



Characteristic



Skill



Subagent



**Decision Points**



2-4



5+



**Autonomy**



Guidance (human decides)



Autonomous (AI decides)



**Reusability**



Horizontal (applies broadly)



Vertical (domain-specific)



**Complexity**



Simple patterns



Complex reasoning



**Format**



Document template



Specification document



**Examples:**



**Skills** (2-4 decisions):



-   Input Validation (types? ranges? error handling? strict/lenient?)

-   API Design (naming? versioning? status codes? resource structure?)

-   Error Messages (clarity? actionability? consistency? user-facing vs debug?)



**Subagents** (5+ decisions):



-   Performance Optimization (volumes? latency? N+1? caching? memory? structures?)

-   Security Review (threat actors? data sensitivity? compliance? attack vectors? defenses?)

-   Accessibility Auditor (visual? motor? cognitive? hearing? platform-specific?)



**Constitutional Rules** (applies to ALL specs):



-   Specification primacy (spec before code)

-   Progressive complexity (respect cognitive load tiers)

-   Factual accuracy (verify all claims)

-   Coherent structure (pedagogical progression)



* * *



## What You've Accomplished



You can now:



âœ… **Design reusable intelligence** using P+Q+P pattern âœ… **Activate reasoning mode** instead of pattern matching âœ… **Distinguish Skills from Subagents** by complexity and autonomy âœ… **Create documentation** that produces consistent AI outputs



**Key insight**: RI design â‰  RI implementation. In this lesson, you designed components. In Chapter 35+, you'll implement them using specific frameworks (skills architecture, subagent runtimes, etc.).



* * *



## Try With AI



**Setup**: Open your AI companion and explore P+Q+P design patterns further.



**Prompt Set**:



**Prompt 1** (Understanding P+Q+P):



```

I'm learning to design reusable intelligence (skills and subagents)using Persona + Questions + Principles pattern.What's the difference between these two approaches?Bad: "Make a secure authentication system"Good: "Persona: security auditor reviewing threat models. Questions:What threats? What data? What standards? What's the breach cost?Principles: Fail secure, least privilege, defense in depth."Why does the good approach produce better AI outputs?

```



**Prompt 2** (Designing a New Skill):



```

I'm designing a "Database Query Optimization" Skill.Help me develop:1. A specific Persona that activates performance-mindset thinking2. Questions that force performance analysis (not generic "optimize")3. Principles that guide optimization decisionsMake sure the Persona goes beyond "you're an expert" and givesspecific cognitive stance.

```



**Prompt 3** (Subagent vs Skill Decision):



```

I've identified this recurring pattern in my specifications:[Describe a pattern from your work]Should I encode this as a Skill (2-4 decision guidance) or a Subagent(5+ autonomous reasoning)?Help me count decision points and determine the right approach.

```



**Expected Outcomes**:



-   Prompt 1: Understanding why P+Q+P activates reasoning over pattern matching

-   Prompt 2: Creating a distinctive Persona that's more specific than "expert"

-   Prompt 3: Clear decision framework for Skill vs Subagent choice



**Safety Note**: When using AI to help design intelligence patterns, verify that the Persona activates thinking (not just performance), Questions demand context analysis (not generic patterns), and Principles guide decisions (not state aspirations).



### Core Concept



Persona + Questions + Principles (P+Q+P) is a pattern-activation framework that shifts AI from generic pattern-matching to context-specific reasoning, enabling reusable intelligence that actually adapts to your constraints.



### Key Mental Models



-   **Prediction mode vs reasoning mode**: Generic instructions (e.g., "make it secure") trigger pattern-matching; P+Q+P triggers context analysis

-   **Persona as cognitive stance**: Specific thinking pattern (e.g., "defensive programmer focused on attack surfaces") activates domain expertise better than generic "expert"

-   **Questions force context analysis**: Context-specific questions (5+ for Subagents, 2-4 for Skills) replace vague aspirations with measurable reasoning

-   **Principles provide decision frameworks**: When tradeoffs appear, principles guide consistent choices (not aspirations like "be efficient")



### Critical Patterns



-   **Input Validation Skill**: Persona (defensive programming), Questions (valid types? ranges? error handling? strict/lenient? boundaries?), Principles (validate boundaries, fail fast, type hints + runtime, documentation)

-   **Performance Optimization Subagent**: Persona (performance engineer analyzing scalability), Questions (data volumes? latency? N+1 queries? caching? memory? data structures?), Principles (optimize actual use case, measure, document requirements, choose intentionally)

-   **Security Review Subagent**: Persona (threat modeler reviewing attack surfaces), Questions (threat actors? data sensitivity? compliance? attack vectors?), Principles (defense in depth, fail secure, least privilege)



### AI Collaboration Keys



-   Specifications + P+Q+P design enable AI agents to reason rather than pattern-match

-   Skills guide AI consistently (apply same error-handling approach to all APIs)

-   Subagents provide expert review automatically (security audit, performance analysis, accessibility check)

-   P+Q+P documentation produces more consistent outputs than repeated prompting



### Common Mistakes



-   Weak Persona ("You are an expert") vs specific Persona ("Defensive programmer focused on input validation attack surfaces")

-   Questions that pattern-match ("Is this secure?") vs questions that force analysis ("What could an attacker provide?")

-   Principles that aspirate ("Use best practices") vs principles that guide ("Validate at boundaries, fail fast, type hints + runtime")

-   Confusing decision point count (2-4 for Skills, 5+ for Subagents) leading to over-complex Skills or under-utilized Subagents



### Connections



-   **Builds on**: Recognizing when patterns recur (Lesson 6 identifies intelligence opportunities)

-   **Leads to**: Implementing P+Q+P in actual framework tools (Claude Code Skills, subagent runtimes, MCP servers)



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/SDD-RI-Fundamentals/specification-driven-development-fundamentals/designing-skills-and-subagents

</p+q+p>









<sdd-ri-framework>

-   [](/)

-   [Part 4: SDD-RI Fundamentals](/docs/SDD-RI-Fundamentals)

-   [Chapter 13: Understanding Spec-Driven Development with Reusable Intelligence](/docs/SDD-RI-Fundamentals/specification-driven-development-fundamentals)

-   SDD-RI Framework Landscape & What's Next



# SDD-RI Framework Landscape & What's Next



You've now completed your journey through SDD fundamentals (Lessons 1-5) and learned how to design reusable intelligence with RI (Lessons 6-7). Before you move into implementation in Chapter 35, it's critical to understand the landscape of SDD frameworks and recognize where Spec-Kit Plus â€” the framework this book teaches â€” fits within the larger ecosystem.



**This lesson is not about choosing a framework for YOUR projects** (that's a decision you'll make later with your own team constraints). Instead, this is **landscape awareness**: understanding four major SDD approaches that emerged in 2025, recognizing what context factors matter in framework selection, and understanding why Panaversity designed Spec-Kit Plus specifically for AI-native teams learning SDD-RI.



* * *



## Know Your Context



Before comparing frameworks, ground yourself in reality. **These three questions determine which tool actually fits.**



### Question 1: Who's Building This?



-   **Solo developer** â€” Just you, learning or building side projects

-   **Small team** â€” 2-5 people, probably in an early-stage company or internal project

-   **Medium team** â€” 5-15 people, established project or startup scaling

-   **Large team** â€” 15+ people across multiple services/domains



### Question 2: What's the Problem Scale?



-   **Simple** â€” One feature, one service

-   **Medium** â€” Multiple features, system integrations

-   **Complex** â€” Multiple services, architectural decisions, or ongoing maintenance



### Question 3: Are There Regulatory/Compliance Constraints?



-   **No constraints** â€” Build what works. Move fast. Iterate.

-   **Light constraints** â€” "Best practices" (like 80% test coverage)

-   **Heavy constraints** â€” Regulated domain (healthcare, finance, payments, aerospace)

-   **Strict constraints** â€” FDA/SOX/HIPAA/PCI compliance required. Auditable decisions matter.



**Quick self-assessment**: Even though you won't select a framework today, answer these questions for your current understanding. These factors appear in every real framework decision.



* * *



## Four SDD Approaches



Four main SDD approaches emerged to serve different needs. Here's how to think about each one and when it fits.



### 1: Kiro â€” "Start Simple"



**Philosophy**: SDD shouldn't require learning complex processes.



**Workflow**: Simple requirements â†’ Design â†’ Tasks â†’ Code



**Best For**:



-   Solo developers or tiny teams (1-3 people)

-   Learning SDD for the first time

-   Building medium-sized features (not systems, not tiny bug fixes)



**Trade-off**: You get simplicity and low cognitive load. You sacrifice governance and consistency enforcement across projects.



* * *



### 2: Spec-Kit (GitHub's Framework) â€” "Strong Governance"



**Philosophy**: Immutable principles (Constitution) enforce consistency across everything.



**Workflow**: Constitution defines rules â†’ Specify requirements â†’ Plan architecture â†’ Tasks â†’ Code (with Constitution checks at each phase)



**Best For**:



-   Teams of 5-50+ people where consistency matters

-   Enterprise environments requiring strong governance

-   Open-source frameworks (GitHub's official SDD standard)

-   Projects where "all passwords use bcrypt" must be enforced



**Strengths**: Comprehensive traceability (requirements â†’ plan â†’ tasks â†’ code â†’ tests), scales to large teams, strong governance



**Limitations**: Doesn't document architectural "why" decisions, no AI interaction tracking, no built-in domain expertise



* * *



### 3: Spec-Kit Plus (Panaversity's Evolution) â€” "Spec-Kit + Intelligence"



**Philosophy**: Spec-Kit foundation + three critical intelligence layers for AI-native teams.



**What It Adds**:



1.  **Architectural Decision Records (ADRs)** â€” Document the "why" behind choices (why JWT instead of sessions? why this database? why this language?)

2.  **Prompt History Records (PHRs)** â€” Track every AI interaction (what prompt â†’ what response â†’ accepted/modified/why?) for compliance and learning

3.  **Intelligence Templates** â€” Pre-built domain expertise (education, healthcare, fintech, aerospace) so teams don't rebuild domain-specific rules from scratch



**Best For**:



-   Teams collaborating with AI agents (the focus of this book)

-   Building systems that last years (long-term maintainability matters)

-   Regulated domains where auditable decisions are critical

-   Learning from AI interactions over time



**How It Implements SDD-RI Concepts**:



-   ADRs document the reasoning you've been learning in Lessons 6-7 (why you chose skill X over subagent Y)

-   PHRs capture the co-learning pattern you've seen throughout this book (AI suggests â†’ you refine â†’ you converge)

-   Intelligence Templates encode the domain expertise you've been building



* * *



### 4: Tesel â€” "Specs as Source of Truth"



**Philosophy**: Take spec-driven development to its logical extremeâ€”specs are the ONLY source of truth. Code is generated and never hand-edited.



**Workflow**: Write specification â†’ Run code generator â†’ Code produced (read-only) â†’ Later: update spec â†’ regenerate code



**Best For**:



-   Safety-critical systems (aerospace, medical devices)

-   Domains where code-spec divergence is unacceptable

-   Still in private beta (invite-only in 2025)



* * *



## Why This Book Teaches Spec-Kit Plus



You might wonder: "If Kiro is simpler, shouldn't I start there? If Spec-Kit is GitHub's standard, why not use that?"



The answer lies in how this book teaches SDD-RI and what you need to succeed.



### The SDD-RI Choice: Why Spec-Kit Plus



**Chapter 17 (This Chapter)** teaches SDD fundamentalsâ€”the WHAT and HOW of specification-driven development.



**Chapter 35+ (Next)** teaches SDD-RI implementationâ€”the WHY and HOW TO DESIGN alongside AI agents.



When you're learning to design reusable intelligence (L6-7) and orchestrate components (L8+), you need a framework that:



1.  **Captures Your Reasoning (ADRs)**

    

    -   In Lesson 7, you learned the Persona + Questions + Principles pattern for designing skills

    -   ADRs are how you DOCUMENT that reasoning for your team and AI agents

    -   Example: "Why did we create a git-workflow skill instead of a single git-commit subagent?" â†’ ADR explains the tradeoff

2.  **Logs AI Interactions (PHRs)**

    

    -   Throughout this book, you've seen AI collaboration: suggest â†’ refine â†’ converge

    -   PHRs are how you CAPTURE and LEARN from those interactions

    -   Example: "Prompt A generated vulnerable code. Prompt B fixed it. Use B next time."

3.  **Provides Domain Expertise (Intelligence Templates)**

    

    -   You've been learning the Education template (Bloom's levels, CEFR proficiency, code requirements)

    -   Templates prevent teams from rebuilding this knowledge every time

    -   Example: "New healthcare team? Start with healthcare template. Know HIPAA rules before coding."



**These three additions are not optional overhead.** They are how AI-native teams learn, reason together, and scale knowledge across projects.



### The Pedagogical Choice



**This book chose Spec-Kit Plus because it's designed for exactly what you're learning:**



-   Spec-Kit Plus assumes you're working with AI agents (not against them)

-   Spec-Kit Plus tracks decisions and interactions (not just deliverables)

-   Spec-Kit Plus includes domain templates (not generic, one-size-fits-all rules)



You're not learning abstract theory. You're learning the practical framework that professional AI-native teams use today.



* * *



## From Understanding to Implementation



### What You've Completed



-   **Lessons 1-5**: SDD fundamentals â€” how to write clear specifications and plan implementations

-   **Lessons 6-7**: RI concepts â€” how to design reusable intelligence, capture reasoning, recognize when to create skills vs subagents

-   **Lesson 8**: Framework landscape â€” industry context and why Spec-Kit Plus fits AI-native teams



### What's Next



Next chapter 35 takes everything you've learned and puts it into practice:



-   **Build real features** using Spec-Kit Plus (not toy examples)

-   **Practice the complete SDD-RI workflow**: Specification â†’ ADR decisions â†’ Prompt â†’ Code â†’ PHR logging â†’ Validation

-   **Experience framework-in-action**: See how ADRs capture your reasoning, how PHRs log AI interactions, how Intelligence Templates guide your domain choices

-   **Compose components**: Use skills and subagents you've learned about, orchestrating them through specifications



### The Synthesis



```

Chapter 17 Complete  â””â”€ SDD Fundamentals (L1-5)  â””â”€ RI Concepts (L6-7)  â””â”€ Framework Awareness (L8)       â†“Chapter 35+ Implementation  â””â”€ Spec-Kit Plus in practice  â””â”€ Real feature development  â””â”€ SDD-RI workflow executed

```



**In Chapter 35, you'll see:**



-   How specifications trigger Spec-Kit Plus phases

-   How ADRs capture architectural reasoning

-   How PHRs log AI interactions and improve future prompts

-   How Intelligence Templates guide domain decisions

-   How all of this works together in real, professional development



You're ready. Let's implement.



* * *



## Try With AI



**Setup**: Open your AI tool (Claude, ChatGPT, etc.) and explore these framework-awareness prompts. Remember: the goal is understanding the landscape, not making a framework choice for your project yet.



**Prompt 1: Explore Framework Tradeoffs**



> "I'm learning about four SDD frameworks (Kiro, Spec-Kit, Spec-Kit Plus, Tesel). My situation: \[describe team size, problem scale, any compliance constraints\]. For THIS situation, compare what each framework gives and sacrifices. Which seems like the best fit?"



**Expected Outcome**: Your AI should ask clarifying questions and explain tradeoffs specific to your context (governance vs simplicity, team size, compliance needs).



* * *



**Prompt 2: Understand Spec-Kit Plus Evolution**



> "Compare GitHub's Spec-Kit with Panaversity's Spec-Kit Plus. What three features does Spec-Kit Plus add? Why does each matter for teams working with AI agents?"



**Expected Outcome**: Your AI should explain ADRs (capture reasoning), PHRs (log interactions), and Intelligence Templates (domain expertise) and connect each to AI-native development.



* * *



**Prompt 3: Recognize When Frameworks Matter**



> "Explain when a solo developer should use Kiro vs Spec-Kit Plus. What threshold (team size, project complexity, AI agent usage) makes you switch from one to the other?"



**Expected Outcome**: Your AI should articulate decision factors: governance burden increases with team size, complexity justifies overhead, AI agent collaboration requires ADRs/PHRs.



* * *



**Prompt 4 (Optional Stretch): Apply to Your Context**



> "My project: \[describe your actual situation\]. Am I currently using any SDD framework? If I adopted one, which would fit best and why? What would I need to change about my current process?"



**Expected Outcome**: Concrete framework selection criteria based on YOUR situation (not abstract).



* * *



### Core Concept



Multiple SDD frameworks emerged (Kiro, Spec-Kit, Spec-Kit Plus, Tesel), each optimizing for different contexts; Spec-Kit Plus adds ADRs (document reasoning), PHRs (log AI interactions), and Intelligence Templates (domain expertise) specifically for AI-native teams learning SDD-RI.



### Key Mental Models



-   **Context determines framework fit**: Team size, problem complexity, and compliance constraints drive framework choice (not one-size-fits-all)

-   **Spec-Kit Plus evolution**: Builds on GitHub's Spec-Kit (strong governance) + adds intelligence tracking (ADRs), interaction logging (PHRs), and domain templates

-   **Compliance-driven design**: Regulated domains (healthcare, finance, aerospace) need auditable decisions (PHRs track "why did AI suggest this?")

-   **Knowledge persistence**: Intelligence Templates encode domain expertise so new teams don't rebuild (healthcare template has HIPAA rules pre-loaded)



### Critical Patterns



-   **Kiro**: Simple (requirements â†’ design â†’ tasks â†’ code), best for solo developers, learns SDD for first time

-   **Spec-Kit**: Strong governance (Constitution enforces consistency), best for 5-50+ teams, enterprise environments

-   **Spec-Kit Plus**: Spec-Kit + ADRs (why decisions) + PHRs (AI interaction logs) + Templates (domain expertise), designed for AI-native teams

-   **Tesel**: Specs are ONLY source of truth (code generated, never hand-edited), for safety-critical systems (aerospace, medical)



### AI Collaboration Keys



-   ADRs capture reasoning you've learned in P+Q+P design (why Skill X instead of Subagent Y?)

-   PHRs log co-learning pattern throughout book (AI suggests â†’ you refine â†’ convergence)

-   Intelligence Templates provide pre-built domain expertise so teams start with knowledge baseline



### Common Mistakes



-   Choosing framework before understanding context (team size, compliance needs)

-   Assuming Spec-Kit Plus overhead is unnecessary (ADRs/PHRs are not overhead; they're how AI-native teams learn)

-   Treating framework choice as permanent (can migrate between frameworks as team grows)



### Connections



-   **Builds on**: All SDD-RI concepts (Specifications, Constitutions, Skills, Subagents, P+Q+P design)

-   **Leads to**: Chapter 35+ implementation using Spec-Kit Plus in practice, real feature development, SDD-RI workflow execution



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/SDD-RI-Fundamentals/specification-driven-development-fundamentals/sdd-ri-framework-landscape

</sdd-ri-framework>









<constitution>

-   [](/)

-   [Part 4: SDD-RI Fundamentals](/docs/SDD-RI-Fundamentals)

-   [Chapter 14: Master Spec-Kit Plus](/docs/SDD-RI-Fundamentals/spec-kit-plus-hands-on)

-   Constitution Phase â€” Project-Wide Quality Standards



# Constitution Phase â€” Project-Wide Quality Standards



You've installed Spec-Kit Plus and initialized your project. Now it's time to create the foundational rules that will guide every part of your research paper project.



The Constitution answers a critical question: **What standards apply to every piece of work you do?** Not just for this paper, but for all papers. Not just this deadline, but for your professional reputation.



Think of it like team rules before building a big LEGO project:



-   What if you want all towers square, but your helper builds round ones?

-   What if you decide the roof must be blue, but your helper builds red?



The Constitution is your team's **Rulebook**. It lists the most important rules that both you and your AI companion MUST follow, no matter what.



* * *



## What Is a Constitution?



### Constitution: Global Rules, One Per Project



A **Constitution** defines **immutable standards** applying to **all work** in a project. It's distinct from a **Specification**, which applies to **one feature**.



**Constitution applies to (research paper project)**:



-   Citation standards for ALL papers (APA format, source verification)

-   Writing clarity for ALL papers (Flesch-Kincaid grade level)

-   Academic integrity for ALL papers (plagiarism checking)

-   Source requirements for ALL papers (peer-reviewed minimum)



**Specification applies to (one specific paper)**:



-   THIS paper's thesis statement

-   THIS paper's specific research questions

-   THIS paper's word count and deadline

-   THIS paper's acceptance criteria



**Example**:



```

CONSTITUTION (applies to ALL papers):  âœ… "All papers must cite primary sources"  âœ… "All claims must be verified against sources"  âœ… "APA citation format required"  âœ… "Zero plagiarism tolerance"SPECIFICATION (applies only to THIS paper):  âœ… "Thesis: AI-native development requires spec-first thinking"  âœ… "Target length: 5,000 words"  âœ… "Minimum 12 peer-reviewed sources"  âœ… "Due date: December 15"

```



### Why Constitution Matters: The Cascade



The Constitution is the **starting point of the cascade**:



```

Clear Constitution    â†“(ensures every spec respects quality standards)    â†“Clear Specification    â†“(ensures planning accounts for quality gates)    â†“Clear Plan    â†“(ensures tasks include verification)    â†“Clear Tasks    â†“(enables AI to generate properly cited writing)    â†“Published Research Paper

```



**Weak Constitution** produces:



-   Specs that don't specify citation requirements

-   Plans that skip plagiarism checking

-   Writing with uncited claims

-   Papers that fail fact-checking



**Strong Constitution** produces:



-   Specs that automatically include source quality requirements

-   Plans with built-in verification steps

-   Writing that's properly cited

-   Papers that pass publication standards



### Constitution is One-Time, Feature Work is Repetitive



You write the Constitution **once per project**. Then, for each paper:



```

1. Initialize project2. Write Constitution (quality standards for ALL papers)3. Commit Constitution to git4. FOR EACH PAPER:   - Run /sp.specify (new specification for this paper)   - Run /sp.clarify (refine specification)   - Run /sp.plan (new plan for this paper)   - Run /sp.tasks (new tasks for this paper)   - Run /sp.implement (write paper with AI)   - Commit paper to git

```



* * *



## Part A: Reading the Base Constitution



Before writing your own, look at the base Constitution file that `specifyplus init` created:



```

# Open the constitution filecat .specify/memory/constitution.md

```



**What you'll see**: A starter template with placeholder sections for principles, standards, and constraints.



**The Key Insight**: Constitutions are project-specific. Your research paper Constitution would never mention "type hints" because that's for code. A software project Constitution wouldn't need "citation format" because that's for papers.



* * *



## Part B: Writing Your Research Paper Constitution



Now let's write YOUR Constitution for the research paper project.



### Step 1: Run `/sp.constitution`



Open your AI tool and run the constitution command with your project requirements:



```

/sp.constitutionProject: Research paper on AI-native software developmentCore principles:- Accuracy through primary source verification- Clarity for academic audience (computer science background)- Reproducibility (all claims cited and traceable)- Rigor (peer-reviewed sources preferred)Key standards:- All factual claims must be traceable to sources- Citation format: APA style- Source types: minimum 50% peer-reviewed articles- Plagiarism check: 0% tolerance before submission- Writing clarity: Flesch-Kincaid grade 10-12Constraints:- Word count: 5,000-7,000 words- Minimum 15 sources- Format: PDF with embedded citationsSuccess criteria:- All claims verified against sources- Zero plagiarism detected- Passes fact-checking review

```



**What the agent does**:



-   Creates a comprehensive Constitution file at `.specify/memory/constitution.md`

-   Defines testable quality standards

-   Documents all constraints and success criteria

-   Shows you the generated Constitution



### Step 2: Review Your Constitution



After the agent generates your Constitution, **review it carefully**.



**Your Prompt**:



```

Show me the generated constitution file and explain what it contains.

```



**Agent shows**:



-   **Core Principles** â€” Your research philosophy

-   **Quality Standards** â€” Testable criteria for all papers

-   **Source Requirements** â€” Citation and verification rules

-   **Constraints** â€” Length, format, deadlines

-   **Success Criteria** â€” How to know if standards are met



### Step 3: Improve Your Constitution



Think about what "good research" means for YOUR project. Ask the agent:



```

Review my Constitution at .specify/memory/constitution.md and improve it:1. Are all standards testable (not vague)?   - âŒ Vague: "Papers should be well-written"   - âœ… Testable: "Flesch-Kincaid grade 10-12; active voice 75%+ of time"2. Did I cover essential categories?   - Citation accuracy   - Source verification   - Writing clarity   - Plagiarism checking   - Review process3. Are any standards unrealistic?Suggest 2-3 concrete improvements.

```



**What the agent does**:



-   Identifies vague standards and makes them testable

-   Suggests missing categories

-   Flags unrealistic constraints

-   Updates the Constitution file



* * *



## Part C: Commit Constitution to Git



Here's a critical best practice: **Always commit the Constitution before starting feature work.**



### Why Commit First?



1.  **Immutability**: Constitution is foundational; committing signals "this is our standard"

2.  **Clarity**: Everyone (including your AI) sees Constitution as the baseline

3.  **Traceability**: Git history shows when and why Constitution was created

4.  **Reversibility**: You can revert if needed (rare, but important)



### Commit Steps



**Your Prompt**:



```

/sp.git.commit_pr Commit the constitution to a feature branch

```



**Agent Does**:



-   Creates a conventional commit for the constitution

-   Pushes to a new feature branch

-   Creates a draft PR (or shares compare URL)



The Constitution is now the foundation for all your paper work. Every specification you write, every plan you generateâ€”they all work within the Constitution's constraints.



* * *



## How Constitution Guides Downstream Phases



Now that you've created a Constitution, let's see how it cascades through every other phase.



### Constitution â†’ Specification Phase



**Your Constitution says**:



```

- All papers must cite primary sources- Minimum 50% peer-reviewed sources- APA citation format- Zero plagiarism tolerance

```



**Your Specification for Paper #1 must respect this**:



```

This specification inherits Constitution standards:- Thesis: "AI-native development requires spec-first thinking"- Length: 5,000 words- Sources: minimum 15 total, minimum 8 peer-reviewed- Format: APA style (inherited from Constitution)- Success criteria: All claims cited, Flesch-Kincaid 10-12

```



Notice: You DON'T re-specify citation format or plagiarism checking. The Constitution already requires it.



### Constitution â†’ Plan Phase



**Your Constitution says**:



```

- All claims must be verified against sources- Plagiarism check required before submission

```



**Your Plan must account for this**:



```

Phase 1: Research and source identificationPhase 2: Detailed outline with source assignmentsPhase 3: Draft writing with inline citationsPhase 4: Fact-checking pass (verify all claims)Phase 5: Plagiarism scanning (0% tolerance)Phase 6: Final review and submission

```



The plan INCLUDES verification steps because Constitution REQUIRES them.



### Constitution â†’ Implementation



When you write the paper with AI, Constitution standards guide every interaction:



```

You: "Write the Introduction section. Use these sources: [list].Follow the Constitution: APA citations, verify all claims."AI: "I'll write the introduction with:- In-text APA citations for each claim- Plain language targeting grade 10-12 reading level- Verification of claims against provided sources"[AI writes introduction with citations]You: "The third claim needs a primary source, not secondary.Constitution requires 50% primary sources."AI: "You're right. Let me find the primary research on that topicand revise the citation."[AI revises with primary source]

```



Constitution REQUIREMENTS shape every interaction.



* * *



## Common Mistakes



### Mistake 1: Copying Constitution Without Customization



**The Error**: "I'll use the example Constitution as-is."



**Why It's Wrong**: Constitutions are project-specific. A code project Constitution mentions "type hints"â€”irrelevant for papers.



**The Fix**: Read examples for structure, but write rules specific to YOUR project.



### Mistake 2: Vague Quality Standards



**The Error**: "Papers must be good quality" or "Sources should be credible"



**Why It's Wrong**: "Good" and "credible" are subjective. No one can verify these.



**The Fix**: Use testable criteria:



-   âŒ Vague: "Good writing quality"

-   âœ… Testable: "Flesch-Kincaid grade 10-12; all claims cited; zero plagiarism"



### Mistake 3: Forgetting to Commit Constitution



**The Error**: Create Constitution, then start spec without committing.



**Why It's Wrong**: Constitution becomes "whatever I remember" instead of "documented standard."



**The Fix**: Always commit Constitution BEFORE starting `/sp.specify`.



* * *



## Try With AI



Validate your Constitution and understand how quality rules cascade:



**Test Constitution Completeness:**



> "Review my Constitution at `.specify/memory/constitution.md`. Check: (1) Are all standards testable and specific? (2) Did I cover citation, source verification, writing clarity, plagiarism checking? (3) Are any standards unrealistic? Suggest 2-3 improvements."



**Explore Cascade Effect:**



> "I wrote a Constitution with standards for APA citations, source verification, and zero plagiarism tolerance. Explain how these rules cascade: How do they affect my Specification? My Plan? My Implementation? Give me a concrete example."



**Test Specification Alignment:**



> "Based on my Constitution rules, what constraints MUST a specification for my first paper section include? Walk through: citation requirements, source types, verification steps, and quality standards."



# Constitution Phase â€” Project-Wide Quality Standards



This lesson teaches students how to create a Constitution using `/sp.constitution`â€”a document that defines project-wide quality standards applying to ALL work in a project. Students run the command, review the generated constitution, improve it with testable criteria, then commit it to git. The key distinction: Constitution applies to ALL papers (global rules), Specification applies to ONE paper (feature-specific requirements). Students learn that weak constitutions produce vague downstream work, while strong constitutions cascade quality through every phase.



### Mental Models



-   **Constitution vs Specification**: Constitution = global rules for ALL features ("all papers must cite primary sources, APA format, zero plagiarism"). Specification = rules for ONE feature ("this paper's thesis is X, due December 15"). Constitution written once; Specification written per feature.

    

-   **The Cascade Effect**: Clear Constitution â†’ Clear Specification â†’ Clear Plan â†’ Clear Tasks â†’ Quality Implementation. Each phase respects Constitution constraints without re-specifying them. Weak Constitution produces specs without citation requirements, plans without verification steps, papers with uncited claims.

    

-   **Testability Principle**: Standards must be measurable, not subjective. âŒ Vague: "Papers should be well-written." âœ… Testable: "Flesch-Kincaid grade 10-12; active voice 75%+ of time; all claims cited."

    



### Key Patterns



-   **Run `/sp.constitution` with project requirements**: Provide core principles, key standards, constraints, and success criteria. Agent generates comprehensive constitution file at `.specify/memory/constitution.md`.

    

-   **Review for testability**: After generation, check: Are all standards testable (not vague)? Are essential categories covered (citation, verification, clarity, plagiarism)? Are any standards unrealistic?

    

-   **Commit before feature work**: Always `git commit` the Constitution BEFORE running `/sp.specify`. This establishes Constitution as documented standard with full traceability.

    

-   **Constitution-First Workflow**: Initialize project â†’ Write Constitution (once) â†’ Commit to git â†’ FOR EACH PAPER: `/sp.specify` â†’ `/sp.clarify` â†’ `/sp.plan` â†’ `/sp.tasks` â†’ `/sp.implement` â†’ commit.

    



### Common Mistakes



-   **Copying Constitution without customization**: Using example Constitution as-is. A code project Constitution mentions "type hints"â€”irrelevant for papers. Read examples for structure, write rules specific to YOUR project.

    

-   **Vague quality standards**: "Papers must be good quality" or "Sources should be credible." These are subjective and unverifiable. Use testable criteria: "Flesch-Kincaid grade 10-12; all claims cited; zero plagiarism."

    

-   **Forgetting to commit Constitution**: Creating Constitution then starting `/sp.specify` without committing. Constitution becomes "whatever I remember" instead of documented standard. Always commit BEFORE starting feature work.

    



### Progression Context



-   **Builds on**: Lesson 2 (Installation and Setup) where students installed Spec-Kit Plus and initialized their project. The constitution file from `specifyplus init` is a starter templateâ€”this lesson expands it.

    

-   **Leads to**: Lesson 4 (Specification Phase) where students run `/sp.specify` to write requirements for their first paper. The Specification must respect Constitution standards without re-specifying them.

    



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/SDD-RI-Fundamentals/spec-kit-plus-hands-on/constitution-phase

</constitution>









<specify-phase>

-   [](/)

-   [Part 4: SDD-RI Fundamentals](/docs/SDD-RI-Fundamentals)

-   [Chapter 14: Master Spec-Kit Plus](/docs/SDD-RI-Fundamentals/spec-kit-plus-hands-on)

-   Specify Phase â€” Writing Complete Specifications



# Specify Phase â€” Writing Complete Specifications



Welcome to the most important phase: Specification. This is where you translate your vague ideas ("I want to write a research paper") into crystal-clear requirements that AI can build from.



Here's the paradigm shift: **In AI-native development, your ability to write a clear specification is more valuable than your ability to write code.** Bad code can be refactoredâ€”but a bad spec breaks everything downstream. In this lesson, you'll learn how to collaborate with AI to define success and turn it into a complete, testable specification.



* * *



## Thinking About What Success Looks Like



Before writing a specification, define criteria for evaluating whether the solution succeeds. This is NOT a formal phase; it's an informal human-AI conversation that clarifies intent.



Not "does it compile?" or "do tests pass?" (those are technical validation). Real evals are business/user criteria that define success:



**For a research paper:**



-   âœ… "Paper defines 3+ concrete AI applications in K-12 classrooms"

-   âœ… "All claims supported by peer-reviewed sources"

-   âœ… "Reader can explain ROI of classroom AI after reading"

-   âœ… "Paper passes plagiarism check with 0% match"



**For any document:**



-   âœ… "Audience can complete the intended action after reading"

-   âœ… "All factual claims are traceable to sources"

-   âœ… "Structure follows the established format requirements"



* * *



## Part A: Pre-Specification Conversation



Here's how professionals work: **YOU drive the exploration through questions.** The AI doesn't interview you; you use the AI to think through requirements.



### Step 1: Start the Conversation



Open your AI tool in your `my-research-paper` directory:



```

I want to write a research paper on AI's impact on K-12 classroom efficiency.Before I write the formal spec, help me think through:1. What aspects of "AI in education" should I focus on?2. Who is my target audience?3. What does success look like for this paper?4. What edge cases or constraints should I consider?

```



**Example conversation:**



```

You: "I want to write a research paper on AI in K-12 education.      What aspects should I focus on?"AI: "Key focus areas for K-12 AI research:     - Teacher workload reduction (grading, planning automation)     - Student engagement improvements (adaptive learning)     - Administrative efficiency (scheduling, resource allocation)     - Learning outcome measurement (assessment analytics)     Which resonates with your goals?"You: "I'm interested in teacher workload and student outcomes.      What would success look like for this paper?"AI: "For a paper on those topics, success criteria might be:     - Identifies 3+ specific AI applications with evidence     - Cites peer-reviewed sources (8+ minimum for academic credibility)     - Reader understands economic ROI (time saved, outcomes improved)     - Clear enough that school administrators could act on recommendations"You: "What constraints should I consider?"AI: "Key constraints for academic papers:     - Word count: 3000-5000 words is standard for this scope     - Citation style: APA is common for education research     - Source quality: Peer-reviewed journals, recent publications (past 10 years)     - Scope boundaries: What you're NOT covering (ethics, specific products)"

```



### Step 2: Summarize What You Learned



From the conversation, you now know:



-   **Focus**: Teacher workload reduction + student outcomes

-   **Audience**: Education administrators and technology decision-makers

-   **Success**: 3+ AI applications, 8+ sources, clear ROI explanation

-   **Constraints**: 3000-5000 words, APA format, peer-reviewed sources

-   **Not covering**: Ethics debates, specific product comparisons



This becomes your specification.



* * *



## Part B: Write Your Specification



Now formalize the conversation into a specification using `/sp.specify`.



### Step 1: Run `/sp.specify`



```

/sp.specify Research paper on AI's impact on K-12 classroom efficiencyTarget audience: Education administrators evaluating AI adoptionFocus: Teacher workload reduction and student outcome improvementsSuccess criteria:- Identifies 3+ concrete AI applications with evidence- Cites 8+ peer-reviewed academic sources- Reader can explain ROI of classroom AI after reading- All claims supported by evidenceConstraints:- Word count: 3000-5000 words- Format: Markdown source, APA citations- Sources: Peer-reviewed journals, published within past 10 years- Timeline: Complete within 2 weeksNot building:- Comprehensive literature review of entire AI field- Comparison of specific AI products/vendors- Discussion of ethical concerns (separate paper)- Implementation guide or code examples

```



**What the agent does:**



-   Creates a new feature branch automatically

-   Generates comprehensive spec file at `specs/[feature-name]/spec.md`

-   Defines user scenarios and edge cases

-   Establishes acceptance criteria

-   Sets up testing requirements



### Step 2: Review the Generated Specification



After the agent creates your spec, review it:



```

Show me the generated specification and explain what each section contains.

```



**Agent shows:**



-   **Intent** â€” What you're building and why

-   **Constraints** â€” Boundaries and requirements

-   **Success Evals** â€” Measurable acceptance criteria

-   **Non-Goals** â€” What you're explicitly NOT building



### Step 3: Verify Completeness



Check that your specification has:



```

Specification Checklist:[ ] Intent is clear (someone unfamiliar can understand the goal)[ ] Constraints are specific and testable (not vague "do good work")[ ] Success Evals are SMART (Specific, Measurable, Achievable, Relevant, Time-bound)[ ] Non-Goals are explicit (prevents scope creep)[ ] No "how" leaked in (describes what, not how to build)[ ] Written clearly enough that another person could write from it

```



* * *



## Part C: The SMART Test



Success Evals must be **SMART**: Specific, Measurable, Achievable, Relevant, Time-bound.



### Test Your Success Criteria



**âŒ Vague (NOT SMART):**



```

- Paper is high-quality- Sources are credible- Writing is clear- Reader finds it valuable

```



**âœ… SMART:**



```

- Paper is between 3000-5000 words- Paper cites 8+ peer-reviewed academic sources- Each major claim is supported by evidence- Reader can explain 3 concrete AI use cases after reading- Paper completed within 2-week timeframe

```



### Ask AI to Review



```

Review my specification at specs/[feature-name]/spec.md.For each success criterion, check if it's SMART:- Specific (not vague)?- Measurable (can verify objectively)?- Achievable (realistic)?- Relevant (matters for this paper)?- Time-bound (has deadline)?Identify any vague criteria and suggest specific alternatives.

```



* * *



## Common Mistakes



### Mistake 1: Leaking Implementation Into Specification



**The Error:**



```

## Specification: Research PaperThe paper will be written using Claude AI to:1. Research the topic2. Outline the structure3. Generate each section based on the outline4. Ask Claude to review and refine

```



**Why It's Wrong:** This is IMPLEMENTATION (HOW). Specification is WHAT.



**The Fix:** Keep specification focused on outcomes, not process:



```

## Intent3000-5000 word research paper on AI in K-12 education.## Success Evals- Paper identifies 3+ concrete AI applications- At least 8 citations from peer-reviewed sources- Each claim supported by evidence

```



### Mistake 2: Vague Success Criteria



**The Error:** "Paper should be well-written" or "Sources should be credible"



**Why It's Wrong:** "Well-written" and "credible" are subjective. No one can verify these.



**The Fix:** Use testable criteria:



-   âŒ Vague: "Paper should be well-researched"

-   âœ… SMART: "Paper cites 8+ peer-reviewed sources; average publication date within past 10 years"



### Mistake 3: Missing Non-Goals



**The Error:** Only specifying what you ARE building, not what you're NOT building.



**Why It's Wrong:** Scope creep happens. AI might add "helpful" sections you didn't want.



**The Fix:** Explicitly state boundaries:



```

## Non-Goals- Not a comprehensive literature review (focused analysis only)- Not a comparison of specific AI products- Not an implementation guide- No discussion of ethical concerns (separate paper)

```



* * *



## Validation: The Cascade Effect



Now test your specification's quality by asking: **Will this spec produce a good plan?**



**A good spec has:**



-   âœ… Crystal-clear intent (no ambiguity)

-   âœ… Explicit constraints (no surprises during planning)

-   âœ… Measurable success criteria (AI can build acceptance tests from them)

-   âœ… Constitution alignment (specification respects Constitution standards)



**A bad spec has:**



-   âŒ Vague intent ("should work correctly"â€”what's "correct"?)

-   âŒ Missing constraints (surprises emerge during implementation)

-   âŒ Ambiguous criteria ("handle errors"â€”how?)

-   âŒ Ignores Constitution (specification asks for things Constitution forbids)



**Action:** Read your specification aloud. Does it sound clear? Would someone else understand exactly what to write?



* * *



## Try With AI



Ready to write your specification? Practice with your AI companion:



**Explore Success Criteria:**



> "I want to write a research paper on \[your topic\]. Before I write the formal spec, help me define success: What does 'good' mean for this paper? What would prove it's valuable? Don't let me write vague requirements like 'well-researched'â€”push me toward specific, measurable criteria."



**Practice SMART Criteria:**



> "Review my specification at specs/\[feature-name\]/spec.md. For each success criterion, check if it's SMART (Specific, Measurable, Achievable, Relevant, Time-bound). Identify vague criteria like 'good quality' and suggest specific alternatives."



**Test Specification Completeness:**



> "Looking at my spec, identify what's missing: (1) Are constraints explicit? (2) Are non-goals defined? (3) Could someone else write this paper from just the spec? (4) Does it respect my Constitution standards? Generate a list of improvements."



**Apply to Your Paper:**



> "Help me write a specification for my research paper on \[topic\]. Walk me through: (1) What exactly am I building? (2) Who is the audience? (3) What does success look likeâ€”specifically? (4) What constraints apply? (5) What am I NOT covering? Then run /sp.specify with everything we discussed."



# Specify Phase â€” Writing Complete Specifications



This lesson teaches students how to transform vague project ideas into clear, measurable specifications through a two-part workflow: (1) Pre-specification conversation where YOU drive exploration through questions with AI, then (2) Formalize into specification using `/sp.specify`. Students learn that specification answers "What are we building?" with Intent, Constraints, Success Evals, and Non-Goalsâ€”not "How" (that's planning). The key paradigm shift: your ability to write a clear specification is more valuable than your ability to write code.



### Mental Models



-   **Pre-Specification Conversation**: Before writing formal specs, have an informal conversation with AI to clarify intent. YOU drive the exploration through questions ("What aspects should I focus on? Who is my audience? What does success look like?"). The AI doesn't interview you; you use AI to think through requirements.

    

-   **SMART Criteria Filter**: Success Evals must be Specific, Measurable, Achievable, Relevant, Time-bound. "Paper is high-quality" fails (subjective). "Paper cites 8+ peer-reviewed sources within past 10 years" passes (verifiable).

    

-   **What vs How Boundary**: Specifications describe WHAT (outcomes). Plans describe HOW (process). If you're writing about steps, sequence, or toolsâ€”you're in planning, not specification.

    

-   **Cascade Effect**: Clear spec â†’ clear plan â†’ clear tasks â†’ quality output. Vague spec breaks everything downstream. Test your spec by asking: "Will this produce a good plan?"

    



### Key Patterns



-   **Two-Part Workflow**: (1) Pre-specification conversation to explore requirements, then (2) `/sp.specify` to formalize into structured document. Never skip the conversationâ€”it surfaces constraints and edge cases.

    

-   **Four-Section Structure**: Intent (what problem, why, audience) â†’ Constraints (limits, format, scope) â†’ Success Evals (measurable done-ness) â†’ Non-Goals (explicit out-of-scope). This structure works for any project type.

    

-   **Specification Checklist**: After generating spec, verify: Intent clear? Constraints specific? Success Evals SMART? Non-Goals explicit? No HOW leaked? Could someone else build from this?

    

-   **Ask AI to Review**: After `/sp.specify`, prompt: "For each success criterion, check if it's SMART. Identify vague criteria and suggest specific alternatives."

    



### Common Mistakes



-   **Leaking Implementation**: Writing "Use Claude AI to research, outline, generate sections" in spec. This is HOW, not WHAT. Keep spec focused on outcomes: "5000-word paper with 8+ sources."

    

-   **Vague Success Criteria**: "Paper is high-quality" or "sources are credible" can't be verified objectively. Use testable criteria: "8+ peer-reviewed sources; APA formatting; 3000-5000 words."

    

-   **Missing Non-Goals**: Not specifying what you're NOT building causes scope creep. Explicitly state: "Not a comprehensive literature review; not a product comparison; no ethical discussion."

    

-   **Skipping Pre-Specification Conversation**: Jumping straight to `/sp.specify` without clarifying intent, audience, and success criteria. The conversation surfaces requirements you'd otherwise miss.

    



### Progression Context



-   **Builds on**: Lesson 3 (Constitution Phase) established global rules. Lesson 4 applies those rules to a specific feature specification.

    

-   **Leads to**: Lesson 5 (Clarify Phase) uses `/sp.clarify` to identify gaps and ambiguities in the specification. Lesson 6 (Plan Phase) takes the clarified specification and designs HOW to build it.

    



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/SDD-RI-Fundamentals/spec-kit-plus-hands-on/specify-phase

</specify-phase>









<clarify-phase>

-   [](/)

-   [Part 4: SDD-RI Fundamentals](/docs/SDD-RI-Fundamentals)

-   [Chapter 14: Master Spec-Kit Plus](/docs/SDD-RI-Fundamentals/spec-kit-plus-hands-on)

-   Clarify Phase



# Clarify Phase



In Lesson 04, you wrote a specification for your research paper. It looked complete. But there are always gaps you didn't catchâ€”ambiguities that seemed clear in your head but are actually vague on paper. Assumptions about scope, audience, or success that you didn't state explicitly.



This is where the `/sp.clarify` command helps. **Clarify is a quick check** that your specification is complete before moving to planning.



Think of `/sp.clarify` as your AI companion putting on a "detail detective" hat and asking: "Wait, who exactly is your audience? What counts as a 'well-researched' paper? How many sources is enough? What format should this follow?" It finds gaps you might have missed, then you decide whether to update your spec.



The goal: Make your specification **so clear** that the planning phase can generate a perfect implementation plan.



* * *



## What Does /sp.clarify Do?



### The Clarify Command



`/sp.clarify` analyzes your specification and reports:



1.  **Ambiguous Terms** - Words that could mean multiple things

    

    -   Example: "well-researched paper" (5 sources? 50 sources? What counts as credible?)

    -   Example: "professional format" (APA? MLA? Chicago? Single-spaced? Double-spaced?)

2.  **Missing Assumptions** - Things you assumed but didn't state

    

    -   Example: You assumed academic paper but didn't specify citation style

    -   Example: You assumed 3,000-word length but didn't state minimum or maximum

    -   Example: You assumed English but didn't specify if other languages acceptable

3.  **Incomplete Requirements** - Scenarios or cases you didn't cover

    

    -   Example: You specified content but didn't specify editing or revision process

    -   Example: You specified research but didn't specify how to handle conflicting sources

    -   Example: You specified "introduce topic" but didn't define what introduction contains

4.  **Scope Conflicts** - Places where scope is unclear or inconsistent

    

    -   Example: "Comprehensive research" on what exactly? All historical context or just recent developments?

    -   Example: "Clear structure" using what organization method? Chronological? Thematic?

    -   Example: "Compelling conclusion" appealing to whom? Academic audience? General readers?



### Why Clarify Matters Before Planning



A vague specification creates a vague plan. When the planning phase can't understand exactly what you want, it generates ambiguous design decisions. Then you spend time during implementation realizing your actual intention wasn't captured.



**Without clarification** (vague spec):



```

Intent: Write research paper on climate changeSuccess Criteria: Paper is well-researched and professionally written

```



Planning phase has questions:



-   What specific aspect of climate change? Global warming trends? Policy solutions? Historical development?

-   How many sources? Academic only or include journalistic sources?

-   What length? 2 pages? 10 pages? 50 pages?

-   What citation style? Who's the audience?



**With clarification** (precise spec):



```

Intent: Write research paper on climate policy solutions adopted since 2015Success Criteria:- Minimum 5 peer-reviewed sources (journals, not news)- APA format, 3,000-4,000 words- Three policy solutions compared (effectiveness, adoption barriers, future outlook)- Audience: undergraduate economics students- Conclusion: assessment of which policy approach shows most promise

```



Planning phase now has clear requirements and generates specific implementation tasks.



* * *



## The Clarify Workflow



### Step 1: Run /sp.clarify



In Claude Code, from your research-paper directory:



```

/sp.clarifyMy research paper specification is at specs/paper/spec.mdPlease analyze it for:1. Ambiguous terms (what does "well-researched" mean? How many sources? What type?)2. Missing assumptions (citation style? audience? paper length? structure?)3. Incomplete requirements (what does "introduce topic" contain? how to handle conflicting sources? revision process?)4. Scope conflicts (is this historical overview or current policy analysis? broad or narrowly focused?)What gaps should I address before planning the paper structure?

```



Your AI companion will analyze your specification, identify gaps or ambiguities, and ask clarifying questions. Review its findings and consider which gaps are critical versus nice-to-have.



### Step 2: Update Your Specification



For each clarifying question, decide: **Do I need to answer this before planning?**



-   **Critical gaps** (planning can't work without this): Update spec immediately

    

    -   Example: Citation style is critical (affects all references)

    -   Example: Paper length is critical (determines research scope)

    -   Example: Audience is critical (determines tone and complexity)

-   **Nice-to-have clarifications** (planning can proceed): Update spec or defer

    

    -   Example: Specific revision timeline

    -   Example: Preferred formatting tools

    -   Example: Aesthetic preferences



### Step 3: Re-Run /sp.clarify (Optional)



If you made significant changes, run `/sp.clarify` again:



```

I've updated my research paper specification based on your feedback.Please analyze it again for remaining gaps.Is this specification clear enough to proceed to the planning phase?

```



Most specifications need 1-2 clarification rounds. After that, they're ready for planning.



* * *



## Clarifying Your Paper Specification



Now let's clarify YOUR research paper specificationâ€”the one you wrote in Lesson 04.



### Step 1: Run /sp.clarify on Your Specification



In Claude Code, from your research-paper directory, run:



```

/sp.clarifyMy research paper specification is at specs/paper/spec.mdPlease analyze it for:1. AMBIGUOUS TERMS   - What does "well-researched" mean in my spec? (how many sources? which types?)   - What does "professional format" mean? (which citation style? spacing? margins?)   - What does "clear structure" mean? (how many sections? what should each contain?)2. MISSING ASSUMPTIONS   - What citation style should I use? (APA, MLA, Chicago, Harvard?)   - What's the target audience? (academic, general readers, specific field?)   - What's the paper length? (minimum and maximum word count?)   - How recent should sources be? (published in last 5 years? 10 years?)3. INCOMPLETE REQUIREMENTS   - What should the introduction contain? (background? thesis statement? scope?)   - How do I handle conflicting sources? (which viewpoints to include?)   - What constitutes a "credible" source? (peer-reviewed only? news acceptable?)   - How should I structure the paper? (chronological? thematic? by source?)4. SCOPE CONFLICTS   - Is this narrowly focused on one aspect or broadly covering the topic?   - Is this historical overview or current-state analysis?   - Are there sub-questions I should address or exclude?List any gaps or questions. Which ones are CRITICAL (planning won't work without them) vs NICE-TO-HAVE (improve quality but not blocking)?

```



### Step 2: Evaluate Feedback



Review the clarifying questions your AI companion identified. For each one, ask:



-   Is this critical to planning the paper structure?

-   Can planning proceed without this answer, or does it affect section design?

-   Should I resolve this now or defer it?



### Step 3: Update Your Specification



Update your spec.md with the clarifications you decide are critical. You might add:



```

Audience: Undergraduate economics students (not specialized researchers)Citation Style: APA format, 7th editionLength: 3,000-3,500 wordsSource Types: Peer-reviewed journals (80%), reputable news sources (20%)Structure: Introduction â†’ Problem Analysis â†’ Three Solutions â†’ Comparison â†’ Conclusion

```



### Step 4: Verify Readiness



Ask your AI companion:



```

Based on the clarifications I've made, is my research paper specification now ready for the planning phase?Can you explain the paper structure and success criteria back to me to confirm we're aligned?

```



* * *



## Why Clarification Prevents Implementation Problems



Skipping clarification creates cascading problems during implementation:



**Missing specification clarity** â†’ **Vague planning decisions** â†’ **Confused implementation tasks** â†’ **Rework and frustration**



Here's how clarification breaks this chain:



1.  You run `/sp.clarify` and discover ambiguity: "What's the minimum number of sources?"

2.  You update spec: "Minimum 6 peer-reviewed sources"

3.  Planning phase generates clear implementation task: "Research and select 6+ peer-reviewed sources on \[topic\]"

4.  Implementation proceeds smoothly because the requirement is explicit



* * *



## Common Mistakes



### Mistake 1: Skipping /sp.clarify Because "Spec Looks Good to Me"



**The Error**: "I wrote a detailed spec. I don't need clarification."



**Why It's Wrong**: Every specification has ambiguities you didn't notice. Clarify surfaces them now (5 minutes) instead of during implementation (5 hours).



**The Fix**: Always run `/sp.clarify`. You'll be surprised what gaps emerge. Most specs need 1-2 clarification rounds.



### Mistake 2: Ignoring Critical Clarifications



**The Error**: "AI asked about citation style but I'll just figure that out later."



**Why It's Wrong**: Citation style affects every source reference. Deferring this decision means planning the paper structure without knowing how citations work, then discovering mid-implementation you chose wrong.



**The Fix**: Address critical gaps upfront. Test: "If planning didn't know this, would they make a different choice?" If yes, it's critical.



### Mistake 3: Accepting All AI Suggestions Without Thinking



**The Error**: AI suggests adding source diversity requirements â†’ immediately adding without evaluating necessity



**Why It's Wrong**: Not all suggestions improve your spec. Some add unnecessary complexity.



**The Fix**: Evaluate each suggestion:



-   Is this critical to paper quality or nice-to-have?

-   Does this affect planning or just implementation?

-   Can I defer this to revision?



Then decide: Accept, Reject, or Modify.



* * *



## Try With AI



Ready to clarify your research paper specification? Test these prompts:



**Explore Specification Gaps:**



> "I'm ready to run /sp.clarify on my research paper specification. Before I do, what are the most common gaps in paper specifications? What questions should I expect the clarification process to surface?"



**Interpret Clarification Feedback:**



> "Here are the clarifying questions my AI identified about my paper spec: \[paste the questions\]. Help me categorize them: (1) Which are CRITICAL (planning won't work without this)? (2) Which are NICE-TO-HAVE (improve quality but not blocking)? (3) Which can I defer? Explain how each critical gap affects the planning phase."



**Validate Clarity:**



> "I've updated my research paper specification based on clarification feedback. Read my updated spec and tell me: (1) Is the paper scope clear? (2) Would a planner know what paper structure to design? (3) Are there any remaining ambiguities? (4) Is this specification ready for the planning phase?"



**Practice Decision-Making:**



> "My clarification feedback included suggestions about \[topic\]. Help me decide whether to address each suggestion now: Is it critical for planning? Does it affect success criteria? Can I defer it to revision? Walk me through your decision framework."



* * *



# Clarify Phase



The `/sp.clarify` command systematically identifies gaps, ambiguities, and missing assumptions in specifications before the planning phase. Students learn to recognize specification weaknesses (vague terms, unstated assumptions, incomplete requirements, scope conflicts) and iteratively refine specs through AI feedback, preventing cascading problems during implementation and planning.



## Mental Models



-   **Detail Detective Pattern**: `/sp.clarify` acts as a critical reader that identifies gaps you didn't catchâ€”ambiguities that seemed clear in your head but are vague on paper (assumptions about scope, audience, success, terminology, requirements, scenarios).

    

-   **Specification Clarity â†” Plan Quality**: Vague specifications produce vague plans; precise specifications enable specific implementation tasks. Clarification is the bridge between intent and actionable design.

    

-   **Critical vs. Nice-to-Have**: Distinguish gaps that block planning (audience, citation style, length requirements) from improvements that enhance quality (formatting preferences, timeline details). Prioritize critical gaps upfront; defer nice-to-have clarifications.

    

-   **Iterative Specification Refinement**: Most specifications require 1-2 clarification rounds. Each round surfaces remaining gaps, iteratively improving clarity until the spec is ready for planning.

    



## Key Patterns



-   **Four Gap Categories** (identified by `/sp.clarify`):

    

    -   Ambiguous Terms (undefined concepts like "well-researched," "professional format")

    -   Missing Assumptions (unstated scope: citation style, audience, word count, source recency)

    -   Incomplete Requirements (unspecified scenarios: how to handle conflicts, what sections contain, revision process)

    -   Scope Conflicts (unclear focus: historical vs. current, broad vs. narrow, what's in/out)

-   **Critical Gap Recognition**: Test whether planning can proceed without this answerâ€”if planning would make different choices without it, it's critical and should be addressed upfront.

    

-   **Ambiguity Prevention Chain**: Specification clarity â†’ Planning precision â†’ Implementation clarity â†’ Smooth execution. Skipping clarification creates cascading problems: vague planning â†’ confused tasks â†’ rework.

    

-   **AI-Driven Gap Discovery**: `/sp.clarify` systematically analyzes specs and returns categorized feedback; students evaluate each finding and decide to accept, reject, or modify the suggestion.

    



## Common Mistakes



-   **Skipping clarification because "spec looks good"**: Every specification has ambiguities you didn't notice. Clarify surfaces them in minutes rather than hours during implementation rework.

    

-   **Ignoring critical clarifications for later**: Deferring decisions that affect all downstream work (citation style, paper length, audience) means planning proceeds without understanding critical constraints, requiring rework mid-implementation.

    

-   **Accepting all AI suggestions without evaluation**: Not all clarification suggestions improve the spec; some add unnecessary complexity. Evaluate each suggestion before accepting: Is it critical? Does it affect planning? Can it be deferred?

    

-   **Over-clarifying (nice-to-have details)**: Treating every suggestion as critical blocks progress. Some details (formatting tools, aesthetic preferences) don't affect planning and can be deferred or omitted.

    



## Progression Context



-   **Builds on**: Lesson 04 (Specify Phase) â€” Students have written initial specifications. Clarify phase validates specification quality before moving to planning.

    

-   **Leads to**: Lesson 06 (Plan Phase) â€” Clear, gap-free specifications enable planning phase to generate precise implementation tasks. Clarification ensures planning has all information needed for design decisions.

    



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/SDD-RI-Fundamentals/spec-kit-plus-hands-on/clarify-phase

</clarify-phase>









<plan-phase>

-   [](/)

-   [Part 4: SDD-RI Fundamentals](/docs/SDD-RI-Fundamentals)

-   [Chapter 14: Master Spec-Kit Plus](/docs/SDD-RI-Fundamentals/spec-kit-plus-hands-on)

-   Plan Phase â€” Architecture Decisions and ADRs



# Plan Phase â€” Architecture Decisions and ADRs



With your specification complete and clarified, you now face a new question: **How will you actually build it?** This is the essence of the Plan Phaseâ€”transforming the "What" of your specification into the "How" of architecture and implementation strategy.



`/sp.plan` generates an implementation plan that breaks your specification into:



-   **Architectural components** (sections, research management, quality validation)

-   **Implementation phases** (research first, then writing, then polish)

-   **Dependencies** (what must be completed before what)

-   **Design decisions** (which ones matter enough to document)



This lesson teaches you how to work with generated plans and how to capture important architectural decisions using **ADRs (Architectural Decision Records)**.



* * *



## Understanding the `/sp.plan` Command



`/sp.plan` analyzes your specification and generates a detailed implementation plan by:



1.  **Breaking spec into components** â€” Which parts of your spec need separate phases?

2.  **Ordering dependencies** â€” What must be built first?

3.  **Identifying design decisions** â€” Where are there multiple valid approaches?

4.  **Proposing architecture** â€” How should work be organized?



**Input**: Your specification (what the paper must accomplish)



**Output**: Implementation plan with:



-   Architecture overview

-   Implementation phases

-   Component breakdown

-   Dependencies and sequencing

-   Design decisions highlighted



**The Cascade Effect**: Detailed spec â†’ detailed plan. Vague spec â†’ vague plan.



* * *



## Part A: Generating Your Implementation Plan



Let's generate the plan for your research paper.



### Step 1: Run `/sp.plan`



In your AI tool, from your `my-research-paper` directory:



```

/sp.planCreate: architecture sketch, section structure, research approach, quality validation.Decisions needing documentation: list important choices with options and tradeoffs.Testing strategy: validation checks based on acceptance criteria.Technical details:- Use research-concurrent approach (research while writing, not all upfront)- Follow APA citation style from Constitution- Organize by phases: Research â†’ Foundation â†’ Analysis â†’ Synthesis

```



**What the agent does:**



-   Creates technical implementation plan

-   Defines section structure and dependencies

-   Establishes validation strategy

-   Identifies architectural decisions

-   Generates `plan.md` file in your specs directory



**Why This Matters**: The plan defines architecture for ALL sections at once. This ensures consistencyâ€”same research approach, same citation handling, same quality validation. Much more efficient than planning each section separately!



### Step 2: Review Generated Plan



The generated plan should include:



**Architecture Overview:** How work will be organized (phases, dependencies)



**Implementation Phases:** 3-5 phases building from research to polish



**Component Breakdown:** Research management, thesis development, evidence integration, writing phases, quality gates



**Sequencing:** Research before findings? Outline before detailed writing?



**Design Decisions:** Where are there choices? (Research-first vs research-concurrent? Section order?)



### Step 3: Verify Plan Completeness



After the agent generates your plan, review it:



```

Show me the generated plan and explain:1. How does it break down the work into phases?2. What dependencies does it identify?3. What design decisions are highlighted?4. Does it account for all specification requirements?

```



**Agent shows:**



-   **Technical Approach** â€” Overall strategy for building the paper

-   **Major Components** â€” Logical parts that need to be built

-   **Dependencies** â€” What blocks what

-   **Phases** â€” Milestones with deliverables

-   **Success Criteria** â€” How you'll know each phase is complete



* * *



## Part B: Understanding ADRs (Architectural Decision Records)



Planning exposes architectural decisionsâ€”choices about HOW to build that have long-term consequences.



### What Is an ADR?



An ADR documents:



-   **The Decision**: What choice did you make?

-   **The Context**: Why did you need to make this choice?

-   **The Alternatives**: What other options existed?

-   **The Rationale**: Why did you choose this over alternatives?

-   **The Consequences**: What are the long-term impacts?



### When Should You Create an ADR?



**Create an ADR when:**



-   The decision has long-term impact (affects paper structure, not just word choice)

-   Multiple valid alternatives existed (not an obvious choice)

-   Future readers/collaborators will question the decision

-   The decision constrains future choices (e.g., choosing research approach)



**Don't create ADRs for:**



-   Style choices (formatting preferences)

-   Obvious choices (of course we use APAâ€”Constitution requires it!)

-   Temporary decisions (will revisit before submission)

-   Out-of-scope decisions (already decided by Constitution)



### Example ADR Decisions for Research Paper



Decision



ADR Needed?



Why?



Research-concurrent vs research-first approach



âœ… YES



Affects entire writing workflow



Section ordering (Lit Review before Methodology?)



âœ… YES



Affects logical flow and dependencies



APA citation style



âŒ NO



Constitution already decided this



Font choice



âŒ NO



Trivial, no long-term impact



Whether to include ethics discussion



âœ… YES



Scope decision with tradeoffs



* * *



## Part C: Creating ADRs for Your Plan



Now let's identify and document the architectural decisions from your plan.



### Step 1: Run `/sp.adr`



```

/sp.adr Review the generated plan and record key Architectural Decisions.Focus on decisions that:1. Affect multiple sections or phases2. Had alternatives we considered3. Will shape how we write the paper4. Someone might question later

```



**What the agent does:**



-   Reviews your plan.md

-   Identifies architecturally significant decisions

-   Creates ADR files in `history/adr/` directory

-   Documents context, alternatives, rationale, and consequences



### Step 2: Review Generated ADRs



After the agent creates your ADRs, examine them:



```

Show me the ADRs created. For each one, explain:1. What decision was documented?2. What alternatives were considered?3. Why was this choice made over alternatives?4. What are the consequences (both positive and negative)?

```



**Example ADR Structure:**



```

# ADR-001: Research-Concurrent Writing Approach## StatusAccepted## ContextWe need to decide when research happens relative to writing.Two approaches exist: research-first (gather all sources, then write)vs research-concurrent (research while writing each section).## DecisionWe will use research-concurrent approach.## Alternatives Considered1. **Research-first**: Gather all 8+ sources before writing anything   - Pro: Complete knowledge before writing   - Con: Delays writing, may gather irrelevant sources2. **Research-concurrent**: Research each section as we write it   - Pro: Research stays focused and relevant   - Con: May discover knowledge gaps mid-writing## RationaleResearch-concurrent keeps engagement with material high and ensuressources are directly relevant to sections being written. Risk ofknowledge gaps is mitigated by outline phase identifying key topics.## Consequences- Positive: More focused research, faster initial writing- Negative: May need additional research passes for cross-section references- Constraint: Must complete detailed outline before starting section writing

```



### Step 3: Verify ADR Completeness



Check that your ADRs:



```

ADR Completeness Checklist:[ ] Each ADR has clear context (why this decision was needed)[ ] Alternatives are documented (not just the chosen option)[ ] Rationale explains WHY this choice over others[ ] Consequences include both positives and negatives[ ] Decision is architecturally significant (not trivial)[ ] ADR would help future collaborator understand the choice

```



* * *



## Common Mistakes



### Mistake 1: Documenting Every Small Decision as ADR



**The Error**: Creating ADRs for trivial choices like "Use headings for sections" or "Put references at the end"



**Why It's Wrong**: ADRs are for architecturally significant decisions (long-term impact, multiple alternatives, future questioning). Trivial choices clutter your ADR history.



**The Fix**: Apply the three-part test:



1.  Does this have long-term consequences?

2.  Are there multiple viable alternatives?

3.  Will someone ask "why did we choose this" in 6 months?



If not all three â†’ Skip the ADR.



### Mistake 2: Vague ADR Consequences



**The Error**: ADR says "This approach is better" without explaining tradeoffs



**Why It's Wrong**: Future developers need to understand WHY you chose this and WHAT you gave up.



**The Fix**: Document both positives and negatives:



-   âœ… "Pros: More focused research. Cons: May need additional passes."

-   âœ… "Alternatives considered: Research-first (rejected: delays writing)"



### Mistake 3: Skipping the Plan Phase



**The Error**: Going straight from specification to task writing, skipping planning.



**Why It's Wrong**: You lose sight of overall architecture. Tasks become disconnected. You discover structural problems late.



**The Fix**: Always plan. The `/sp.plan` command makes this quick and automatic.



* * *



## Connecting Spec â†’ Plan â†’ Tasks



This is important: your specification, plan, and tasks form a clear chain.



**Specification says**: "Write a 3000-5000 word research paper on AI in K-12 education, APA format, 8+ sources, define 3+ concrete applications"



**Plan says**: "Structure: Research Phase â†’ Foundation Writing â†’ Analysis Writing â†’ Synthesis. Phases in order, dependencies mapped. Key decision: research-concurrent approach."



**Tasks** (next lesson) will say: "Task 1: Define thesis (15 min), Task 2: Outline all sections (30 min), Task 3: Research for Lit Review (45 min)..." etc.



Each level adds specificity:



-   **Spec**: What is success? (Measurable criteria)

-   **Plan**: How will we organize the work? (Architecture + ADRs)

-   **Tasks**: What are the 15-30 minute units? (Atomic work)



* * *



## Try With AI



Ready to generate your implementation plan and document architectural decisions?



**Generate Your Plan:**



> "I have a research paper specification. Run `/sp.plan` to generate an implementation plan. Show me: (1) The technical approach for structuring the paper, (2) Major components and phases, (3) Dependencies between components, (4) Design decisions that need documenting. Create the plan.md file."



**Analyze Plan Quality:**



> "Review my generated plan at specs/\[feature-name\]/plan.md. Does it: (1) Match the specification's requirements? (2) Show clear dependencies? (3) Break work into logical phases? (4) Identify parallel work opportunities? Where could the plan be clearer?"



**Create ADRs:**



> "Run `/sp.adr` to review my plan and record key Architectural Decisions. Focus on decisions that: (1) Affect multiple sections, (2) Had alternatives we considered, (3) Will shape how we write the paper. For each ADR, document context, alternatives, rationale, and consequences."



**Test Plan-to-Tasks Readiness:**



> "Based on my plan, simulate breaking it into tasks. For each implementation phase, can you create 3-5 atomic tasks? If you struggle to create clear tasks, identify which parts of my plan are too vague and need more detail. This tests if my plan is detailed enough for the Tasks phase."



# Plan Phase â€” Architecture Decisions and ADRs



This lesson teaches students to transform specifications into implementation plans using `/sp.plan`, then document key architectural decisions using `/sp.adr`. Students run the plan command, review the generated architecture (components, phases, dependencies), and create ADRs for decisions that have long-term impact, multiple alternatives, and will be questioned later. The lesson emphasizes the cascade effect: clear spec â†’ clear plan. Plans answer HOW we'll build what the spec defines, bridging intent to executable tasks.



### Mental Models



-   **Cascade Effect**: Specification quality determines plan quality. Detailed spec â†’ clear plan with well-defined components. Vague spec â†’ vague plan that exposes missing requirements. Plans act as quality checkpoint.

    

-   **Three-Layer Hierarchy**: Spec defines WHAT (success criteria), Plan defines HOW (architecture), Tasks define WORK UNITS (15-30 min pieces). Each layer adds specificity. Plans prevent jumping from abstract requirements to granular tasks.

    

-   **ADR as Decision Documentation**: Architectural Decision Records capture WHY choices were made, not just WHAT was decided. Include context, alternatives considered, rationale, and consequences (both positive and negative).

    

-   **ADR Significance Test**: Create ADR only if: (1) long-term consequences, (2) multiple viable alternatives existed, (3) someone will question it later. If not all three â†’ skip the ADR.

    



### Key Patterns



-   **Run `/sp.plan` with context**: Provide architecture sketch, section structure, research approach, quality validation strategy. Agent generates plan.md with components, dependencies, phases.

    

-   **Review generated plan**: Check that plan maps to all spec requirements, shows clear dependencies, breaks work into logical phases, identifies parallel opportunities.

    

-   **Run `/sp.adr` to document decisions**: Focus on decisions affecting multiple sections, with considered alternatives, that shape how work proceeds. Agent creates ADRs in `history/adr/`.

    

-   **ADR Structure**: Status â†’ Context (why decision needed) â†’ Decision â†’ Alternatives Considered (with pros/cons) â†’ Rationale (why this over others) â†’ Consequences (positives AND negatives).

    

-   **Spec â†’ Plan â†’ Tasks chain**: Each level adds specificity. Spec: measurable success criteria. Plan: architecture + ADRs. Tasks: 15-30 minute atomic units.

    



### Common Mistakes



-   **Documenting trivial decisions as ADRs**: Creating ADRs for "use headings for sections" or style choices. Apply three-part test: long-term consequences? multiple alternatives? future questioning?

    

-   **Vague ADR consequences**: "This approach is better" without tradeoffs. Document both positives AND negatives. Show what you gave up.

    

-   **Skipping plan phase**: Jumping from spec to tasks loses architecture visibility. Tasks become disconnected, structural problems emerge late.

    



### Progression Context



-   **Builds on**: Lesson 5 (Clarify Phase) where students refined specifications. Students now have complete, detailed specs ready for planning.

    

-   **Leads to**: Lesson 7 (Tasks Phase) where students break the plan into atomic 15-30 minute work units. Plan's components and phases structure the task breakdown.

    



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/SDD-RI-Fundamentals/spec-kit-plus-hands-on/plan-phase

</plan-phase>









<tasks-phase>

-   [](/)

-   [Part 4: SDD-RI Fundamentals](/docs/SDD-RI-Fundamentals)

-   [Chapter 14: Master Spec-Kit Plus](/docs/SDD-RI-Fundamentals/spec-kit-plus-hands-on)

-   Tasks Phase - Atomic Work Units and Checkpoints



# Tasks Phase - Atomic Work Units and Checkpoints



You now have:



-   âœ… A clear research paper specification (intent, success criteria, scope)

-   âœ… A detailed implementation plan (research approach, structure, timeline)

-   âœ… Documented architecture decisions (citation style, source strategy, outline format)



Next: Break the plan into **atomic work units** (tasks) that you'll execute. Each task is 15-30 minutes, has one acceptance criterion, and produces a verifiable output.



This lesson teaches the **checkpoint pattern**â€”the critical workflow practice that keeps YOU in control. The pattern is:



```

Agent: "Here's Section 1 research complete"You: "Review... sources are credible!"You: "Commit to git"You: "Tell me what's next"Agent: "Section 2 outline refinement starting"

```



NOT:



```

Agent: "Here's everything at once" (no human control)

```



The difference is huge. Checkpoints keep you in control and catch issues early before wasting time on downstream tasks.



* * *



## What Are Tasks?



A **task** is a unit of work that:



-   Takes 15-30 minutes to complete

-   Has a single, clear acceptance criterion

-   Depends on specific other tasks

-   Produces one verifiable output (file, section, validated state)



### Task Properties



**Size**: 15-30 minutes



-   Too small (under 10 minutes) = too many micro-tasks, checkpoint overhead

-   Too large (over 45 minutes) = hard to review, hard to fix if wrong

-   Just right (15-30) = meaningful progress, reviewable scope



**Criterion**: Single, testable



-   "Research section 1 sources and verify credibility" âœ…

-   "Research section 1 AND outline section 1 AND find images" âŒ (three things)

-   "Work on research stuff" âŒ (untestable)



**Independence**: Can be reviewed individually



-   Doesn't require other tasks to be done first

-   Or clearly depends on specific other tasks



**Clarity**: Defines exact acceptance criterion that signals completion



-   âœ… "Section 1 has 5+ credible sources (peer-reviewed or authoritative), each with full citation"

-   âŒ "Section 1 is researched"



* * *



## The Checkpoint Pattern (CRITICAL)



This is **the most important concept** in this lesson. The checkpoint pattern is how you maintain control of the workflow.



### Pattern Definition



```

Loop:  1. Agent: "I've completed Phase X (description of output)"  2. Human: "Review the work (output visible and testable)"  3. Human: "APPROVE" â†’ Commit to git  4. Human: "Tell me next phase"

```



### Why Checkpoints Matter



**Without Checkpoints** (dangerous):



```

You: "Write my research paper"Agent: "Done! I've completed 15 tasks, researched all sections,        synthesized 50 sources, written full paper, formatted        everything. All automated. You're welcome."You: "Wait, which sources did you use? Is section 3 accurate?      How do I verify what you wrote?"Agent: "Already committed. Sorry! Check it now?"

```



**With Checkpoints** (controlled):



```

You: "Start research paper workflow"Agent: "Phase 1 (Section 1 Research) complete:        âœ“ 5 credible sources identified        âœ“ Notes summarizing key points        âœ“ Citations formatted        Ready for review."You: "Read sources... all high-quality. Commit. What's next?"Agent: "Phase 2 (Section 1 Outline) - Key points ordered..."You: "Found issue with point sequence. Fixing..."Agent: "Phase 3 (Section 2 Research) - Starting literature review..."You: "Paper structure looks good so far. Paper complete!"

```



### Your Role in Each Checkpoint



**Step 1: Human Reviews**



-   See the actual output (written section, bibliography, research notes)

-   Ask: "Does this match the plan?"

-   Ask: "Are there accuracy issues I should fix before continuing?"

-   Ask: "Is this ready for the next phase?"



**Step 2: Human Decides**



-   Approve ("Looks good, commit")

-   Reject ("Fix this issue before continuing")

-   Request clarification ("Where did you get this source?")



**Step 3: Human Directs**



-   "What's the next phase?"

-   You initiate next phase

-   Agent doesn't autonomously continue



* * *



## Task Structure for Research Paper



Your research paper project breaks into **4 phases with 10 atomic tasks**. Here's the breakdown:



### Phase 1: Research Foundation (3 tasks, 45-60 minutes)



These tasks establish credible sources and research notes BEFORE writing.



**Task 1.1: Research Section 1 - Find Credible Sources**



-   **Duration**: 20 minutes

-   **Depends on**: Nothing

-   **What to do**: Identify 5+ credible sources for Section 1 (topic introduction)

-   **Acceptance**: "5+ sources identified; each is peer-reviewed OR from authoritative domain expert; full citations recorded"

-   **Output**: Bibliography file with 5+ sources and notes on why each is credible



**Task 1.2: Research Section 1 - Synthesize Key Points**



-   **Duration**: 15 minutes

-   **Depends on**: Task 1.1

-   **What to do**: Read sources and extract key points for Section 1

-   **Acceptance**: "3-5 key points documented; each has source attribution; points relate directly to introduction goal"

-   **Output**: Research notes file with key points and source citations



**Task 1.3: Create Outline Structure**



-   **Duration**: 15 minutes

-   **Depends on**: Task 1.2

-   **What to do**: Draft outline for all sections with main points and sub-points

-   **Acceptance**: "Outline has all 4+ sections; each section has 2-3 main points; structure flows logically"

-   **Output**: Markdown outline file showing section hierarchy



### Phase 2: Content Research and Organization (4 tasks, 60-90 minutes)



These tasks research remaining sections and organize findings.



**Task 2.1: Research Section 2 - Find Credible Sources**



-   **Duration**: 20 minutes

-   **Depends on**: Task 1.3

-   **What to do**: Identify 5+ credible sources for Section 2 (main topic)

-   **Acceptance**: "5+ sources identified; each is peer-reviewed OR from domain expert; full citations recorded"

-   **Output**: Bibliography update with Section 2 sources and credibility notes



**Task 2.2: Research Section 2 - Synthesize Key Points**



-   **Duration**: 15 minutes

-   **Depends on**: Task 2.1

-   **What to do**: Read sources and extract key points for Section 2

-   **Acceptance**: "4-6 key points documented; source attributed; points advance main argument"

-   **Output**: Research notes update with Section 2 key points



**Task 2.3: Research Section 3 and Beyond - Find Sources**



-   **Duration**: 20 minutes

-   **Depends on**: Task 2.2

-   **What to do**: Research remaining sections (conclusion, implications) and gather sources

-   **Acceptance**: "All remaining sections have 3+ credible sources each; citations recorded"

-   **Output**: Complete bibliography with all sections covered



**Task 2.4: Organize All Research Notes by Section**



-   **Duration**: 15 minutes

-   **Depends on**: Task 2.3

-   **What to do**: Consolidate all research notes; organize by section; verify coverage

-   **Acceptance**: "All sections have research notes; notes are organized by topic; no gaps identified"

-   **Output**: Organized research notes file; verified coverage checklist



### Phase 3: Writing and Synthesis (2 tasks, 60-90 minutes)



These tasks transform research into written paper.



**Task 3.1: Write and Synthesize Content**



-   **Duration**: 45 minutes

-   **Depends on**: Task 2.4

-   **What to do**: Write paper sections using research notes; synthesize findings

-   **Acceptance**: "All sections written; each section 300+ words; citations embedded; argument flows"

-   **Output**: Complete draft paper with citations



**Task 3.2: Format and Verify Citations**



-   **Duration**: 20 minutes

-   **Depends on**: Task 3.1

-   **What to do**: Apply citation style; verify all sources cited; check bibliography completeness

-   **Acceptance**: "All citations follow APA format; bibliography complete; in-text citations present"

-   **Output**: Formatted paper with verified citations; complete bibliography



### Phase 4: Review and Finalization (1 task, 30 minutes)



These tasks validate final paper quality.



**Task 4.1: Review Paper Quality and Finalize**



-   **Duration**: 30 minutes

-   **Depends on**: Task 3.2

-   **What to do**: Read complete paper; verify accuracy, clarity, argument coherence; finalize

-   **Acceptance**: "Paper reads smoothly; argument is clear; sources are credible; no typos; ready to submit"

-   **Output**: Final paper ready for submission; quality checklist completed



* * *



## Checkpoint Sequence



Here's how the workflow actually progresses with human checkpoints:



### Checkpoint 1: After Phase 1 (Research Foundation)



```

AGENT: "Research foundation complete:        âœ“ Section 1 sources identified (5 peer-reviewed articles)        âœ“ Key points synthesized from sources        âœ“ Outline structure created (4 sections, logical flow)        Ready for Phase 2: Content Research"YOU: "Review sources... all high-quality academic journals.      Read outline... structure makes sense.      Looks good! Committing Phase 1."YOU: (git commit)YOU: "Proceed to Phase 2"

```



### Checkpoint 2: After Phase 2 (Content Research)



```

AGENT: "Content research complete:        âœ“ Sections 2-4 sources identified (15+ total sources)        âœ“ Key points extracted and organized by section        âœ“ Research notes consolidated and verified for gaps        Ready for Phase 3: Writing"YOU: "Review research notes... coverage is comprehensive.      Check bibliography... all sources have full citations.      Committing Phase 2."YOU: (git commit)YOU: "Proceed to Phase 3"

```



### Checkpoint 3: After Phase 3 (Writing)



```

AGENT: "Writing complete:        âœ“ All sections written (2,500+ words total)        âœ“ Research synthesized into narrative        âœ“ Citations formatted in APA style        âœ“ Bibliography complete        Ready for Phase 4: Finalization"YOU: "Read draft... argument is clear and well-supported.      Check citations... all formatted correctly.      Committing Phase 3."YOU: (git commit)YOU: "Proceed to Phase 4"

```



### Checkpoint 4: After Phase 4 (Finalization)



```

AGENT: "Review and finalization complete:        âœ“ Paper reads smoothly, no formatting issues        âœ“ All sources properly cited        âœ“ Argument coherent from introduction to conclusion        âœ“ Quality gates: All pass        PROJECT COMPLETE: Paper ready for submission"YOU: "Final read-through... excellent quality.      All requirements met. Committing final version.      Project complete!"YOU: (final git commit)

```



* * *



## Task Dependency Graph



Here's how your tasks depend on each other:



```

Phase 1 (Research Foundation): Sequential path (each depends on previous)â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  Task 1.1: Research Section 1 Sources                      â”‚â”‚      â†“                                                      â”‚â”‚  Task 1.2: Synthesize Section 1 Key Points                 â”‚â”‚      â†“                                                      â”‚â”‚  Task 1.3: Create Outline Structure                        â”‚â”‚      â†“ [CHECKPOINT 1]                                      â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜Phase 2 (Content Research): Sequential path (each depends on previous)â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  Task 2.1: Research Sections 2+ Sources (depends on 1.3)   â”‚â”‚      â†“                                                      â”‚â”‚  Task 2.2: Synthesize Sections 2+ Key Points               â”‚â”‚      â†“                                                      â”‚â”‚  Task 2.3: Research Final Sections Sources                 â”‚â”‚      â†“                                                      â”‚â”‚  Task 2.4: Organize All Research by Section                â”‚â”‚      â†“ [CHECKPOINT 2]                                      â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜Phase 3 (Writing): Linear path (each depends on previous)â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  Task 3.1: Write and Synthesize Content (depends on 2.4)   â”‚â”‚      â†“                                                      â”‚â”‚  Task 3.2: Format and Verify Citations                     â”‚â”‚      â†“ [CHECKPOINT 3]                                      â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜Phase 4 (Finalization): Final review (depends on Phase 3)â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  Task 4.1: Review and Finalize (depends on 3.2)            â”‚â”‚      â†“ [CHECKPOINT 4 - PROJECT COMPLETE]                   â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜Legend: Each task must complete before next starts (strict dependency)        Checkpoints occur after each phase group

```



* * *



## Lineage Traceability



Can you trace a task back to specification? Try this one:



```

Specification: "Write 2,500+ word research paper on AI in education               with academic rigor and clear argument structure"  â†“Plan: "Phase 1: Establish research foundation with credible sources;       Phase 2: Organize research by section;       Phase 3: Synthesize into written paper;       Phase 4: Verify quality and finalize"  â†“Task 2.1: "Research Section 2 - Find Credible Sources"  â†“Acceptance Criterion: "5+ sources identified; each peer-reviewed OR                       from domain expert; full citations recorded"

```



If you can trace this lineage for each task, your task breakdown is well-connected to your specification.



* * *



## Common Mistakes



### Mistake 1: Tasks Too Large (45+ Minutes)



**The Error**: "Task: Complete entire section research and writing (2+ hours)"



**Why It's Wrong**: Large tasks hide complexity, delay feedback, and make checkpoints meaningless. You can't validate progress until the entire section completes.



**The Fix**: Break into atomic units (15-30 minutes each):



-   âŒ Large: "Research and write section 1"

-   âœ… Atomic: "Find sources" (20 min), "Synthesize points" (15 min), "Outline structure" (15 min), "Write section" (45 min)



### Mistake 2: Combining Research and Writing



**The Error**: Task includes "research section, synthesize findings, write content, format citations" all as one task



**Why It's Wrong**: If you find issues with source credibility mid-task, you can't easily restart research without redoing writing. Mixing research + writing confuses where quality issues originate.



**The Fix**: Separate research from writing:



-   Task 2.1: "Research Section 2 Sources" (automation/research)

-   Task 2.2: "Synthesize Section 2 Points" (analysis)

-   **CHECKPOINT**: Human reviews research quality before continuing

-   Task 3.1: "Write and Synthesize Content" (composition)



### Mistake 3: Vague Acceptance Criteria



**The Error**: "Task: Section 1 is researched" (what does "researched" mean?)



**Why It's Wrong**: You won't know if the task is done or if there's a hidden gap.



**The Fix**: Make acceptance criteria specific and testable:



-   âœ… "Section 1 has 5+ peer-reviewed sources AND each source has full citation AND notes summarize key points"

-   âœ… "Paper has 2,500+ words AND all sources cited AND bibliography complete AND no formatting errors"



* * *



## What Makes /sp.tasks Powerful



The `/sp.tasks` command analyzes your specification and plan, then **generates a complete task breakdown** that includes:



1.  **Atomic Unit Definition** - Each task is 15-30 minutes with one acceptance criterion

2.  **Dependency Ordering** - Tasks ordered so dependencies are clear

3.  **Checkpoint Placement** - Human review points between phases

4.  **Lineage Traceability** - You can trace each task back to specification

5.  **Acceptance Criteria** - Each task has specific, testable completion condition



You don't write tasks from scratch. **`/sp.tasks` writes them for you** based on your specification and plan. Your job is to **understand the task structure, validate it's atomic, and execute it with checkpoints**.



* * *



## Try With AI



Ready to validate your task breakdown and understand how `/sp.tasks` works? Test your understanding:



**Explore Task Atomicity:**



> "I'm using `/sp.tasks` to break my research paper into atomic work units. Review my task list: (1) Is each task atomic (does ONE thing with ONE acceptance criterion)? (2) Are they sized right (15-30 minutes, not hours or minutes)? (3) Can each be reviewed independently? (4) Identify any tasks that should be split further or combined. (5) Which tasks would you add or remove?"



**Practice Checkpoint Validation:**



> "Walk me through the checkpoint pattern for my research paper workflow. For each checkpoint (after Phases 1, 2, 3, 4): (1) What should I review for? (2) What makes a 'good' output at this checkpoint? (3) What issues could arise that I should catch before continuing? (4) Create a checklist I can use at each checkpoint to decide 'ready to proceed'."



**Analyze Dependencies:**



> "Examine my task dependencies: (1) Are they logically correct? (2) Which tasks could theoretically run in parallel? (3) What's the critical path (minimum sequence to completion)? (4) If one task failed (e.g., couldn't find enough credible sources), which downstream tasks would be affected? (5) How would I recover and restart?"



**Understanding /sp.tasks Command:**



> "Explain what `/sp.tasks` does: (1) What INPUT does it take (spec + plan)? (2) What OUTPUT does it generate (task list structure)? (3) How does it ensure tasks are atomic? (4) How does it order tasks by dependency? (5) When would you run `/sp.tasks` in a Spec-Kit Plus workflow?"



* * *



## Tasks Phase - Atomic Work Units and Checkpoints



The `/sp.tasks` command transforms a specification and plan into a structured breakdown of atomic work units (15-30 minute tasks) with explicit dependencies and human-controlled checkpoints. This lesson teaches task atomicity (single acceptance criterion per task), the four-phase checkpoint pattern (Agent Complete â†’ Human Review â†’ Human Approve â†’ Continue), and how to maintain human control throughout execution by requiring explicit approval at each phase boundary before proceeding to the next phase.



### Mental Models



-   **Atomic Task Unit**: A 15-30 minute unit of work with a single, testable acceptance criterion that produces one verifiable output. Size constraint prevents tasks too small (micro-management) or too large (hidden complexity, delayed feedback, hard to fix mid-stream).

    

-   **Checkpoint Pattern**: Agent â†’ Review â†’ Approve â†’ Continue loop that keeps humans in control. Without checkpoints, agents can complete all tasks autonomously and you lose visibility. With checkpoints, you see each phase output, validate quality, catch issues early, and explicitly authorize the next phase before proceeding.

    

-   **Dependency Graph**: Visual representation of task ordering showing which tasks must complete before others can start (strict sequential path for research paper: Phase 1 â†’ Phase 2 â†’ Phase 3 â†’ Phase 4, with sub-task dependencies within each phase).

    

-   **Lineage Traceability**: Ability to trace any task backward through the chain: Specification â†’ Plan â†’ Task â†’ Acceptance Criterion. If traceability breaks, the task may be solving the wrong problem or may not be needed for specification fulfillment.

    



### Key Patterns



-   **Four-Phase Task Structure**: Research Foundation (sources + outline) â†’ Content Research (gather all materials) â†’ Writing and Synthesis (transform research into prose) â†’ Review and Finalization (quality validation). Each phase ends with human checkpoint.

    

-   **Separation of Concerns**: Research tasks (find sources, extract key points) separate from writing tasks. This prevents quality issues in research from cascading through written content without human review between phases.

    

-   **Sequential Dependency within Phase**: Each task in a phase depends on the previous task completing first. Task 1.1 (find sources) â†’ Task 1.2 (synthesize points) â†’ Task 1.3 (create outline), enabling strict checkpoints at phase boundaries.

    

-   **Clear Acceptance Criteria**: "5+ sources identified; each peer-reviewed OR from domain expert; full citations recorded" (testable) vs. "Section 1 is researched" (untestable). Testable criteria are measurable and allow human validation.

    

-   **Checkpoint Sequence**: Four explicit human approval points (after Phase 1, Phase 2, Phase 3, and Phase 4 completion) where human reviews output, validates against plan, and explicitly authorizes progression to next phase.

    



### Common Mistakes



-   **Tasks Too Large (45+ minutes)**: Hiding complexity behind overly large tasks defeats the checkpoint pattern. Task "Research and write Section 1" (2+ hours) prevents human feedback until too much work has been done. Fix: Split into atomic units (find sources, synthesize points, outline, write content).

    

-   **Combining Research and Writing**: Including "research section, synthesize findings, write content, format citations" in a single task prevents early quality validation of sources. If sources are poor, human discovers this too late (after writing has already started). Fix: Separate research tasks from writing tasks with a checkpoint between phases.

    

-   **Vague Acceptance Criteria**: Criteria like "Section 1 is researched" leave hidden ambiguity about when task truly completes. Does "researched" mean 3 sources? 5? Peer-reviewed? Full citations? Fix: Make criteria specific and measurable: "5+ peer-reviewed sources, full citations, notes summarizing key points."

    

-   **Missing Dependencies**: Treating all tasks as independent or assuming parallel execution when tasks actually have strict sequence requirements. Correct understanding of dependencies prevents attempting Task 2.1 before Task 1.3 completes, which would invalidate the outline structure needed for targeted research.

    

-   **Skipping Checkpoints**: Letting agent autonomously complete multiple phases without human review ("Tell me everything") removes human control and prevents early issue detection. The checkpoint pattern explicitly requires human approval between phases.

    



### Progression Context



-   **Builds on**: Lesson 6 (Plan Phase) taught how to structure implementation approach through phases, timelines, and strategies. This lesson decomposes that plan into executable atomic work units with clear sequencing and human approval gates.

    

-   **Leads to**: Lesson 8 (Execution Phase) teaches how to actually execute the tasks with the `/sp.implement` command, monitoring progress through the checkpoint pattern, and handling task failures or refinements discovered during human review.

    

-   **Within SDD-RI Workflow**: `/sp.tasks` is the bridge between planning (intent, approach) and execution (atomic work units, checkpoints). Spec â†’ Plan â†’ Tasks â†’ Implement â†’ Validate forms the complete workflow.

    

-   **Requires**:

    

    -   Understanding of specification structure (Chapter 12, Lesson 1)

    -   Understanding of plan structure and phases (Lesson 6)

    -   Ability to trace lineage from specification through plan to tasks (validation skill)

    -   Familiarity with acceptance criteria as measurable completion signals (Lesson 6)

-   **Prepares for**:

    

    -   Executing tasks using `/sp.implement` with checkpoint validation (Lesson 8)

    -   Creating skills and subagents that orchestrate task sequences autonomously (Chapter 16+)

    -   Recognizing when tasks are too large or unclear and requesting refinement from `/sp.tasks`



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/SDD-RI-Fundamentals/spec-kit-plus-hands-on/tasks-phase

</tasks-phase>









<implement-phase>

-   [](/)

-   [Part 4: SDD-RI Fundamentals](/docs/SDD-RI-Fundamentals)

-   [Chapter 14: Master Spec-Kit Plus](/docs/SDD-RI-Fundamentals/spec-kit-plus-hands-on)

-   Implement Phase - Execute Tasks with AI Collaboration



# Implement Phase â€” Execute Tasks with AI Collaboration



You have a specification that defines what you're building, a plan that outlines the strategy, and tasks that break the work into atomic units. Now comes the execution phase: actually doing the work with your AI companion.



This lesson focuses on **control and validation**. Implementation isn't just "run tasks autonomously." It's you and AI working togetherâ€”you deciding direction, AI handling execution, both of you validating results against the specification.



* * *



## What Implementation Means in SDD-RI



**Implementation = executing your tasks.md to fulfill your specification.**



This is fundamentally different from ad-hoc coding. Consider the difference:



**Without specification (ad-hoc)**:



```

You: "Generate text for a research paper introduction"AI: [Produces 500 words]You: "Good enough?" (You have no objective standard)

```



**With specification (SDD-RI)**:



```

You: "Execute Task 1.1: Write research paper introduction (500-700 words,      academic tone, establish context for three key claims from spec)"AI: [Produces text]You: Check against spec:  âœ“ 650 words (within 500-700)  âœ“ Academic tone maintained  âœ“ All three key claims established  âœ“ Task complete

```



The difference: **Objective acceptance criteria from your spec**.



Implementation executes against these criteria. When a task is done, you know it's done because it meets the spec's success criteria.



* * *



## The /sp.implement Command



When you run `/sp.implement` in Claude Code, the command reads your tasks.md and orchestrates their execution with your AI companion.



**Basic usage**:



```

/sp.implement

```



The agent will:



1.  Read your tasks.md

2.  Begin executing tasks in dependency order

3.  Show outputs and intermediate results

4.  Wait for your review at checkpoint boundaries

5.  Continue on your approval



**You maintain control.** The agent doesn't proceed autonomously; it presents work and waits for your decision.



* * *



## Checkpoint Pattern: The Core Control Mechanism



Implementation uses checkpoints to maintain human decision-making at critical boundaries.



**The pattern:**



```

Task 1 â†’ Task 2 â†’ Task 3 â†’ CHECKPOINT 1                            (You review)                            (You approve or iterate)                                â†“                            Task 4 â†’ Task 5 â†’ CHECKPOINT 2                                            (You review)

```



**At each checkpoint**, you answer one question: **"Does the output meet the specification?"**



If yes: Commit and move forward. If no: Work with AI to refine until it meets spec.



### Example Checkpoint Review



**Your tasks.md specifies:**



```

Task 1.1: Write research paper introduction- Success criteria: 500-700 words, academic tone, introduces three main arguments- Dependencies: None

```



**Agent completes Task 1.1, shows output:**



```

[Introduction text, 623 words]

```



**Your checkpoint review (2 minutes)**:



-   Count words? 623 (within 500-700) âœ“

-   Academic tone? Yes, peer-reviewed style âœ“

-   Three arguments introduced? Argument 1, 2, 3 all present âœ“

-   Ready for next task? Yes âœ“



**Your decision**: "Task 1.1 meets spec. Committing. Proceed to Task 1.2."



This prevents cascading problems. If Task 1.1 had missed an argument, you'd catch it before Task 2 builds on faulty foundation.



* * *



## The Four Concepts You're Learning



### Concept 1: Implementation â‰  Autonomous Execution



Many students assume implementation means "Tell AI to build it and walk away."



**Reality**: Implementation is orchestrated execution with human judgment at every phase.



You direct. AI executes. You validate. You iterate if needed. This cycle repeats for each phase.



### Concept 2: Spec Success Criteria Are Your Acceptance Standard



Before running /sp.implement, your spec already defines what "done" means.



**In your spec:**



-   Introduction section: 500-700 words, academic tone, three arguments

-   Literature review: 800-1000 words, 10+ sources, critical analysis

-   Methodology: Clear procedure, reproducible, 300-400 words



**During implementation:**



-   Task completes

-   You check: "Does it meet success criteria from spec?"

-   Not yet? "AI, adjust X aspect and try again"

-   Meets spec? "Commit. Next task"



This objective standard prevents scope creep and "almost good enough" mediocrity.



### Concept 3: Iteration Loops Accelerate Problem-Solving



When a task doesn't meet spec, iteration (not abandonment) is the solution.



**Failure scenario**: You request introduction. AI produces 1200 words (exceeds 700-word spec).



**Without iteration (wrong)**:



```

You: "That's too long. Project failed."

```



**With iteration (correct)**:



```

You: "This exceeds the 700-word spec limit. Can you condense to 500-700      while keeping all three arguments?"AI: [Revises to 680 words]You: [Validates against spec]Done.

```



Iteration transforms failures into learning. The spec guides refinement.



### Concept 4: AI Collaboration During Execution



AI doesn't just executeâ€”it collaborates during execution.



**Example iteration dialogue:**



You: "Task 2.3: Write methodology section (300-400 words, reproducible procedure)"



AI proposes approach: "I'll structure this with (1) research population, (2) data collection method, (3) analysis approach. Does this align with what you're aiming for?"



You refine: "Add one more: (4) validity checks. Want to ensure readers can replicate this."



AI adjusts proposal: "Perfect. Here's revised structure with validity checks integrated..."



AI executes revised version: \[Produces 385-word methodology\]



You validate against spec: âœ“ 385 words (within 300-400), âœ“ Reproducible, âœ“ Validity checks present



**This is AI collaboration in implementation.** Not "execute blindly," but "suggest â†’ refine â†’ execute â†’ validate."



* * *



## Example: Implementing Your Research Paper



Let's walk through what happens when you execute `/sp.implement` for your paper project.



### Setup Before Running /sp.implement



You have:



-   **spec.md**: Paper specification with success evals (5-7 sections, 3000-4000 total words, academic sources, coherent argument)

-   **plan.md**: Implementation plan (write intro â†’ lit review â†’ methodology â†’ analysis â†’ conclusion, each section builds on previous)

-   **tasks.md**: Atomic tasks (Task 1.1, 1.2, 1.3... Task 5.4, 5.5)



### Running the Command



```

/sp.implement

```



Agent reads tasks.md, recognizes structure:



```

Phase 1: Introduction & Literature Foundationâ”œâ”€ Task 1.1: Research paper introduction (500-700 words)â”œâ”€ Task 1.2: Literature review outline (sources identified, 10+ required)â””â”€ CHECKPOINT 1: Validate intro + outline against specPhase 2: Methodology & Analysisâ”œâ”€ Task 2.1: Methodology section (300-400 words, reproducible)â”œâ”€ Task 2.2: Analysis section (800-1000 words, critical synthesis)â””â”€ CHECKPOINT 2: Validate sections meet quality criteriaPhase 3: Integration & Refinementâ”œâ”€ Task 3.1: Conclusion synthesizing all sectionsâ”œâ”€ Task 3.2: Final review and adjustmentâ””â”€ CHECKPOINT 3: Full document validation against spec

```



### CHECKPOINT 1 Review



**Agent reports:**



```

Task 1.1: Introduction (623 words)- Opening hook explaining research context- Three main arguments clearly stated- Transition to literature review preparedTask 1.2: Literature outline- 12 sources identified (exceeds 10-source requirement)- Categories: foundational (3), recent empirical (5), contrasting (4)- Annotations show critical analysis planned

```



**Your 3-minute review**:



-   âœ“ Introduction 623 words (within 500-700)

-   âœ“ Three arguments present and clear

-   âœ“ Outline has 12 sources (meets 10+ requirement)

-   âœ“ Sources span theoretical, empirical, and contrasting perspectives



**Your checkpoint decision**: "Both tasks meet spec criteria. Committing Phase 1. Ready for Phase 2."



### CHECKPOINT 2 Review



**Agent reports:**



```

Task 2.1: Methodology (385 words)- Clear procedure described- Population/sample specified- Data collection method documented- Analysis approach outlined- Reproducibility criteria metTask 2.2: Analysis (920 words)- Synthesizes all 12 sources- Identifies patterns across sources- Addresses contradictions- Connects to research questions

```



**Your 5-minute review**:



-   Methodology: 385 words (within 300-400) âœ“

-   All steps reproducible âœ“

-   Analysis: 920 words (within 800-1000) âœ“

-   Critical synthesis visible âœ“

-   Connects back to spec's three main arguments âœ“



**Your checkpoint decision**: "Both tasks meet spec. Committing Phase 2. Proceed to Phase 3."



### CHECKPOINT 3: Final Validation



**Agent reports:**



```

Complete document assembled:- Introduction: 623 words âœ“- Literature Review: 1050 words (from outline + full section)- Methodology: 385 words âœ“- Analysis: 920 words âœ“- Conclusion: 450 words âœ“Total: 3,428 words (within 3000-4000 spec requirement)Final checks:âœ“ All sections presentâœ“ Academic tone throughoutâœ“ 12 sources integratedâœ“ Argument coherence verifiedâœ“ Meets specification success criteria

```



**Your validation**:



-   Word count: 3,428 (within 3000-4000) âœ“

-   All required sections âœ“

-   Sources properly cited âœ“

-   Argument is coherent and supported âœ“



**Your final decision**: "Document meets specification. Implementation complete. Ready for next project phase."



* * *



## What Happens When Tasks Don't Meet Spec



**Real scenario**: Agent completes Task 1.1 (introduction) with 450 words. Your spec requires 500-700.



**At Checkpoint 1**:



```

Agent: "Task 1.1 complete: 450 words"You: "This is below the 500-700 word spec requirement.      Missing depth on which aspect?"Agent: "The second argument needs more context. I can expand       that section to add 75-100 words."You: "Yes, expand the second argument while keeping tone consistent."Agent: [Revises to 520 words]You: [Validates]Result: Task 1.1 now meets spec. Move forward.

```



**The key**: You used the spec to identify the gap, guided the refinement, and validated the fix. This is iterative implementationâ€”normal, expected, productive.



* * *



## Common Implementation Patterns



### Pattern 1: Serial Task Execution (Most Common)



Tasks execute in order. Each builds on previous.



```

Task 1 â†’ Task 2 â†’ Task 3 â†’ CHECKPOINT â†’ Task 4 â†’ Task 5 â†’ CHECKPOINT

```



**When to use**: Linear projects (writing, sequential analysis)



**Your role**: Validate after each task block, ensure quality before next block uses output



### Pattern 2: Parallel Task Execution (When Possible)



Some tasks don't depend on others. Execute simultaneously.



```

Task 1.1 â”€â”Task 1.2 â”€â”¼â”€â†’ CHECKPOINT 1Task 1.3 â”€â”˜           Task 2.1 â†’ Task 2.2 â†’ CHECKPOINT 2

```



**When to use**: Independent sections (research paper: intro, methodology, analysis can draft in parallel)



**Your role**: Ensure parallel tasks don't create integration conflicts at checkpoint



### Pattern 3: Iterative Refinement (When First Pass Insufficient)



Task executes, checkpoint review shows gap, task re-executes with refinement.



```

Task 1 â†’ Review â†’ Gap Identified â†’ Task 1 (refined) â†’ Review â†’ Approved

```



**When to use**: Complex tasks, creative work (good first draft exists, needs polish)



**Your role**: Provide specific feedback ("expand argument 2," "add example," "tighten analysis")



* * *



## Validation Against Specification



At each checkpoint, validation answers: **"Does this meet our specification?"**



### Validation Checklist Structure



**For each task completion:**



1.  **Success Criteria Met?**

    

    -   Does output match explicit criteria from task definition?

    -   Word count? Format? Tone? Completeness?

2.  **Specification Requirements Fulfilled?**

    

    -   Does this task's output advance toward spec's overall success evals?

    -   Are we building the right thing?

3.  **Quality Standards?**

    

    -   Does output meet your constitutional standards (from Lesson 3)?

    -   Academic rigor? Clarity? Coherence?

4.  **Ready for Next Task?**

    

    -   Can the next task safely build on this output?

    -   Any risks or dependencies that aren't met?



**Example checklist for Task 1.1 (Introduction)**:



```

Success Criteria:  â˜ 500-700 words  â˜ Academic tone  â˜ Introduces three main arguments  â˜ Smooth transition to literature reviewSpecification Requirements:  â˜ Advances toward "coherent research argument" eval  â˜ Establishes context for paper  â˜ Reader understands what paper will argueQuality Standards:  â˜ Engaging opening hook  â˜ Clear language (no jargon without definition)  â˜ Proper grammar and citation formatReady for Next Task:  â˜ Literature review can build on these arguments  â˜ No placeholder text or unresolved questions

```



If all checkboxes checked: Commit and continue. If any unchecked: Iterate.



* * *



## The AI Collaboration Dynamic



Implementation isn't you alone or AI alone. It's **collaboration with clear structure**.



### Your Responsibilities



-   **Direction**: "Here's what we're building (spec). Here's the next task."

-   **Judgment**: "Does this meet our spec? If not, what's missing?"

-   **Decision**: "Commit this task. Iterate on that task. Move forward."



### AI's Responsibilities



-   **Execution**: "I'll complete this task to your specification."

-   **Suggestion**: "I notice this approach might work better..." (AI as Teacher)

-   **Adaptation**: "You want to add X requirement. Adjusting..." (AI as Student)

-   **Refinement**: "Re-executing with your feedback..." (AI as Co-Worker)



### The Cycle



```

You specify task â†’ AI proposes approach â†’ You refine â†’ AI executes                    â†‘                                     â†“                    â†â†â† You validate â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†

```



This cycle repeats for each task. It's predictable, controlled, and produces measurable results.



* * *



## Checkpoint Decisions



At each checkpoint, you make one of three decisions:



### Decision 1: "Commit and Proceed"



**When**: All tasks meet spec requirements.



**Your statement**: "Phase X complete. All success criteria met. Committing. Proceed to Phase Y."



**What happens**: Agent records completion, moves to next phase.



### Decision 2: "Iterate on This Task"



**When**: A specific task doesn't meet spec.



**Your statement**: "Task 2.1 needs adjustment. It's 450 words but spec requires 500-700. Expand the analysis section by 75-100 words while keeping tone consistent."



**What happens**: Agent refines that task, re-executes, shows updated output.



**You review again**: Does it meet spec now? If yes â†’ commit. If no â†’ iterate again.



### Decision 3: "Revise the Plan"



**When**: A task completed reveals a planning problem.



**Your statement**: "Task 1.2 showed that the literature review is larger than planned (1050 vs 1000 words budgeted). We need to adjust the conclusion (reduce by 100 words) to keep total under 4000. AI, adjust plan and re-execute Task 1.3 accordingly."



**What happens**: Agent updates plan, adjusts downstream tasks, re-executes.



**When to use**: Infrequent. Reserve for when checkpoint review reveals structural issues, not just tweaks.



* * *



## Anti-Patterns: What Not to Do



### Anti-Pattern 1: Approving Without Review



```

âŒ WRONG:Agent: "Task complete"You: "Looks good" (without checking against spec)âœ“ RIGHT:Agent: "Task complete"You: [Spend 2-5 minutes validating against spec]

```



Skipping review defeats the purpose. Specs guide validation; validation ensures quality.



### Anti-Pattern 2: Accepting "Close Enough"



```

âŒ WRONG:Spec says: 500-700 wordsTask delivers: 780 wordsYou: "Close enough, let's move on"âœ“ RIGHT:Spec says: 500-700 wordsTask delivers: 780 wordsYou: "Over spec limit. AI, condense to 500-700 while keeping key points."

```



"Close enough" is how requirements creep. Specs exist to prevent this.



### Anti-Pattern 3: Not Iterating When First Pass Fails



```

âŒ WRONG:Agent: "Task 2.1 complete"You: "This doesn't meet spec"Agent: "Too bad. Moving on."You: "Okay, I guess project is compromised now"âœ“ RIGHT:Agent: "Task 2.1 complete"You: "This doesn't meet spec. Here's the gap: [specific feedback]"Agent: "Understood. Adjusting and re-executing..."Agent: [Shows revised output]You: [Validates again]Result: Task now meets spec

```



Iteration is normal. It's how you converge on specification compliance.



* * *



## Try With AI



Ready to understand `/sp.implement` deeply? Explore these prompts with your AI companion:



**Explore the Command Structure:**



> "I'm about to use `/sp.implement` to execute a research paper project. Walk me through what happens: (1) What does the agent read first (spec, plan, or tasks)? (2) How does it decide execution order? (3) How are checkpoints triggered? (4) What happens if I say 'iterate on Task 2' at a checkpoint? (5) When is implementation truly 'done'?"



**Practice Checkpoint Review:**



> "I just completed Task 1.2 of my research paper (literature review outline). The spec says 'identify 10+ academic sources, span theoretical and empirical work, show critical perspective.' The outline has 12 sources. How do I validate this meets spec in 3 minutes? What questions should I ask? What would make me say 'iterate' vs 'commit'?"



**Handle Iteration:**



> "My research paper introduction is 450 words, but spec requires 500-700. Rather than just asking to 'add 50-250 words,' how should I direct the AI? What specific aspect should expand? How do I ensure iteration improves the introduction rather than padding it with fluff?"



**Reflect on Specification Alignment:**



> "During implementation, I notice the research paper's conclusion feels disconnected from the introduction's three main arguments. This is a checkpoint review moment. Should I (a) iterate on the conclusion, (b) revise the plan to reconnect pieces, (c) go back and adjust the specification? What's the right decision and why?"



**Design Your First Checkpoint:**



> "For my next project, I'll write a technical spec document (not a research paper). Based on `/sp.implement` patterns, design what my first checkpoint should look like: What tasks go into Phase 1? What success criteria matter? What 3-5 questions should I answer during Phase 1 review?"



* * *



# Implement Phase â€” Execute Tasks with AI Collaboration



Implementation transforms your specification and plan into reality by executing tasks with measurable validation. This lesson teaches that implementation is orchestrated execution with human decision-making at checkpointsâ€”not autonomous code generation. The core skill is validating task completion against specification success criteria at critical phase boundaries, using the `/sp.implement` command to coordinate AI execution with your judgment.



### Mental Models



-   **Implementation â‰  Autonomous Execution**: Implementation is you directing strategy, AI handling execution, both validating results. You maintain control through checkpoints, not by walking away and hoping the AI completes the project autonomously.

    

-   **Specification as Acceptance Standard**: Your spec defines "done" before implementation begins. Success criteria (word counts, requirements, quality measures) become objective measures of task completion. A task meets spec when its output satisfies all predefined criteria; iteration refines tasks that don't meet criteria.

    

-   **Checkpoint Pattern (Control Mechanism)**: Work executes in phases separated by checkpoints. At each checkpoint, you answer one questionâ€”"Does this meet the specification?"â€”then decide: commit and proceed, iterate the current task, or revise the plan if checkpoint reveals structural issues. Checkpoints prevent cascading failures from undetected task deficiencies.

    

-   **Iteration as Solution Pattern**: When tasks don't meet spec, iteration (not abandonment) solves the problem. AI refines output based on your feedback guided by the spec. Iteration transforms task failures into productive learningâ€”spec shows what's missing, you guide adjustment, AI re-executes, you re-validate.

    



### Key Patterns



-   **Specâ†’Promptâ†’Executeâ†’Validate Cycle**: Tasks execute within a structured cycle where you specify requirements from tasks.md, AI proposes execution approach, you refine direction, AI executes, you validate against spec success criteria, cycle repeats or proceeds based on checkpoint decision.

    

-   **Checkpoint Decisions (Three Options)**: At each checkpoint you choose: (1) "Commit and Proceed"â€”all tasks meet spec, move to next phase; (2) "Iterate on This Task"â€”specific task needs refinement, provide targeted feedback based on spec gaps; (3) "Revise the Plan"â€”checkpoint revealed structural issue requiring plan adjustment (rare, reserved for systematic problems).

    

-   **Serial Task Execution**: Tasks execute in dependency order (Task 1 â†’ Task 2 â†’ Task 3 â†’ Checkpoint). Each task builds on previous output. You validate after each task block to ensure quality before next block uses output as foundation.

    

-   **Parallel Task Execution**: Independent tasks execute simultaneously (Task 1.1, 1.2, 1.3 in parallel â†’ Checkpoint). Reduces time when tasks have no dependencies. Requires checkpoint validation to ensure parallel outputs don't create integration conflicts.

    

-   **Iterative Refinement Pattern**: Task executes, checkpoint review identifies gap, task re-executes with refinement, review validates against spec, process repeats until specification met. Used for complex work where polished first draft exists but needs targeted improvement.

    

-   **AI Collaboration During Execution**: AI doesn't just execute blindlyâ€”it collaborates with suggestionâ†’refinementâ†’executionâ†’validation cycle. AI proposes approach (suggesting pattern you might not discover), you refine based on project constraints, AI adapts and executes revised version, you validate outcome.

    



### Common Mistakes



-   **Approving Without Review**: Checkpoint appears and you say "Looks good" without validating against specification success criteria. Skipping review defeats checkpoint purposeâ€”specs guide validation; validation ensures spec-aligned quality. Always spend 2-5 minutes validating each checkpoint.

    

-   **Accepting "Close Enough"**: Spec requires 500-700 words, task delivers 780 words. You move on without iteration because it's "close enough." This is how requirements creep occurs. Specs exist to prevent creepâ€”enforce them at checkpoints through iteration when tasks exceed or fall short of criteria.

    

-   **Not Iterating When First Pass Fails**: Task doesn't meet spec, and you either give up or accept the failed output. Iteration is normal and productive. When a task misses spec, identify the specific gap using specification criteria, provide targeted feedback to AI, re-execute with refinement, validate again. This convergence is how implementation works.

    

-   **Attempting Autonomous Execution**: You run `/sp.implement` and let the agent execute all tasks without checkpoint review. This eliminates human judgment and risks cascading failures. Instead, wait at each checkpoint, validate against spec, make informed decision, then proceed.

    

-   **Missing Dependencies in Validation**: Checkpoint review checks individual task quality but doesn't verify that downstream tasks can safely build on this output. Always ask: "Can the next task safely use this output? Are there integration risks?" This prevents downstream failures.

    

-   **Conflating Iteration with Plan Revision**: When a task doesn't meet spec, iteration refines that task. When checkpoint reveals a systematic planning problem (word budget overflow, structural mismatch), plan revision adjusts downstream tasks. Confusing these patterns leads to excessive plan changes instead of targeted task refinement.

    



### Progression Context



-   **Builds on**: Lesson 7 (Plan Phase) where you created atomic tasks with dependencies and success criteria. That plan becomes the roadmap `/sp.implement` executes. You needed clear task definitions and dependencies before implementation can proceed efficiently.

    

-   **Leads to**: Validation and completion workflows where you systematically verify that final implementation meets specification success evals (Lesson 9, Validation Phase). Implementation executes tasks; validation verifies that accumulated task outputs fulfill the original specification intent.

    



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/SDD-RI-Fundamentals/spec-kit-plus-hands-on/implement-phase

</implement-phase>









<designing-reusable-intelligence>

-   [](/)

-   [Part 4: SDD-RI Fundamentals](/docs/SDD-RI-Fundamentals)

-   [Chapter 14: Master Spec-Kit Plus](/docs/SDD-RI-Fundamentals/spec-kit-plus-hands-on)

-   Designing Reusable Intelligence



# Designing Reusable Intelligence



You've completed the SDD workflow (Lessons 01-08): Constitution â†’ Specify â†’ Clarify â†’ Plan â†’ Tasks â†’ Implement. You wrote specifications, refined requirements, planned architecture, and executed implementation with AI collaboration.



But here's what separates AI-native developers from AI-assisted developers: **The ability to transform good sessions into reusable skills.**



With Skills, you teach your AI specific workflows, tools, and processes. By creating a skill, you're giving your AI a playbook it can reference whenever you need that particular type of helpâ€”whether that's writing research sections, validating sources, or refining outlines.



* * *



## When to Create a Skill



Not every workflow deserves a skill. Create one when you notice:



**After a good session, ask yourself:**



1.  **Will I do this again?** (Frequency: 3+ times across projects)

2.  **Did it involve multiple decisions?** (Complexity: 5+ decision points)

3.  **Would I want the same quality next time?** (Value: consistent results matter)



If 2+ answers are YES â†’ Create a skill.



**Examples from your research paper project:**



Pattern



Frequency



Complexity



Value



Create Skill?



Writing quality sections



âœ… Every paper



âœ… 6+ decisions



âœ… Consistency



**YES**



Validating research sources



âœ… Every source



âœ… 5+ decisions



âœ… Accuracy



**YES**



Refining outlines



âœ… Every draft



âœ… 5+ decisions



âœ… Structure



**YES**



Formatting citations



âœ… Every source



âŒ 2-3 decisions



âŒ Tool handles



NO



Spell checking



âœ… Every draft



âŒ 1 decision



âŒ Automated



NO



* * *



## Part A: Creating a Skill Through Conversation



Creating a skill with AI means having a conversation where you explain your approach and share any materials you want included. AI translates this into a properly formatted skill.



### Step 1: Start the Conversation



After a good session (like writing a great section), start a new conversation:



```

I want to create a skill for writing research paper sections.During our last session, I noticed my best sections had:- Clear opening that establishes what the section explains- Evidence integrated naturally, not tacked on- Logical flow from one paragraph to the next- Conclusions that connect back to thesisCan you help me turn this into a reusable skill?

```



**What happens:**



-   AI asks clarifying questions about your process

-   You explain what makes your approach work

-   AI structures this into a skill file



### Step 2: Answer AI's Questions



AI will ask about your process. Provide enough detail that someone capable but unfamiliar could follow your approach.



**Example conversation:**



```

AI: "What makes a section 'good' in your view? How do you know when     a section is ready vs needs more work?"You: "A good section has three things:     1. Every claim is supported by evidence or citation     2. The opening hooks interest and the ending connects to thesis     3. Someone unfamiliar with the topic could follow the logic     It needs more work if I have to re-read sentences to understand them,     or if claims feel unsupported."AI: "What's your process for writing a section? Do you outline first,     write linearly, or iterate?"You: "I start with a rough outline of key points. Then I write the opening     paragraph to establish context. Then I fill in evidence for each point.     Finally I write transitions and the closing. I always read it aloud     to check flow."AI: "What quality checks do you apply before considering a section done?"You: "I check:     - Does every paragraph have a clear topic sentence?     - Is every factual claim cited?     - Does it read smoothly without stopping?     - Does it connect to the thesis?"

```



### Step 3: AI Builds the Skill File



Based on your conversation, AI creates a complete SKILL.md file. Ask:



```

Create the skill file with:- YAML frontmatter (name, description, version)- When to use this skill- Process steps from our conversation- Output format- Example with input and outputFormat it as a SKILL.md file I can save.

```



**AI generates:**



```

---name: "section-writer"description: "Write clear, evidence-supported research paper sections with proper citations and logical flow. Use when user asks to write a section, needs help with academic writing, or wants to structure research content."version: "1.0.0"---# Section Writing Skill## When to Use This Skill- User asks to "write a section" or "draft content" for a research paper- User mentions thesis, evidence integration, or academic writing- User needs help structuring research content with citations## How This Skill Works1. **Outline key points**: Identify 3-5 main points the section must make2. **Write opening paragraph**: Establish context, hook interest, connect to thesis3. **Develop each point**: Topic sentence â†’ evidence/citation â†’ significance â†’ transition4. **Write closing**: Summarize insights, connect to thesis, set up next section5. **Quality check**: Verify claims cited, flow smooth, thesis connected## Output FormatProvide:- **Opening Paragraph**: Hook + context + thesis connection- **Body Segments**: For each key point: topic sentence, evidence, significance- **Transitions**: Smooth connections between paragraphs- **Closing Paragraph**: Summary + thesis connection + setup for next section## Quality CriteriaA section is ready when:- Someone unfamiliar with topic can follow the logic- All claims supported by evidence- No sentences require re-reading to understand- Opening hooks interest, closing connects to thesis## Example**Input**: "Write the Literature Review section on AI in K-12 education. Key points: (1) Current state of AI in K-12, (2) Teacher workload research, (3) Student outcome studies. Thesis: AI tools can measurably improve K-12 classroom efficiency."**Output**:- **Opening**: "The integration of artificial intelligence into K-12 classrooms represents one of the most significant shifts in educational technology since the introduction of personal computers. As schools face mounting pressure to improve outcomes while managing limited resources, AI tools offer promising solutionsâ€”but what does the research actually show?"- **Body**: [3 segments with topic sentences, evidence from studies, significance explained]- **Closing**: "The evidence suggests AI tools can meaningfully reduce teacher workload while improving student engagement. However, implementation challenges and equity concerns require careful attention as districts consider adoption."

```



### Step 4: Save the Skill File



Skills use a specific directory structure. Create the skill directory and save:



```

# Create skill directory (skill-name/SKILL.md structure)mkdir -p .claude/skills/section-writer

```



Then tell AI:



```

Save this skill to .claude/skills/section-writer/SKILL.md

```



Your directory structure now includes:



```

my-research-paper/â”œâ”€â”€ .claude/â”‚   â”œâ”€â”€ commands/              # Slash commands from specifyplus initâ”‚   â””â”€â”€ skills/                # YOUR reusable intelligenceâ”‚       â””â”€â”€ section-writer/    # Skill directoryâ”‚           â””â”€â”€ SKILL.md       # Main skill fileâ”œâ”€â”€ .specify/â”‚   â””â”€â”€ memory/â”‚       â””â”€â”€ constitution.mdâ”œâ”€â”€ specs/â”‚   â””â”€â”€ [your-paper]/â”‚       â”œâ”€â”€ spec.mdâ”‚       â”œâ”€â”€ plan.mdâ”‚       â””â”€â”€ tasks.mdâ””â”€â”€ ...

```



**Note**: Skills can also include supporting files (scripts, reference docs) in the skill directory. For now, SKILL.md is all you need.



### Step 5: Test the Skill



**Test on a real task:**



```

I need to write the Discussion section for my paper.Use the section-writer skill to guide me.Key points:1. How AI applications address teacher workload2. Evidence of student outcome improvements3. Limitations and areas needing more researchThesis: AI tools can measurably improve K-12 classroom efficiency.

```



**Evaluate the result:**



-   Did AI follow the skill's process (outline â†’ opening â†’ evidence â†’ closing)?

-   Did output meet the quality criteria (claims cited, flows smoothly)?

-   What's missing or needs adjustment?



### Step 6: Iterate Until It Works



If something's off, ask AI to update the skill:



```

The section-writer skill worked well, but I noticed:- It didn't emphasize checking source credibility- The quality checklist could be more specificUpdate the skill to:1. Add source credibility check in Step 32. Add "minimum 3 sources per major point" to quality criteriaThen save the updated version to .claude/skills/section-writer.md

```



Repeat testing until your skill produces consistent, high-quality results.



* * *



## Part B: Skill vs Subagent â€” When to Create Which



As you identify more patterns, you'll wonder: **Should I create a skill or a subagent?**



### Decision Framework



**Create a SKILL (2-6 decision points)** when:



-   Human guides the process, AI assists

-   You apply the framework, AI helps execute

-   Examples: section-writer, outline-refiner, citation-formatter



**Create a SUBAGENT (7+ decision points)** when:



-   AI should work autonomously with minimal guidance

-   AI makes judgments and returns verdicts

-   Pattern requires complex, multi-step reasoning

-   Examples: research-validator (evaluates source credibility), fact-checker (verifies claims)



**From your research paper project:**



Component



Decision Points



Human Involvement



Create



Section writing



6



Human guides, AI assists



**SKILL**



Outline refinement



5



Collaborative iteration



**SKILL**



Research validation



8+



AI judges credibility



**SUBAGENT**



Fact checking



7+



AI verifies autonomously



**SUBAGENT**



### What Makes Subagents Different



A subagent adds three things beyond a skill:



**1\. Role Definition** (autonomous identity):



```

**Name**: source-validator**Autonomy Level**: High (makes accept/reject decisions)**Invocation**: Automatic (after adding source) or manual

```



**2\. Decision Authority** (what it can decide vs escalate):



```

**Can ACCEPT**: Peer-reviewed sources from past 10 years**Can REJECT**: Non-peer-reviewed sources, outdated sources**Must ESCALATE**: Borderline cases, conflicting information

```



**3\. Reporting Format** (structured output):



```

=== SOURCE VALIDATION ===Source: [title]Verdict: [ACCEPT | REJECT | ESCALATE]Reasoning: [specific criteria applied]Required Actions: [if any]

```



### Creating a Subagent (Preview)



Subagents follow a similar conversation-based creation process:



```

I want to create a subagent for validating research sources.The subagent should autonomously:1. Check if sources are peer-reviewed2. Evaluate publication date (prefer recent)3. Assess author credibility4. Flag potential bias5. Verify source actually supports the claim being madeIt should return a verdict (accept/reject/escalate) with reasoning.Help me design this subagent with:- Role definition- Decision authority (what it can decide vs escalate)- Reporting format

```



You'll learn more about subagents in later chapters. For now, focus on skills.



* * *



## Part C: Validating Your Skill Works



Good skills trigger **reasoning mode** (context-specific analysis). Bad skills trigger **prediction mode** (generic responses).



### Test Your Skill's Effectiveness



Ask your AI to apply your skill to a challenging case:



```

Apply the section-writer skill to this vague request:"Write about AI in education"Walk me through how you're applying each step of the skill.Show your reasoning for each quality check.

```



**Good response (reasoning mode):**



```

Applying section-writer skill:STEP 1 - OUTLINE KEY POINTS:The request is too vague. Before outlining, I need:- What specific aspect of AI in education?- What thesis should this connect to?- What audience level?[Skill triggers clarification before proceeding]STEP 2-4 - [Would proceed after clarification]QUALITY CHECK:- Cannot verify "connects to thesis" without knowing thesis- Cannot ensure "someone unfamiliar can follow" without knowing audienceSKILL VERDICT: Request needs refinement before skill can be applied effectively.

```



**Bad response (prediction mode):**



```

Here's a section about AI in education:AI is transforming education in many ways. It helps teachersand students learn better. Many schools are using AI tools...[Generic content that ignores skill process]

```



If you get prediction mode responses, your skill needs strengthening. Revise with more specific criteria and quality checks.



* * *



## Part D: Building Your Intelligence Library



### Organizing Your Skills



Standard directory structure (each skill gets its own folder):



```

my-research-paper/â”œâ”€â”€ .claude/â”‚   â”œâ”€â”€ commands/                    # Slash commands (from specifyplus)â”‚   â””â”€â”€ skills/                      # YOUR accumulated intelligenceâ”‚       â”œâ”€â”€ section-writer/          # Skill directoryâ”‚       â”‚   â””â”€â”€ SKILL.md             # Main skill fileâ”‚       â”œâ”€â”€ outline-refiner/         # Future skillâ”‚       â”‚   â””â”€â”€ SKILL.mdâ”‚       â””â”€â”€ source-evaluator/        # Future skillâ”‚           â”œâ”€â”€ SKILL.mdâ”‚           â””â”€â”€ scripts/             # Optional supporting filesâ”‚               â””â”€â”€ verify_source.pyâ”œâ”€â”€ .specify/â”‚   â””â”€â”€ memory/â”‚       â””â”€â”€ constitution.mdâ”œâ”€â”€ specs/â””â”€â”€ ...

```



### Intelligence Reuse Strategy



**Skill reuse** (apply to new contexts):



```

# Project 2: Different research paperI'm writing a section on climate policy impacts.Use the section-writer skill.Context: This is for a policy paper, not education research.Key points: (1) Current policy landscape, (2) Economic impacts, (3) Implementation challengesThesis: Carbon pricing is the most efficient policy mechanism.

```



**Intelligence composition** (combine multiple skills):



```

# Project 3: Comprehensive paperApply these skills in sequence:1. outline-refiner â†’ improve paper structure2. section-writer â†’ write each section3. source-evaluator â†’ validate all citationsStart with outline-refiner on my current outline.

```



* * *



## Common Mistakes



### Mistake 1: Creating Skills for Trivial Patterns



**The Error**: Creating a skill for "How to format headings"



**Why It's Wrong**: 1-2 decision points don't justify a skill. Save skills for complex, recurring workflows.



**The Fix**: Only create skills for patterns with 5+ decisions that recur across 3+ projects.



### Mistake 2: Skipping the Testing Phase



**The Error**: Saving a skill and assuming it works



**Why It's Wrong**: Skills need iteration. Your first version probably misses edge cases.



**The Fix**: Always test skills on real tasks. Update based on what's missing.



### Mistake 3: Over-Specific Skills



**The Error**: Creating "AI-Education-Literature-Review-Writer" that only works for one topic



**Why It's Wrong**: Intelligence should be reusable. Over-specificity limits value.



**The Fix**: Generalize patterns:



-   âŒ "AI-Education-Literature-Review-Writer"

-   âœ… "Section-Writer" (works for any research paper section)



### Mistake 4: No Quality Criteria



**The Error**: Skill describes process but not what "good" looks like



**Why It's Wrong**: Without quality criteria, you can't verify output or improve skill.



**The Fix**: Every skill needs explicit quality criteria:



-   What makes output "ready"?

-   What makes output "needs work"?

-   How do you check?



* * *



## Skill Reuse in Practice



### Project 1: Research Paper (Lessons 04-08)



You execute the complete workflow from scratch:



-   Write specification, plan, tasks

-   Write sections through trial and error

-   Learn what works through iteration

-   **Total**: 8-10 hours



### Project 2: New Paper (With `section-writer` Skill)



With your skill, dramatically faster:



1.  Write paper specification (30 min)

2.  Plan sections (30 min)

3.  Write sections using skill guidance (3 hoursâ€”skill provides structure)

4.  **Total**: 4 hours (50% faster)



### Project 3: Multi-Paper Work (With Multiple Skills)



With accumulated skills:



1.  Use `section-writer` skill to write (2 hours)

2.  Use `source-evaluator` skill to check citations (1 hour)

3.  Use `outline-refiner` skill to improve structure (30 min)

4.  **Total**: 3.5 hours



**Intelligence compounds**: Each skill accelerates future work.



* * *



## Try With AI



Ready to create your first reusable skill? Practice conversation-based skill creation:



**Start Skill Creation:**



> "I want to create a skill for writing research paper sections. During my work on this paper, I noticed my best sections had clear openings, evidence integrated naturally, logical flow, and conclusions connecting to thesis. Help me turn this into a reusable skill. Ask me questions about my process."



**Generate Complete Skill File:**



> "Based on our conversation, create a complete SKILL.md file with: YAML frontmatter (name, description, version), when to use, process steps, output format, and example. Format it so I can save to .claude/skills/section-writer/SKILL.md"



**Test Your Skill:**



> "Apply the section-writer skill to write my Discussion section. Key points: (1) How AI applications address teacher workload, (2) Evidence of student outcome improvements, (3) Limitations. Walk me through each step of the skill as you apply it."



**Iterate Based on Results:**



> "The skill worked, but I noticed \[specific issue\]. Update the skill to address this. Save the updated version."



**Decide Skill vs Subagent:**



> "I'm thinking about creating reusable intelligence for validating research sources. Help me decide: (1) How many decision points does this involve? (2) Should human guide or AI work autonomously? (3) Based on that, should this be a skill or subagent? (4) Start the creation process."



# Designing Reusable Intelligence



This lesson teaches students to create reusable skills through conversation with AI after good sessions. Students identify patterns worth encoding (frequency + complexity + value), have a conversation where they explain their approach, and AI structures it into a complete skill file with metadata, process steps, quality criteria, usage example, and self-check validation. Students save skills to `.claude/skills/`, test them on real tasks, and iterate until they work. The lesson also distinguishes skills (2-6 decision points, human-guided) from subagents (7+ decisions, AI autonomous).



### Mental Models



-   **Skills from Good Sessions**: After a session that went well, ask: Will I do this again? Did it involve multiple decisions? Would I want the same quality next time? If 2+ YES â†’ create a skill.

    

-   **Conversation-Based Creation**: You describe your process naturally, AI asks clarifying questions (what makes output good? what's your process? what quality checks?), AI structures it into a complete skill file.

    

-   **Test and Iterate Loop**: Skills need iteration. Save skill â†’ test on real task â†’ evaluate results â†’ update skill â†’ repeat until consistent results.

    

-   **Reasoning vs Prediction Mode**: Good skills trigger reasoning mode (AI asks for clarification, applies each step explicitly). Bad skills trigger prediction mode (AI produces generic content ignoring the skill process).

    

-   **Intelligence Compounds**: Project 1: 8-10 hours (from scratch). Project 2 with skill: 4 hours (50% faster). Project 3 with multiple skills: 3.5 hours. Each skill accelerates future work.

    



### Key Patterns



-   **Start skill conversation**: After good session, say "I want to create a skill for \[workflow\]. During my work, I noticed my best \[outputs\] had \[specific qualities\]. Help me turn this into a reusable skill."

    

-   **Complete skill file structure**: YAML frontmatter (`name`, `description`, `version`) â†’ When to use â†’ Process steps â†’ Output format â†’ Quality criteria â†’ Example.

    

-   **Save to `.claude/skills/<skill-name>/SKILL.md`**: Create skill directory with `mkdir -p .claude/skills/section-writer`, then save SKILL.md inside. Skills use directory structure (skill-name/SKILL.md), not flat files.

    

-   **Test skill on real task**: Apply skill to actual work, evaluate: Did AI follow the process? Did output meet criteria? What's missing?

    

-   **Iterate with specific feedback**: "The skill worked but \[specific issue\]. Update skill to \[fix\]. Save updated version."

    

-   **Validate skill triggers reasoning**: Apply skill to vague request. Good skill asks for clarification and shows step-by-step reasoning. Bad skill produces generic content.

    



### Common Mistakes



-   **Creating skills for trivial patterns**: "How to format headings" (1-2 decisions) doesn't justify a skill. Save skills for 5+ decision workflows.

    

-   **Skipping testing**: Saving skill and assuming it works. Skills need iterationâ€”first version misses edge cases.

    

-   **Over-specific skills**: "AI-Education-Literature-Review-Writer" only works for one topic. Generalize to "Section-Writer" (any paper).

    

-   **No quality criteria**: Skill describes process but not what "good" looks like. Every skill needs explicit ready/needs-work criteria.

    



### Progression Context



-   **Builds on**: Lessons 04-08 (complete SDD workflow). Students have executed one full project cycle and understand where patterns recur.

    

-   **Leads to**: Lesson 11 (Capstone) where students invoke created skills to accelerate new work, demonstrating intelligence compounding across projects.

    



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/SDD-RI-Fundamentals/spec-kit-plus-hands-on/designing-reusable-intelligence

</designing-reusable-intelligence>









<brownfield-adoption>

-   [](/)

-   [Part 4: SDD-RI Fundamentals](/docs/SDD-RI-Fundamentals)

-   [Chapter 14: Master Spec-Kit Plus](/docs/SDD-RI-Fundamentals/spec-kit-plus-hands-on)

-   Brownfield Adoption



# Brownfield Adoption



Most of what you've learned in Chapter 14 applies to **greenfield projects**â€”new codebases where you start from scratch and control the entire structure. But real work is different. You'll inherit existing projects with months or years of accumulated code, decisions, and team knowledge.



**Brownfield adoption** is the process of bringing Spec-Kit Plus into an existing project. The challenge isn't technicalâ€”it's strategic. How do you add a structured workflow framework without losing the institutional knowledge your team has already captured? How do you protect against data loss when the framework overwrites key files like `CLAUDE.md`?



This lesson teaches you a safe, proven workflow for brownfield adoption that preserves existing knowledge, prevents accidental data loss, and integrates Spec-Kit Plus incrementally.



* * *



## Foundation â€” Greenfield vs Brownfield



### What's the Difference?



**Greenfield Project**: You start with an empty directory. You have no existing:



-   Code to protect

-   Team conventions to preserve

-   Architectural decisions made

-   Custom tools or workflows



Running `specifyplus init` in a greenfield project is straightforwardâ€”you're building from scratch.



**Brownfield Project**: You inherit an existing codebase with:



-   Working code in `src/`, `lib/`, or equivalent directories

-   Custom `CLAUDE.md` with team knowledge (coding standards, architecture principles, collaboration patterns)

-   Custom slash commands in `.claude/commands/` (project-specific workflows)

-   Git history containing architectural decisions and design rationale

-   Team members relying on existing conventions



Running `specifyplus init --here` (the brownfield command) in this context requires careful strategy to avoid data loss.



### The Core Problem



When you run `specifyplus init --here`, the command initializes Spec-Kit Plus in your **existing directory**. Here's what happens:



**Files that get OVERWRITTEN** (complete replacement):



-   `CLAUDE.md` â€” Your custom AI instructions get replaced with the Spec-Kit Plus template (~240 lines)

-   Any existing `.specify/` directory (if present) gets reset



**Files that are PRESERVED** (completely safe):



-   All custom slash commands in `.claude/commands/` are preserved

-   All source code (`src/`, `lib/`, etc.) remains untouched

-   All tests, configuration files, and project artifacts survive intact

-   Your git history is unaffected



**The Risk**: Your `CLAUDE.md` contains months or years of team knowledge. Without a backup before running init, that content is permanently lostâ€”no recovery mechanism exists.



### Example: Three Real Project Types



Let's see how brownfield adoption affects different project types:



**Scenario 1: Existing Blog/Website Project**



```

blog-project/â”œâ”€â”€ CLAUDE.md          (200 lines of team conventions)â”œâ”€â”€ src/â”‚   â”œâ”€â”€ pages/â”‚   â”œâ”€â”€ components/â”‚   â””â”€â”€ styles/â”œâ”€â”€ README.mdâ””â”€â”€ .claude/commands/    â””â”€â”€ deploy-netlify.md   (custom deployment workflow)

```



After `init --here`:



-   CLAUDE.md (200 â†’ 240 lines) â€” Your content replaced with template

-   `.specify/` directory created

-   `deploy-netlify.md` preserved and working

-   Source code completely untouched



**Scenario 2: Existing API Project**



```

api-project/â”œâ”€â”€ CLAUDE.md          (150 lines of coding standards)â”œâ”€â”€ src/â”‚   â”œâ”€â”€ routes/â”‚   â”œâ”€â”€ models/â”‚   â””â”€â”€ middleware/â”œâ”€â”€ tests/â””â”€â”€ .claude/commands/    â”œâ”€â”€ run-tests.md    â””â”€â”€ format-code.md

```



After `init --here`:



-   CLAUDE.md replaced (your standards lost without backup)

-   `.specify/` created with constitution template

-   Both custom commands preserved

-   Code and tests completely safe



**Scenario 3: Documentation Project**



```

docs-project/â”œâ”€â”€ CLAUDE.md          (100 lines of documentation standards)â”œâ”€â”€ docs/â”‚   â”œâ”€â”€ guides/â”‚   â”œâ”€â”€ api/â”‚   â””â”€â”€ tutorials/â””â”€â”€ .claude/commands/    â””â”€â”€ publish-docs.md

```



After `init --here`:



-   CLAUDE.md replaced

-   `.specify/` created

-   Custom publish command preserved

-   Documentation source intact



* * *



## Collaboration â€” Safe Brownfield Testing Workflow



The key to safe brownfield adoption is **redundant safety measures**: git branches + manual backups + clear inspection of what actually changed.



### Designing the Workflow



**Your situation**: You have an existing project with a custom `CLAUDE.md` that your team values. You want to test `specifyplus init --here` but can't afford to lose the content.



**AI's response**:



> "The solution is creating multiple recovery points before testing. Here's the proven workflow:

> 

> ```

> # STEP 1: Create experimental branch (isolates risk from main)git checkout -b experiment/specifykit# STEP 2: Create backup copies (manual recovery option)cp CLAUDE.md CLAUDE.md.backupcp -r .claude .claude.backup# STEP 3: Commit current state (git recovery option)git add -Agit commit -m 'backup: preserve team knowledge before Spec-Kit Plus init'# STEP 4: NOW test safely (you have multiple recovery paths)specifyplus init --here

> ```

> 

> **Why this approach works**:

> 

> -   **Experiment branch**: Your `main` branch stays completely untouched

> -   **Backup files**: If you need your original content, `.backup` files have it

> -   **Git commit**: Historical recovery via `git show HEAD~1:CLAUDE.md`

> -   **Multiple recovery paths**: If one method fails, you have alternatives

> 

> Want me to walk through exactly what changes, and how to merge your content afterward?"



### Understanding What Actually Changed



After running init, you inspect the results:



```

# Check what changedgit diff --name-only# Output:# CLAUDE.md# .specify/memory/constitution.md# .specify/memory/context-quality-memory.md# .specify/templates/spec-template.md# .specify/templates/plan-template.md# .specify/templates/tasks-template.md# [... more .specify/ files ...]

```



Your `CLAUDE.md.backup` still has the original content. Now you can make an informed decision about merging:



**Option 1: Content goes to Constitution** (Recommended for standards and architecture decisions)



Your team's coding standards, architecture principles, and development conventions belong in `.specify/memory/constitution.md`â€”the project-specific principles file that AI agents read alongside `CLAUDE.md`.



**Option 2: Content appends to CLAUDE.md** (For behavioral collaboration patterns)



Your team's AI collaboration patterns (like "test-first development" or "review AI output critically") belong appended to the end of `CLAUDE.md`, since they're instructions for HOW AI should work with your team.



### Merging Strategy



**For a real project**, here's how you'd merge (using example content):



1.  **Read your backup**: `cat CLAUDE.md.backup`

2.  **Identify content categories**:

    -   Coding standards â†’ Move to `constitution.md`

    -   Architecture principles â†’ Move to `constitution.md`

    -   AI collaboration patterns â†’ Append to new `CLAUDE.md`

3.  **Execute the merge**:

    

    ```

    # Add standards to constitutionecho "## Project Development Standards[paste your coding standards here]" >> .specify/memory/constitution.md# Append patterns to CLAUDE.mdecho "## Team AI Collaboration Patterns[paste your collaboration patterns here]" >> CLAUDE.md

    ```

    

4.  **Verify nothing was lost**:

    

    ```

    # Compare old vs new (backup has all your content)diff CLAUDE.md.backup CLAUDE.md.backup.recovered

    ```

    



* * *



## Practice â€” Identifying Your Project's Content



Before you adopt Spec-Kit Plus on a real project, understand what you'd need to preserve.



### Self-Check: Content Categories



For each category, decide where your team's content would go:



**Category 1: Development Standards**



-   Type hints requirements (Python)

-   Line length limits (80, 100, 120 chars)

-   Import ordering conventions

-   Naming conventions



**Decision**: These are project rules â†’ Move to `constitution.md`



**Category 2: Architecture Principles**



-   Technology choices (Flask vs FastAPI, PostgreSQL vs MongoDB)

-   Design patterns (microservices, monolith, serverless)

-   Deployment strategy (Docker, Kubernetes, serverless)



**Decision**: These are project constraints â†’ Move to `constitution.md`



**Category 3: AI Collaboration Patterns**



-   "Specification first, then code"

-   "Review AI output before merging"

-   "Test coverage minimum 80%"

-   Preferred AI tools and models



**Decision**: These are behavioral instructions â†’ Append to `CLAUDE.md`



### Planning Your Adoption



Write down your actual content before running init:



**What's in your current CLAUDE.md?** (Estimate line counts and categories)



-   Development standards: \_\_\_ lines

-   Architecture principles: \_\_\_ lines

-   AI collaboration patterns: \_\_\_ lines

-   Custom workflow notes: \_\_\_ lines



**What custom commands do you rely on?** (These are safe)



-   `/deploy` or `/deploy-staging`?

-   `/test` or `/test-coverage`?

-   `/format` or `/lint`?

-   Others: \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_



**What would break if CLAUDE.md disappeared?**



-   Team coding standard consensus lost?

-   Architecture decisions undocumented?

-   AI collaboration practices forgotten?



* * *



## Try With AI



You're evaluating whether to adopt Spec-Kit Plus on a real project. Let's plan the actual adoption strategy.



**Setup**: Open your project directory with Spec-Kit Plus ready.



**Prompt Set**:



```

Prompt 1 (Understanding your current state):"My project already has a custom CLAUDE.md with our team's coding standards andarchitecture principles. I want to use Spec-Kit Plus but can't lose this content.What's my safe adoption strategy? Should I back up first? What will actually get overwritten?"Prompt 2 (Planning the workflow):"Here's what's in our CLAUDE.md:[paste your actual content or describe it]Walk me through the exact steps to:1. Create a safe testing environment2. Run specifyplus init --here3. Merge my team's knowledge with the Spec-Kit Plus template"Prompt 3 (Merging strategy for your content):"After running init, how do I decide what content goes to constitution.md vsappends to CLAUDE.md? Give me specific examples based on [your project type: blog, API, docs]"

```



**Expected Outcomes**:



-   Clear understanding of what's safe to overwrite (you know in advance)

-   Concrete backup and recovery plan before running init

-   Decision framework for where your team's knowledge belongs in Spec-Kit Plus structure



**Safety Note**: Always create both a git branch AND manual backup files before running experimental commands. One backup method is good; two is better; three is gold standard.



* * *



## Checkpoint: Reflect on Your Adoption Path



Before moving to the capstone, consider:



-   **Greenfield vs Brownfield**: Which applies to your next project? (You'll use different workflows)

-   **Content Preservation**: What team knowledge would you need to preserve before adopting Spec-Kit Plus?

-   **Recovery Readiness**: Could you recover your CLAUDE.md if it got overwritten accidentally? (Git history? Manual backup?)

-   **Incremental Adoption**: Would you adopt Spec-Kit Plus all at once, or incrementally? Why?



## Lesson Title



Brownfield adoption teaches the safe, strategic process of integrating Spec-Kit Plus into existing projects while preserving team knowledge. Unlike greenfield projects where you control the entire structure from the start, brownfield work requires protecting accumulated institutional knowledge (custom CLAUDE.md, slash commands, team conventions) while avoiding data loss from framework-driven overwrites. The lesson establishes a three-layer safety workflow (git branches + manual backups + git commit history) before running `specifyplus init --here`, then shows students how to classify and merge their existing knowledge into the appropriate Spec-Kit Plus locations (constitution.md for standards, CLAUDE.md for collaboration patterns).



### Mental Models



-   **Greenfield vs Brownfield**: Greenfield means starting from scratch with no existing code, conventions, or institutional knowledge. Brownfield means inheriting an existing codebase with months/years of accumulated decisions, team knowledge, and working code that must be protected during framework adoption.

    

-   **File Preservation Profile**: Not all files are equally at risk during init. Understanding exactly what gets overwritten (CLAUDE.md, .specify/), what gets created (constitution.md, templates), and what stays completely safe (source code, git history, custom slash commands) lets students make informed decisions about backup strategy.

    

-   **Content Classification**: Team knowledge stored in a single CLAUDE.md file must be sorted by destination after adoptionâ€”coding standards and architecture principles belong in constitution.md (project-specific rules), while AI collaboration patterns and behavioral instructions belong appended to CLAUDE.md (instructions for AI working with the team).

    

-   **Redundant Safety Strategy**: Multiple recovery paths (git branch isolation, manual backup files, git commit history, diffs) provide overlapping protection. If one recovery method fails, others can restore contentâ€”eliminating single points of failure.

    



### Key Patterns



-   **Three-Layer Safety Workflow**: Create experiment branch (isolates main branch from risk) + create backup files (manual recovery) + commit current state (git recovery history) BEFORE running init. This produces three independent ways to recover if needed.

    

-   **Diff-Based Verification**: After running init, inspect actual changes via `git diff --name-only` to confirm only expected files changed. This prevents surprising overwrites and builds confidence in the safety of the process.

    

-   **Content Routing Decision Matrix**: Establish clear classification rules before mergingâ€”coding standards/architecture/design patterns go to constitution.md (project constraints), collaboration behaviors/AI interaction patterns go to CLAUDE.md (behavioral instructions). This prevents mixing concerns and maintains clean separation.

    

-   **Incremental Adoption Strategy**: Teams don't have to adopt Spec-Kit Plus all at once. Testing safely on a branch, merging selectively, and keeping both systems coexisting temporarily lets teams transition gradually without disrupting existing workflows.

    



### Common Mistakes



-   **No Backup Before Init**: Running `specifyplus init --here` without backing up CLAUDE.md first. If the init overwrites your team knowledge and you don't have backups or git history, the content is permanently lostâ€”no recovery mechanism exists.

    

-   **Single Point of Failure**: Relying on only one recovery method (like git commit) and ignoring the others. If you used git and the git history gets corrupted or accidentally reset, you have no fallback. Multiple overlapping recovery methods prevent total loss.

    

-   **Wrong Content Destination**: Putting coding standards into CLAUDE.md instead of constitution.md, or putting behavioral AI instructions into constitution.md instead of CLAUDE.md. This confuses concernsâ€”constitution defines project rules, CLAUDE.md defines how AI should behave within those rules.

    

-   **Testing on Main Branch**: Running init directly on main branch instead of an experiment branch. If something unexpected happens, your production branch is affected immediately instead of isolated in a test branch.

    

-   **Losing Context During Merge**: After init creates the new files, failing to understand what your original content was for. If you don't read and categorize your CLAUDE.md.backup content before attempting the merge, you might accidentally discard important team knowledge while integrating the new structure.

    



### Progression Context



-   **Builds on**: Chapter 14 foundation lessons teaching greenfield workflow, CLAUDE.md structure, constitution.md purpose, slash commands, and specifyplus initialization. Students understand what Spec-Kit Plus does and how it organizes knowledge before learning how to integrate it into existing systems.

    

-   **Leads to**: Capstone (Lesson 15, Chapter 14) where students apply full Spec-Kit Plus workflow to real-scale projects combining all previous lessons, including safe brownfield adoption patterns for teams inheriting existing systems.

    



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/SDD-RI-Fundamentals/spec-kit-plus-hands-on/brownfield-adoption

</brownfield-adoption>











<capstone â€” intelligence-acceleration>

-   [](/)

-   [Part 4: SDD-RI Fundamentals](/docs/SDD-RI-Fundamentals)

-   [Chapter 14: Master Spec-Kit Plus](/docs/SDD-RI-Fundamentals/spec-kit-plus-hands-on)

-   Capstone â€” Intelligence Acceleration



# Capstone â€” Intelligence Acceleration



You've completed the full SDD-RI workflow across Lessons 1-10. You've written specifications, executed implementation, and created reusable intelligence (the `section-writer` skill from Lesson 9).



Now comes the proof: **Can you complete a second project faster using the intelligence you've built?**



This capstone answers that question decisively. You'll write a second paper sectionâ€”using the `section-writer` skill created in Lesson 9â€”and measure how much faster execution becomes. This demonstrates the core principle of SDD-RI: **accumulated intelligence compounds**.



By the end of this capstone, you'll have:



-   A complete research paper with multiple sections (portfolio-ready)

-   Evidence that your skills accelerated the work

-   Understanding of why intelligence accumulation matters more than code libraries

-   The ability to tackle future projects with exponentially less effort



* * *



## The Acceleration Principle



Before diving into the capstone, let's measure what's about to happen.



### Project Timeline: First Section (Lesson 8)



Lesson 8 took you from nothing to a complete, validated paper section (introduction):



Phase



Lesson



Time Estimate



What You Did



Constitution



03



30 min



Defined research paper quality standards



Specification



04



45 min



Wrote paper introduction spec



Clarify



05



30 min



Refined spec with research requirements



Plan



06



45 min



Designed writing strategy and outline



Tasks



07



30 min



Broke down into atomic writing units



Implement



08



60 min



Generated, refined, and validated introduction



**Total**



**~3.5 hours**



**Built from scratch**



You had to:



-   Learn how to structure research paper specifications

-   Discover how to validate claims against sources

-   Troubleshoot writing clarity and academic tone

-   Write complete implementation plan

-   Handle iteration when first drafts needed refinement



### Project Timeline: Second Section (Lesson 11 - This Capstone)



This lesson takes you from specification to published section using the `section-writer` skill you created:



Phase



What You'll Do



Time Estimate



Specification



Write main body section spec



15 min



Implement



Execute with `section-writer` skill



30 min



Validation



Verify against spec criteria



15 min



Reflection



Document acceleration



10 min



**Total**



**~70 minutes**



The difference: **You're reusing intelligence.**



### Why This Is Faster



Compare the effort:



**Without skill** (hypotheticalâ€”writing second section from scratch):



-   Discover writing patterns from first section

-   Learn what "quality section" means in your context

-   Determine how to integrate research sources consistently

-   Troubleshoot tone and structure issues

-   Estimate: 2-3 hours (similar to first section)



**With skill** (your realityâ€”using `section-writer` skill):



-   Reference your `section-writer` skill (encodes writing patterns)

-   Let `/sp.implement` invoke that skill

-   Validate output

-   Done in 70 minutes



**The math**:



-   **Lesson 8**: 3.5 hours to build writing intelligence

-   **Lesson 11**: 70 minutes using that intelligence

-   **Savings**: 2 hours 20 minutes (65% time reduction)

-   **But here's the key**: The next paper section will take 45 minutes. The third will take 40 minutes. Intelligence compounds exponentially.



* * *



## Step 1: Write Your Second Section Specification



Your specification for the main body section should be **intentionally shorter** than your introduction specification (from Lesson 4). This demonstrates specification complexity reduction through skill composition.



### Why This Spec Is Simpler



**Lesson 4 (Introduction) spec required:**



-   Research paper structure overview

-   Claim definition and validation approach

-   Source integration strategy

-   Multiple decision points (all handled by you the first time)



**Lesson 11 (Main Body) spec requires:**



-   Intent (develop key claims with evidence)

-   Constraints (academic standards, source requirements)

-   Success Criteria (clarity, evidence completeness)



The upstream complexity is **encapsulated in your skill**. Your specification just needs to say *what* success looks like, not *how* the skill works internally.



### Create Your Main Body Section Specification



Create a file at `specs/paper-project/main-body-section.md` with this structure:



```

# Specification: Research Paper Main Body Section## IntentWrite the main body section of the research paper, developing the three key claims established in the introduction.**Success means**: Reader understands each claim, sees supporting evidence, and recognizes logical progression.**Target scope**: 800-1200 words across 3 subsections (one per key claim)**Foundation**: Introduction (Lesson 8) establishes context; this section builds the argument.## Constraints- **Structure**: 3 subsections (one per key claim from introduction)- **Evidence**: Each claim supported by minimum 2 research sources- **Tone**: Consistent with introduction (academic, professional)- **Clarity**: Accessible to educated reader unfamiliar with topic- **Quality standards**: Follow constitution (Chapter 03) quality metrics## Success Evals- âœ… All three claims developed with clear reasoning- âœ… Each claim supported by minimum 2 credible sources- âœ… Logical progression between sections- âœ… Academic tone consistent throughout- âœ… Word count within 800-1200 word target- âœ… Sources properly cited and integrated- âœ… Reader understands argument progression## Non-Goals- No counter-argument discussion (saved for conclusion)- No external research beyond existing sources- No section reorganization (three claims must map to outline)- No graphics or visual elements## Edge Cases Handled by SkillThe `section-writer` skill from Lesson 9 handles these:- Maintaining tone consistency with introduction- Integrating diverse sources coherently- Structuring long claims into digestible subsections- Ensuring evidence completeness- Validating academic standards

```



**Stop and verify**: Your specification is complete when:



-   âœ… Intent is clear (developing the three claims from introduction)

-   âœ… Constraints are explicit (structure, evidence requirements, tone, quality standards)

-   âœ… Success criteria are measurable (claim development, source count, word count, tone consistency)

-   âœ… Non-goals prevent scope creep (no counter-arguments, no graphics, no reorganization)



* * *



## Step 2: Understand Your Skill and How It Compounds



Now comes the leverage: you're going to invoke `/sp.implement` with explicit skill references. This tells the AI assistant: "Use the intelligence I've already built."



### The `section-writer` Skill From Lesson 9



In Lesson 9, you created the `section-writer` skill at `.claude/skills/section-writer/SKILL.md`:



```

---name: "section-writer"description: "Write clear, evidence-supported research paper sections with proper citations and logical flow. Use when user asks to write a section, needs help with academic writing, or wants to structure research content."version: "1.0.0"---# Section Writing Skill## When to Use This Skill- User asks to "write a section" or "draft content" for a research paper- User mentions thesis, evidence integration, or academic writing- User needs help structuring research content with citations## How This Skill Works1. **Outline key points**: Identify 3-5 main points the section must make2. **Write opening paragraph**: Establish context, hook interest, connect to thesis3. **Develop each point**: Topic sentence â†’ evidence/citation â†’ significance â†’ transition4. **Write closing**: Summarize insights, connect to thesis, set up next section5. **Quality check**: Verify claims cited, flow smooth, thesis connected## Quality CriteriaA section is ready when:- Someone unfamiliar with topic can follow the logic- All claims supported by evidence- No sentences require re-reading to understand- Opening hooks interest, closing connects to thesis

```



When AI discovers this skill (through the description), it applies this reasoning framework, which accelerates the entire writing process.



### Understanding Skill Invocation



Skills are automatically discovered through their descriptions. When you run `/sp.implement` and mention writing a section, AI finds your `section-writer` skill:



```

/sp.implementI need to write the main body section for my research paper.Use my section-writer skill to guide the process.Specification: specs/paper-project/main-body-section.md

```



**How skill discovery works**:



1.  AI scans `.claude/skills/` for available skills

2.  Your `section-writer` description says "Use when user asks to write a section"

3.  AI automatically loads and applies that skill's workflow



**The AI assistant will**:



1.  Read your specification

2.  Discover and load your `section-writer` skill (from `.claude/skills/section-writer/SKILL.md`)

3.  Follow the skill's "How This Skill Works" steps

4.  Apply the skill's quality criteria

5.  Validate output against success evals



**What you're NOT doing**:



-   âŒ Writing from scratch (no discovery phase)

-   âŒ Learning academic structure again (already in skill)

-   âŒ Figuring out evidence integration (skill handles this)

-   âŒ Validating against quality standards (skill applies them)



Your skill has already encoded this knowledge.



* * *



## Step 3: Execute with /sp.implement and Skills



When you're ready (your specification complete, your introduction from Lesson 8 available, research sources gathered), run this command:



```

/sp.implementWrite the main body section of my research paper.Use the section-writer skill from .claude/skills/.Specification: specs/paper-project/main-body-section.mdKey claims to develop:1. [Your first claim from introduction]2. [Your second claim from introduction]3. [Your third claim from introduction]

```



### What Happens During Execution



The AI assistant will:



1.  **Load Your Specification**

    

    -   Read intent, constraints, success criteria

    -   Identify that this section develops three claims from introduction

    -   Extract quality standards from constitution (Chapter 03)

2.  **Discover and Apply Your Skill**

    

    -   Find `section-writer` skill in `.claude/skills/section-writer/SKILL.md`

    -   Follow the skill's process (outline â†’ opening â†’ develop â†’ closing â†’ quality check)

    -   Apply the skill's quality criteria

    -   Check academic tone consistency with introduction

3.  **Execute the Writing**

    

    -   Write main body section with 3 subsections (one per claim)

    -   Integrate research sources (minimum 2 per claim)

    -   Maintain word count (800-1200 words)

    -   Ensure logical progression

4.  **Validate Success**

    

    -   Check that all three claims are developed

    -   Verify evidence completeness

    -   Confirm tone consistency

    -   Validate word count and structure

    -   Confirm success evals are met



### Your Role During Implementation



You don't watch passively. You're part of the execution:



1.  **Review Before Finalization** â€” The assistant will show you the draft section and ask: "Does this meet your specification?" You verify that claims are developed clearly and evidence is compelling.

    

2.  **Provide Feedback If Needed** â€” If something doesn't match your vision (tone feels off, a claim needs more support), the assistant will refine based on your input.

    

3.  **Validate The Outcome** â€” Once finalized, you read the section and verify it integrates well with your introduction and advances your argument.

    



* * *



## Step 4: Validate Your Main Body Section



Once the section is written, run this validation checklist:



### Pre-Implementation Checklist (Before Invoking Skill)



-    **Specification is complete** â€” All required sections filled (Intent, Constraints, Success Evals, Non-Goals)

-    **Introduction from Lesson 8 exists** â€” You have the validated introduction section

-    **Skill is referenced** â€” `/sp.implement` command includes `--skills section-writer`

-    **Research sources gathered** â€” You have credible sources for claims

-    **Claim clarity** â€” You can articulate the three claims your main body will develop



### Post-Implementation Validation (After Section Is Written)



-    **All three claims developed** â€” Each claim has clear reasoning and support

-    **Evidence completeness** â€” Each claim has minimum 2 credible sources cited

-    **Tone consistency** â€” Matches introduction (academic, professional)

-    **Word count appropriate** â€” Falls within 800-1200 word range

-    **Logical progression** â€” Reader can follow argument from claim to claim

-    **Sources integrated** â€” Citations feel natural, not forced

-    **Integrates with introduction** â€” Section builds on introduction without repetition



### Success Criteria Verification



Map your main body section to the specification success evals:



Success Eval



Evidence



Status



All three claims developed with clear reasoning



Section clearly explains each claim



âœ… / âŒ



Each claim supported by minimum 2 sources



Citations visible in text



âœ… / âŒ



Logical progression between sections



Natural flow from claim to claim



âœ… / âŒ



Academic tone consistent throughout



Matches introduction tone



âœ… / âŒ



Word count within target



800-1200 words



âœ… / âŒ



Sources properly cited



References formatted correctly



âœ… / âŒ



Reader understands progression



Argument is clear and persuasive



âœ… / âŒ



**Success condition**: All evals marked âœ…



* * *



## Step 5: Reflection â€” Why Was This Faster?



Now comes the insight: pause and measure what just happened.



### Comparing Two Experiences



**Lesson 8 (First Section from Scratch)**



You had to:



1.  Learn research paper specification structure

2.  Discover what "quality academic writing" means in context

3.  Write detailed implementation plan

4.  Troubleshoot tone and clarity issues

5.  Learn how to validate claims against sources

6.  Iterate when first draft wasn't quite right

7.  Test integration with paper structure



**Result**: Comprehensive understanding of academic writing, but time-consuming (3.5 hours).



**Lesson 11 (Second Section with Skill)**



You only had to:



1.  Write specification (15 min)

2.  Reference existing skill

3.  Run `/sp.implement` with skill reference (30 min)

4.  Validate output (15 min)



**Result**: Main body section written, faster, using intelligence you built (70 minutes).



### The Acceleration Hypothesis



Why was this faster? Document your observations:



```

## Intelligence Acceleration - Reflection### Time Comparison- **Lesson 8 (first section from scratch)**: ~3.5 hours- **Lesson 11 (second section with skill)**: ~70 minutes- **Time savings**: ~2 hours 20 minutes (65% reduction)### Why It Was Faster1. **Specification was simpler** â€” Upstream complexity handled by skill2. **Skill encoded patterns** â€” No discovery needed (academic structure, tone standards, evidence validation)3. **Composition worked** â€” Reused writing intelligence instead of inventing approach4. **Fewer decisions** â€” Skill Persona + Questions meant fewer "how should I structure this?" moments### What Came from Skill vs from Scratch- **From `section-writer` skill**: Academic writing patterns, claim development structure, evidence integration, tone consistency, quality validation- **From scratch** (minimal): Only how this specific main body maps to your paper outline### Next Paper (Hypothetical)If you wrote another research paper next month:- **Without skill**: 3+ hours per section (similar to first time)- **With refined skill**: 45 minutes per section (skill improved, patterns reinforced)### The Compounding Effect- **Paper 1**: 3.5 hours per section (build intelligence)- **Paper 2**: 1 hour per section (use intelligence)- **Paper 3**: 45 min per section (intelligence refined)- **Paper 5**: 30 min per section (mastered, patterns automated)**The math of intelligence accumulation**: Each new project takes progressively less time because you're not rediscovering patternsâ€”you're applying known solutions at increasing velocity.### Skills as YOUR Assets- **Code library**: Reusable technical solutions (functions, classes, frameworks)- **Intelligence skills**: YOUR reasoning frameworks and domain knowledge, saved as P+Q+P files- **Your advantage**: You're building a library of *thinking*, not just codeThis is how YOU scale your expertise: by encoding patterns as reusable intelligence that YOU invoke across future projects.

```



**Stop here and reflect**: What surprised you about the time difference? What did the skill actually buy you? What would change if you needed to write 10 more sections using this approach? Write 2-3 sentences in your own reflection document.



* * *



## Step 6: The Portfolio Value of Your Skills



You now have two documented skills:



1.  **`section-writer` skill** â€” Encodes academic writing patterns, claim development, evidence integration, tone consistency, quality validation

2.  **Future skills** â€” Any patterns you encode from this project



### What Makes These Skills Valuable



**To yourself**:



-   Next paper takes 50% less time

-   Next ten papers compound benefits (each faster)

-   Future writing projects start with foundation, not zero



**To a team**:



-   New team members inherit your writing standards

-   Specifications become templates others can reuse

-   Reasoning frameworks spread consistency



**To clients or organizations**:



-   Writing velocity multiplies across projects

-   Quality standards are codified, not dependent on individual skill

-   Knowledge doesn't leave when people do



### Measuring Skill Value



You can articulate the value of your `section-writer` skill in concrete terms:



**Time savings**:



-   First paper: 3.5 hours per section

-   Subsequent papers: 1.25 hours per section (65% reduction)

-   10-paper project: Save ~23 hours using this one skill



**Quality consistency**:



-   Patterns are encoded (not rediscovered with each paper)

-   Standards are applied consistently (not dependent on mood or fatigue)

-   Knowledge accumulates (second paper better than first)



**Organizational knowledge**:



-   Specification becomes template

-   Skill becomes reference implementation

-   Patterns become organization standard



This is why SDD-RI matters: not because it produces code faster, but because it compounds intelligence. Each pattern you encode makes your team exponentially more capable.



* * *



## Chapter 14 Complete Validation Checklist



You've completed Chapter 14. Verify you have:



### âœ… Projects Completed



-    **First paper section (introduction)** â€” Completed in Lesson 8, validated against spec

-    **Second paper section (main body)** â€” Completed in Lesson 11 using skill reuse

-    **Skills created** â€” `section-writer` skill (Lesson 9), possibly others



### âœ… Documentation Complete



-    **Constitution file** â€” `specs/paper-project-constitution.md` defines quality standards

-    **Introduction specification** â€” `specs/paper-project/introduction.md` completed in Lesson 4

-    **Main body specification** â€” `specs/paper-project/main-body-section.md` completed in this lesson

-    **Specifications refined** â€” Both specs passed `/sp.clarify` validation (Lesson 5)

-    **Implementation plans** â€” Detailed approach for each section

-    **Task breakdowns** â€” Atomic work units for each phase



### âœ… Skills Documented



-    **`section-writer` skill** â€” Persona + Questions + Principles for academic writing

-    **Skill metadata** â€” Proficiency levels, decision frameworks, reasoning principles documented

-    **Skill reuse applied** â€” Second section explicitly used skill from Lesson 9



### âœ… Intelligence Demonstrated



-    **Acceleration measured** â€” Lesson 8 timeline (3.5 hours) vs Lesson 11 (70 minutes) documented

-    **Skill reuse applied** â€” `/sp.implement` explicitly referenced `section-writer` skill

-    **Composition proven** â€” Main body section used writing intelligence without reinvention

-    **Compounding recognized** â€” Reflection on how future papers accelerate beyond current one



### âœ… Portfolio-Ready Project



-    **Paper sections complete** â€” At minimum: introduction + main body (core argument)

-    **Skills documented** â€” Could explain them to another developer

-    **Specification files** â€” Show your thinking (specification primacy)

-    **Timeline evidence** â€” Can demonstrate intelligence acceleration to others



**All boxes checked?** You've completed the full SDD-RI cycle. You have:



-   **Theory** (Chapter 13)

-   **Practice** (Chapter 14, Lessons 1-10)

-   **Application** (Chapter 14, Lesson 11 - this capstone with skill reuse)

-   **Reusable Intelligence** (Documented skills from Lessons 3, 6, 9)

-   **Measurable Acceleration** (Evidence that Project 2 is faster than Project 1)



You're now positioned to approach any new writing project with composition-first thinking instead of build-from-scratch thinking.



* * *



## Try With AI



**Setup**: Open your AI companion (Claude Code, Gemini, or ChatGPT). You're going to reflect on your skill library and plan future projects.



**Prompt Set**:



```

Prompt 1 (Portfolio Recognition):"I completed a research paper using SDD-RI workflow and created a reusable `section-writer` skill.In plain language: What kinds of future writing projects could I tackle faster using this skill?What if I combined this skill with a `research-validator` skill (to fact-check claims)?What would my skill library look like after 5 more papers?"Prompt 2 (Acceleration Measurement):"I spent 3.5 hours writing my first paper section (Lesson 8).I spent 70 minutes writing my second section using the section-writer skill (Lesson 11).That's a 65% time savings.What should I be measuring to prove that intelligence accumulation is real?If I write 5 more papers using this skill: What would my velocity look like?How would I show this improvement to a potential employer or client?"Prompt 3 (Skill Compounding):"If I followed the SDD-RI approach for 10 writing projects over the next year:- What would my skill library look like?- How would my writing velocity compound?- At what point would I be writing papers faster than competitors?- What's the long-term ROI of building reusable intelligence vs writing one-off papers?"

```



**Expected Outcomes**:



-   **Prompt 1**: AI should help you envision future projects where your skills provide leverage (client papers, research projects, content creation, technical writing)

-   **Prompt 2**: AI should highlight what makes intelligence valuable (pattern reuse, decision framework sharing, quality consistency, reproducibility)

-   **Prompt 3**: AI should demonstrate exponential returns from intelligence accumulation (skills compound across projects, velocity multiplies with each iteration)



**Safety Note**: When you discuss your writing project or future client work, remember that you've built this using specifications and skills as the foundation. Credit the methodology. You didn't use AI to write the entire paperâ€”you used AI to *accelerate* structured thinking with documented specifications and reusable intelligence.



**Optional Stretch**: Now that you have a reusable skill, find a real writing project (research paper for a client, technical documentation, proposal, blog series). Write a specification for that project. Estimate how long it would take you to execute using your `section-writer` skill. Then, using your skill, actually write that project. This is how writers monetize their intelligence accumulation: by solving client problems faster than competitors.



* * *



**Chapter 14 is now complete.** You've demonstrated full SDD-RI masteryâ€”from specification to implementation to reusable intelligence to accelerated execution.



You're ready for more complex projects. Your skills compound. Your intelligence library grows. Your development velocity increases exponentially.



This is the power of SDD-RI.



# Capstone â€” Intelligence Acceleration



This capstone proves that skill reuse accelerates work. By writing a second paper section using the `section-writer` skill YOU created in Lesson 9, you demonstrate measurable acceleration: the second section takes 70 minutes instead of 3.5 hours. This is not just about speedâ€”it's about understanding how YOUR skill enables velocity gains. You'll measure this acceleration concretely, recognize why it happens, and understand how skills become valuable assets. This lesson answers the central question: Can you complete a task faster using a skill you've built? The answer is yes, and you'll have the data to prove it.



## Mental Models



-   **Skill-Based Acceleration**: Each skill YOU create encodes patterns into reusable SKILL.md format (at `.claude/skills/<skill-name>/SKILL.md`). The second task using that skill doesn't rediscover patternsâ€”AI discovers and applies them. Lesson 8 takes 3.5 hours from scratch; Lesson 11 takes 70 minutes because AI finds and follows YOUR skill. This is reasoning reuseâ€”YOUR documented workflows applied at increasing velocity.

    

-   **Specification Simplification Through Skills**: When you build the first section, your specification must detail all decision points. For the second section with YOUR skill, your specification shrinks to just intent, constraints, and success criteria. The upstream complexityâ€”how to write academically, how to validate claimsâ€”is now encapsulated in YOUR skill that YOU invoke.

    

-   **Skills as YOUR Assets**: Traditional reuse focuses on code libraries. YOUR skills are different. Your `section-writer` skill (at `.claude/skills/section-writer/SKILL.md`) encodes your workflow, quality criteria, and process steps. When AI discovers this skill, it applies YOUR reasoning patterns.

    

-   **Skills vs PHRs**: PHRs are auto-created within a project and stay there (YOUR learning record). Skills are user-created after good sessions and can be invoked in future work. PHRs document what happened; Skills are active tools the agent uses.

    



## Key Patterns



-   **Acceleration Measurement**: Time tracking creates evidence. First section (Lesson 8): 3.5 hours from scratch. Second section (Lesson 11): 70 minutes using YOUR skill. This 65% time reduction proves that YOUR skill works.

    

-   **Skill Invocation**: When implementing, mention the skill and AI discovers it automatically through its description. The AI follows YOUR process steps and quality criteria from the SKILL.md file. YOU're not creating new reasoning patternsâ€”YOU're applying YOUR proven patterns.

    

-   **Specification Simplification**: Compare your first spec (detailed everything) to your second spec (simpler). Why? YOUR skill already solved "how to structure academic writing." Your new specification just names what success looks likeâ€”YOUR skill handles how.

    

-   **Reflection-Driven Insight**: Pause and document why acceleration happened. What patterns did YOUR skill encode? What decisions did YOU not have to make because the skill handled them? This reflection embeds understanding of why skills compound.

    



## Common Mistakes



-   **Confusing Speed with Skill Benefit**: The mistake is thinking Lesson 11 is faster just because you're experienced. Without YOUR skill, the second section would still take 2-3 hours. With YOUR skill, it takes 70 minutes. The difference is YOUR skill, not just experience.

    

-   **Creating Skills Too Specific**: Build skills that are domain-general (`section-writer` for any academic writing), not domain-specific ("climate change research skill"). Domain-general skills work across future projects.

    

-   **Skipping Reflection**: The real learning happens when YOU document why the second section was faster. By measuring and articulating acceleration, YOU embed understanding of why skills work.

    

-   **Not Invoking Skills Explicitly**: Skills don't apply automatically. YOU must invoke them. YOU tell the AI "use my section-writer skill" and provide the skill content.

    



## Progression Context



-   **Builds on**:

    

    -   Lesson 3: Constitution (quality standards)

    -   Lesson 4: First specification (becomes foundation)

    -   Lessons 6-8: Planning, tasks, implementation (where patterns emerged)

    -   Lesson 9: Skill creation (where YOU built the section-writer skill)

    -   Lesson 10: Brownfield adoption theory

-   **Leads to**:

    

    -   **Future Projects**: YOU invoke YOUR skill on similar tasks

    -   **Skill Library Growth**: YOU create more skills as YOU recognize patterns worth encoding

    -   **Team Value**: YOUR skills can help team members if YOU share them

    -   **Competitive Advantage**: YOUR skill library makes YOU faster than starting from scratch



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/SDD-RI-Fundamentals/spec-kit-plus-hands-on/capstone

</capstone â€” intelligence-acceleration>









<when-NOT-to-use-ai-agents>

-   [](/)

-   [Part 1: Introducing AI-Driven Development](/docs/Introducing-AI-Driven-Development)

-   [Chapter 3: The Digital FTE Strategy](/docs/Introducing-AI-Driven-Development/digital-fte-strategy)

-   When NOT to Use AI Agents



# When NOT to Use AI Agents: Security, Compliance, and Strategic Guardrails



You understand strategy and requirements. You can identify opportunities where AI agents create value. Now comes the harder skill: recognizing when agents would create unacceptable riskâ€”and saying no.



This lesson inverts the question from "How do I build a Digital FTE?" to "What should I absolutely NOT automate?" The answer separates responsible practitioners from reckless ones.



## Six Common Pitfalls: How Digital FTEs Fail



Every pitfall follows the same pattern: Pressure to automate everything. Insufficient controls. Catastrophic failure. Here are the six most common scenarios where "we could automate this" becomes "we shouldn't have."



### Pitfall 1: Fully Autonomous Legal Decisions



**The scenario:** Your Digital FTE reads contracts, case law, and client facts, then sends legal opinions directly to clients without human review.



**Why it fails:**



-   Legal liability: If advice is wrong, YOU are liable, not the AI

-   Qualification: Only licensed attorneys can practice law

-   Judgment calls: Context-dependent decisions require human expertise

-   Precedent: Your agent might miss critical case law changes



**The risk (consequences):**



-   Malpractice lawsuits

-   Bar association complaints

-   Client financial harm

-   Criminal liability (unauthorized practice of law)



**How to fix it:**



-   Human attorney reviews ALL agent-generated opinions before client contact

-   Agent becomes research assistant (preliminary research, document summarization)

-   Attorney makes final judgment calls

-   Document the review process for liability protection



**When it's OK:**



-   Preliminary legal research (case law searches, contract clause identification)

-   Document summarization (agent reads 50 pages, summarizes key sections for attorney review)

-   Regulatory compliance checking (GDPR, SOC 2 requirement verification)

-   **Always: Attorney approval before client sees any content**



* * *



### Pitfall 2: Financial Transactions Without Authorization



**The scenario:** Your Digital FTE reads bank accounts, investment positions, and performance data, then executes trades automatically or initiates transfers without explicit human sign-off.



**Why it fails:**



-   Fraud risk: Compromised agent = unauthorized fund transfers

-   Regulatory risk: Financial transactions require documented authorization

-   Error amplification: Wrong decision at 3 AM affects millions

-   Audit requirements: Every transaction must be traceable



**The risk (consequences):**



-   Customer financial harm and litigation

-   Regulatory fines (SEC, FINRA, banking regulators)

-   Reputational damage ("We lost customer funds to a bot")

-   Criminal charges for wire fraud



**How to fix it:**



-   Agent RECOMMENDS trades; human APPROVES

-   Every transaction requires explicit human authorization with timestamp logging

-   Two-person rule: High-value transactions require two humans

-   Comprehensive audit trail (agent reasoning + human decision + authorization)

-   Rate limiting: Prevent multiple sequential transactions without human review



**When it's OK:**



-   Routine reconciliation (comparing account totals, identifying discrepancies)

-   Reporting (generating account statements, performance reports)

-   Anomaly detection (flagging unusual transactions for human review)

-   Portfolio analysis (suggesting rebalancing strategies without executing)

-   **Always: Human reviews agent recommendations before ANY real money moves**



* * *



### Pitfall 3: Unmonitored Medical Recommendations



**The scenario:** Your Digital FTE reads patient records, medical literature, and lab results, then sends treatment recommendations directly to patients without physician review.



**Why it fails:**



-   Medical liability: Only licensed physicians can recommend treatments

-   Patient harm: Incorrect recommendation can cause injury or death

-   Practitioner-specific context: Recommendations depend on physician specialty and patient relationship

-   Regulatory violation: Practice of medicine without license



**The risk (consequences):**



-   Patient injury or death lawsuits

-   Medical malpractice insurance denial

-   State medical board investigation

-   Criminal charges (practicing medicine without license)

-   FDA scrutiny if claims are made about health outcomes



**How to fix it:**



-   Physician reviews ALL recommendations before patient contact

-   Agent becomes clinical research tool (literature search, clinical guideline summarization)

-   Agent surfaces relevant patient data (labs, comorbidities, contraindications)

-   Physician makes final clinical judgment

-   Document physician review as part of medical record



**When it's OK:**



-   Patient education (agent explains what diabetes is, not how to treat yours)

-   Appointment scheduling (agent books time, doesn't provide medical advice)

-   Lab result interpretation summaries (agent highlights abnormal values for physician review)

-   Clinical trial matching (agent identifies trials matching patient criteria, physician approves enrollment)

-   **Always: Licensed physician reviews before any recommendation reaches patient**



* * *



### Pitfall 4: Biased Hiring Agents



**The scenario:** Your Digital FTE screens resumes, scores candidates, and forwards only "qualified" candidates to hiring managers. No human screens the full candidate pool.



**Why it fails:**



-   Discrimination risk: Biased training data perpetuates discrimination at scale

-   Regulatory violation: Equal Employment Opportunity laws require fair selection

-   Systemic exclusion: Bias might exclude entire demographic groups

-   Legal exposure: Class action discrimination lawsuits have reached $millions



**The risk (consequences):**



-   EEOC investigation and enforcement action

-   Class action discrimination lawsuits ($10M+ settlements common)

-   Reputational damage (PR nightmare: "Our bot discriminates")

-   Talent loss (skilled candidates from excluded groups avoid applying)

-   Compliance failure (SEC/investor penalties if public company)



**How to fix it:**



-   Human reviews ALL resumes (agent as screening accelerator, not decision maker)

-   Agent flags candidates for review; human decides forwarding

-   Regular bias audits (compare hiring outcomes by demographic group)

-   Explainability requirement: Can you explain why agent ranked candidate X over Y?

-   Diversity monitoring: Ensure candidate pool includes underrepresented groups



**When it's OK:**



-   Resume screening with human override (agent suggests screening, human approves)

-   Scheduling (agent books interviews, doesn't rate candidates)

-   Interview note summarization (agent writes summary; hiring manager reviews)

-   Background check coordination (agent manages process, human interprets results)

-   **Always: Human makes final hiring decision with full candidate visibility**



* * *



### Pitfall 5: Untracked Data Access



**The scenario:** Your Digital FTE accesses customer databases, financial records, or health information but leaves no audit trail. No logging of what data was accessed, when, or why.



**Why it fails:**



-   Privacy violation: Untracked access violates fundamental privacy principles

-   Regulatory non-compliance: Every regulation requires audit trails

-   Insider threat: Compromised agent = undetected data breach

-   Incident investigation impossible: If breach happens, you can't investigate



**The risk (consequences):**



-   GDPR fines: 4% of global revenue (up to ~$20M for large companies)

-   CCPA fines: $100-$750 per consumer per violation (class actions: $millions)

-   HIPAA fines: $100-$50,000 per violation (millions for hospital data)

-   Breach notification costs (legal, credit monitoring, PR): $millions

-   Data destruction lawsuits from compromised customers



**How to fix it:**



-   Comprehensive audit logging: Every data access logged with timestamp, user, purpose

-   Access control: Agent only accesses minimum necessary data

-   De-identification: Use anonymized/aggregated data whenever possible

-   Regular audits: Monthly review of access logs for anomalies

-   Immutable logs: Logs cannot be modified (prevents cover-up)



**When it's OK:**



-   De-identified data analysis (aggregate statistics, no individual records)

-   Public data processing (data you're licensed to access publicly)

-   Fully logged access (every read/write is audited)

-   Encryption in transit and at rest (data protected during access)

-   **Always: Comprehensive audit trail for ANY sensitive data access**



* * *



### Pitfall 6: No Audit Trail for Agent Decisions



**The scenario:** Your Digital FTE makes critical decisions (loan approval, content moderation, credit scoring, hiring recommendations) but doesn't log the reasoning. If regulators ask "Why was this decision made?", you have no answer.



**Why it fails:**



-   Regulatory non-compliance: Most industries require documented decision rationale

-   Explainability requirement: AI Act, Fair Lending regulations require you to explain decisions

-   Defense impossible: In litigation, you can't justify decisions without audit trail

-   Consistency impossible: Without logs, agent decisions might be inconsistent (regulatory violation)



**The risk (consequences):**



-   Regulatory enforcement (FTC, CFPB, SEC, industry regulators)

-   Fair lending lawsuits (disparate impact: agent denies loans at higher rate for protected class)

-   FCRA violations (Fair Credit Reporting Act)

-   Discrimination liability (inability to explain decisions suggests bias)

-   Criminal liability (if logs show intentional discrimination)



**How to fix it:**



-   Log all decisions with reasoning: What inputs did agent consider? What rules triggered? What was the output?

-   Explainability requirement: Anyone (including regulator) can ask "Why was this decision made?" and get clear answer

-   Version control: Track how agent logic changed over time (for regression analysis)

-   Regular audits: Analyze decisions for patterns (e.g., do protected classes get worse outcomes?)

-   Appeal mechanism: Humans can review and override agent decisions



**When it's OK:**



-   Internal tools only (no regulatory exposure)

-   Fully logged decisions with explainability

-   Regular bias/fairness audits

-   Human-in-the-loop for significant decisions

-   **Always: Log decision rationale, not just final output**



* * *



## Security and Compliance Minimums



Every Digital FTE, regardless of domain, needs baseline guardrails:



### The Three-Tier Model



**Tier 1: Data Access Controls**



-   Agent accesses only data it needs (principle of least privilege)

-   Different credentials for different data sources

-   Regular access review: Quarterly, remove unnecessary permissions

-   No shared credentials: Each agent gets unique identity



**Tier 2: Audit and Logging**



-   Every action logged: What? When? Who triggered it? What data touched?

-   Immutable logs: Cannot be deleted/modified (prevents cover-up)

-   Log retention: Comply with regulation (typically 3-7 years)

-   Regular log review: Monthly automated analysis for anomalies



**Tier 3: Governance and Oversight**



-   Human accountability: Someone responsible for agent behavior

-   Escalation procedures: Processes for unusual decisions

-   Regular validation: Does agent still perform as intended?

-   Incident response: If agent acts incorrectly, what's the recovery process?



### Implementation Checklist



```

â–¡ Access Control  â–¡ Documented list of data sources agent accesses  â–¡ Verify agent has minimum necessary permissions  â–¡ Monthly access review, remove unnecessary privileges  â–¡ No shared credentials (agent has unique identity)â–¡ Audit Trail  â–¡ All actions logged to immutable storage  â–¡ Logs include: timestamp, action, user/agent ID, data touched, outcome  â–¡ Log retention matches regulatory requirements  â–¡ Logs cannot be modified after-the-factâ–¡ Human Oversight  â–¡ Named individual responsible for agent behavior  â–¡ Process for reviewing agent decisions (frequency depends on risk)  â–¡ Escalation procedure for unusual outputs  â–¡ Incident response plan (what to do if agent fails)â–¡ Documentation  â–¡ Agent specification (what it does, constraints)  â–¡ Data access documentation (what it reads, why)  â–¡ Compliance mapping (which regulations apply)  â–¡ Training for humans who oversee agent

```



* * *



## Industry-Specific Guardrails



If you operate in regulated industries, specific rules apply:



### HIPAA (Healthcare)



**Applies to:** Medical records, patient health information, hospital systems, telemedicine platforms, health insurance companies



**Key requirements:**



-   Business Associate Agreement (if AI vendor processes patient data)

-   Encryption: Data in transit and at rest

-   Access controls: Healthcare workers only

-   Audit logs: All patient data access logged and reviewed

-   Breach notification: Patients notified within 60 days of breach

-   Minimum necessary: Agent only accesses patient data for specific treatment/operation



**Guardrail:** No patient data reaches any AI system without encryption, explicit consent, and human physician oversight.



### SOC 2 (Enterprise Software)



**Applies to:** SaaS companies, cloud platforms, service providers handling customer data



**Key requirements:**



-   Security controls: Defined and tested

-   Availability: Uptime and disaster recovery

-   Processing integrity: Transactions are complete and authorized

-   Confidentiality: Customer data is protected

-   Privacy: Data handling matches customer expectations

-   Annual audit: Third-party verification of controls



**Guardrail:** Before deploying agent to customer-facing service, complete SOC 2 audit showing agent doesn't compromise security/availability/privacy.



### PCI DSS (Payment Card Industry)



**Applies to:** Systems that handle credit/debit card data, payment processors, e-commerce platforms



**Key requirements:**



-   No agent storage of card data (agent never "sees" full card numbers)

-   Encryption: Card data encrypted in transit and at rest

-   Access control: Only authorized systems access card data

-   Audit trail: Every card transaction logged

-   Quarterly vulnerability scanning

-   Annual penetration testing



**Guardrail:** No AI agent directly processes, stores, or transmits card numbersâ€”always use tokenization (replace card # with token that agent uses).



### GDPR (European Data)



**Applies to:** Any system processing data of EU residents (applies globally, not just Europe)



**Key requirements:**



-   Consent: User must opt-in to data processing

-   Right to deletion: Can demand agent never sees their data again

-   Data minimization: Collect only necessary data

-   Purpose limitation: Use data only for stated purpose

-   Data protection impact assessment (DPIA): Before deploying agent with personal data

-   Right to explanation: If agent makes decision about you, you can demand explanation



**Guardrail:** Before deploying agent to EU market, conduct DPIA documenting how agent processes personal data, what risks exist, and how mitigations address them.



### State Privacy Laws (California, Virginia, Colorado, etc.)



**Applies to:** Systems processing data of US state residents



**Key requirements:** Vary by state but generally include:



-   Disclosure: Tell users what data you collect

-   Opt-out: Users can ask agent never to use their data

-   Right to access: Users can see what data agent has

-   Data security: Protect data from unauthorized access

-   Breach notification: Notify users if data is compromised



**Guardrail:** Document which state laws apply, verify agent compliance with each.



* * *



## Shadow Mode Deployment Strategy: Testing Before Full Automation



The safest way to deploy a Digital FTE in high-risk domains is **shadow mode**: agent runs in parallel with human, human makes decisions, agent suggestions are logged but never executed.



### The Three-Phase Approach



**Phase 1: Shadow Mode (Weeks 1-4)**



-   Agent runs and generates recommendations

-   Humans make all final decisions (ignore agent suggestions initially)

-   Log all agent outputs and human decisions

-   Measure: Does agent agree with humans 80%+ of the time?

-   Risk: Low (human is in control)



**Validation Questions:**



-   Where does agent disagree with experienced humans?

-   Are disagreements errors (agent is wrong) or edge cases (agent found something human missed)?

-   What patterns appear in agent mistakes?



**Phase 2: Augmented Decision-Making (Weeks 5-8)**



-   Humans start using agent recommendations as input (not sole source)

-   Humans still make final decisions, but faster (agent does research, human judges)

-   Log: Agent recommendation vs human decision

-   Measure: Do humans override agent less than 20% of time? If more than 20%, more training needed



**Validation Questions:**



-   Are overrides because agent is wrong, or because human adds context agent lacks?

-   Can we improve agent to avoid common overrides?

-   Is augmented mode actually faster than human-alone?



**Phase 3: Selective Automation (Weeks 9+)**



-   Agent makes decisions for low-risk scenarios (high confidence, well-tested)

-   Humans still review high-risk scenarios

-   Thresholds documented: "Agent decides if confidence above 95%, human if below 90%, escalate if 90-95%"

-   Log: All decisions and confidence levels

-   Regular audits: Monthly review of decisions for accuracy, bias, compliance



**Validation Questions:**



-   Do automated decisions maintain quality?

-   Are there patterns in escalated cases?

-   Is bias emerging in specific categories?



### Shadow Mode Checkpoint Template



Before advancing to next phase, answer these:



```

PHASE 1 CHECKPOINT (Shadow Mode Complete)â–¡ Agent ran in parallel for minimum 2-4 weeksâ–¡ 50+ decisions logged for analysisâ–¡ Agent agreement with human: 80%+?â–¡ Where agent disagreed, was it error or insight? List categories:  â–¡ Categories where agent was clearly wrong: [list]  â–¡ Categories where agent identified something human missed: [list]  â–¡ Categories needing more training: [list]â–¡ Ready to advance? YES / NO / NEEDS RETRAININGPHASE 2 CHECKPOINT (Augmented Decision-Making Complete)â–¡ Humans using agent input for 2-4 weeksâ–¡ 50+ decisions logged, comparing recommendation vs human decisionâ–¡ Human override rate under 20%?â–¡ Where humans override, is it because:  â–¡ Agent is wrong: [estimated %]  â–¡ Agent lacks context human has: [estimated %]  â–¡ Human is cautious (both reasonable): [estimated %]â–¡ Can we improve agent to reduce overrides? How?â–¡ Ready to advance? YES / NO / NEEDS REDESIGNPHASE 3 CHECKPOINT (Selective Automation Active)â–¡ Automated decisions running for minimum 4 weeksâ–¡ Decision quality maintained (spot-check 20 decisions)?â–¡ Bias audit: Compare outcomes across demographic groups  â–¡ Decision rates similar across groups?  â–¡ Appeal/escalation rates similar across groups?â–¡ Compliance: All decisions properly logged?â–¡ Ready to expand scope? YES / NO / NEEDS INVESTIGATION

```



* * *



## Red Flag Detection Framework: When to Say No



Sometimes you discover mid-project that an agent idea shouldn't exist. Red flags that indicate stopping is better than proceeding:



### Signal 1: Insufficient Audit Trail Feasibility



If you cannot log agent reasoning and decisions due to system constraints, stop. An agent you can't audit creates liability you cannot defend.



### Signal 2: Irreplaceable Human Judgment



If decisions require judgment that no training dataset contains (unique context, specialized expertise, ethical judgment calls), an agent will fail. Solo experts can't be automated; they can only be augmented.



### Signal 3: Regulatory Uncertainty



If no clear guidance exists on whether automation is allowed (e.g., "Can an AI manage customer complaints?"), don't guess. Consult compliance experts before building.



### Signal 4: High-Consequence Errors



If a single agent error causes severe harm (patient death, financial ruin, discrimination), the deployment cost of adequate validation exceeds the automation benefit.



### Signal 5: Adversarial Pressure



If stakeholders pressure you to skip validation ("We need this live NOW"), stop and escalate. Time pressure is the enemy of security.



### Signal 6: Untrained or Biased Data



If training data contains human biases (hiring datasets biased toward men, credit datasets biased against minorities, medical datasets underrepresent women), the agent will perpetuate that bias at scale.



**Decision Framework:**



-   If 1 signal present: Yellow flag, additional review needed

-   If 2+ signals present: Red flag, reconsider the project

-   If 3+ signals present: Stop, redesign or abandon



* * *



## Try With AI



### Part 1: Identify Your Regulatory Requirements



Pick a proposed Digital FTE from your domain:



```

Propose your agent:"I'm considering building a Digital FTE that [describe what it does].It would serve [describe who: customers/employees/partners].It would handle [describe what data: financial/health/personal].It operates in [describe which industry/geography]."Ask your AI:"Which regulations apply to this agent?- Start with the obvious ones (industry-specific: HIPAA, SOC 2, PCI, GDPR, state privacy laws)- Then identify the hidden ones (accessibility laws? employment laws? consumer protection laws?)For each regulation, explain: Why does it apply? What are the key requirements?Then tell me: If I automate this, what could go wrong legally?"

```



**Expected outcome:** Clear understanding of which rules apply and what audit/control requirements each demands.



* * *



### Part 2: Apply the Six Pitfalls Framework



```

Ask your AI:"I'm building [your agent]. Let's apply the Six Pitfalls framework:For each pitfall, answer:1. Does my agent face this risk? (Yes/No)2. If yes, which of the three elements apply?   - Full autonomy without oversight   - No audit trail   - High-consequence error possible3. How would I mitigate each applicable risk?Then prioritize: Which mitigations are mandatory vs nice-to-have?"

```



**Expected outcome:** Mitigation roadmap identifying which controls are non-negotiable before launch.



* * *



### Part 3: Design Your Shadow Mode Plan



```

Ask your AI:"Help me plan 12-week shadow mode deployment for my agent:Phase 1 (Weeks 1-4): What metrics prove agent is ready to augment human decision-making?  - How many decisions should I test?  - What agreement rate with humans is acceptable?  - What counts as an 'error' vs 'edge case'?Phase 2 (Weeks 5-8): What's the human override rate that indicates we're ready for selective automation?  - If override rate is >30%, what should I do?  - How do I distinguish between 'agent is wrong' and 'human is being cautious'?Phase 3 (Weeks 9+): What bias audits should I run?  - What demographic groups should I compare?  - If outcomes differ by group, when does that indicate bias vs legitimate variation?Finally: Write my Phase 1 checkpoint template (the checklist I'll complete before advancing)."

```



**Expected outcome:** Detailed shadow mode plan with measurable success criteria, ready to execute.



* * *



### Part 4: Critical Evaluation Exercise



This is the hardest question. Answer honestly:



```

"I'm excited about my Digital FTE. Now, let's pressure-test it:Ask my AI: 'Play devil's advocate. Give me your top 3 reasons this agent shouldn't existâ€”not in theory, but in practice. What could go wrong that I haven't considered? When might I regret building this?'Then ask yourself: 'Which of those reasons resonates with me? Am I ignoring real risks because I'm excited about the opportunity?'Finally: 'If this agent fails, who gets hurt? How bad could it be? Is the automation benefit worth that risk?'"

```



**Expected outcome:** Honest assessment of whether this agent should exist. Some ideas survive this. Some should be abandoned.



* * *



### Core Concept



Knowing when NOT to automate separates responsible practitioners from reckless ones. Six common pitfallsâ€”autonomous legal/medical/financial decisions, biased hiring, untracked data access, missing audit trailsâ€”create unacceptable liability that no automation benefit justifies.



### Key Mental Models



-   **Three-Tier Security Model**: Data access controls (minimum necessary) â†’ Audit and logging (immutable, comprehensive) â†’ Governance and oversight (human accountability, escalation procedures).

-   **Shadow Mode Deployment**: Phase 1 (agent suggests, human decides, log agreement rate) â†’ Phase 2 (human uses agent input, measure override rate) â†’ Phase 3 (selective automation for high-confidence scenarios).

-   **Red Flag Framework**: Insufficient audit feasibility, irreplaceable human judgment, regulatory uncertainty, high-consequence errors, adversarial pressure, biased data. Two+ signals = reconsider; three+ = stop.



### Critical Patterns



-   Human reviews ALL agent outputs before external contact (legal opinions, medical recommendations, financial transactions)

-   Log all decisions with reasoning, not just outcomesâ€”"Why was this decision made?" must be answerable

-   Industry-specific guardrails: HIPAA (healthcare), SOC 2 (enterprise), PCI DSS (payments), GDPR (EU data)

-   Shadow mode checkpoints: 80%+ agreement with humans (Phase 1), under 20% override rate (Phase 2), bias audits across demographic groups (Phase 3)



### Common Mistakes



-   Automating fully without human oversight because "it's faster"â€”creates liability you cannot defend

-   Skipping audit trails because logging is "expensive"â€”makes incident investigation impossible

-   Ignoring bias in training dataâ€”agent perpetuates discrimination at scale



### Connections



-   **Builds on**: Three requirements for success (Lesson 8)

-   **Leads to**: Strategic synthesis (Lesson 10) where you integrate guardrails into your personal Digital FTE plan



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/Introducing-AI-Driven-Development/digital-fte-strategy/when-not-to-use-agents

</when-NOT-to-use-ai-agents>









<AGENTS.md - project-context-for-agents>

-   [](/)

-   [Part 1: Introducing AI-Driven Development](/docs/Introducing-AI-Driven-Development)

-   [Chapter 2: AIFF Foundation & Agent Standards](/docs/Introducing-AI-Driven-Development/aiff-foundation)

-   AGENTS.md - Project Context for Agents



# AGENTS.md - Project Context for Agents



You're deploying your Digital SDR to a new client. Their codebase uses:



-   TypeScript with strict mode

-   Jest for testing (not Mocha)

-   Conventional commits (not freeform messages)

-   No console.log in production code



Your Digital FTE doesn't know any of this. It starts generating JavaScript instead of TypeScript. It writes `console.log` everywhere. It creates commits with messages like "fixed stuff."



The client's code review rejects everything.



**AGENTS.md prevents this.**



AGENTS.md is a standard Markdown file that teaches AI agents local rules. When your Digital FTE enters a new project, it reads the AGENTS.md and immediately understands how to behave in that environmentâ€”without custom configuration.



* * *



## Why AGENTS.md Exists: Humans â‰  Agents



Every developer knows README.md. It tells humans:



-   What the project does

-   How to install it

-   How to contribute

-   License and credits



But AI agents need different information:



Humans Need



Agents Need



Project motivation and goals



Build and test commands



Getting started tutorial



Code style rules



Contribution guidelines



Security constraints



Screenshots and demos



File organization patterns



README.md answers "What is this project?" AGENTS.md answers "How should I behave in this project?"



**AGENTS.md is README for AI agents.**



Without it, your Digital FTE might:



-   Use the wrong coding style (tabs vs spaces, naming conventions)

-   Skip required tests (doesn't know which command to run)

-   Violate security practices (hardcodes API keys)

-   Produce work that fails code review (wrong commit format)



With AGENTS.md, your Digital FTE adapts to each client's environment automatically. That's the difference between a demo and a product.



* * *



## What Goes in AGENTS.md



AGENTS.md is standard Markdown. No special syntax. Include whatever is relevant for an AI agent working in your codebase:



### Build and Test Commands



Tell the agent how to verify its work:



```

## Build Commands- `pnpm install` - Install dependencies- `pnpm run build` - Production build- `pnpm run dev` - Development server with hot reload- `pnpm test` - Run all tests- `pnpm test:watch` - Run tests in watch mode- `pnpm run lint` - Check for lint errors- `pnpm run typecheck` - Verify TypeScript types

```



**Why this matters:** An agent that can't verify its work produces unreliable output. Build commands let it check before committing.



### Code Style Guidelines



Tell the agent your conventions:



```

## Code Style- Use TypeScript strict mode for all new code- Prefer functional components over class components- Maximum function length: 50 lines- Use named exports, not default exports- File names: kebab-case (e.g., `user-profile.tsx`)- Variable names: camelCase- Constants: SCREAMING_SNAKE_CASE

```



**Why this matters:** Style violations create noise in code reviews. Agents following your style integrate seamlessly.



### Security Considerations



Tell the agent what to avoid:



```

## Security- Never hardcode API keys, tokens, or secrets- Use environment variables for all credentials- Sanitize all user inputs before database queries- No `eval()` or `Function()` constructors- HTTPS only for external API calls- Log errors, never log sensitive data

```



**Why this matters:** Security mistakes are expensive. Explicit rules prevent common vulnerabilities.



### Architecture and Patterns



Tell the agent your design decisions:



```

## Architecture- All API routes go in `/src/api/`- Shared components in `/src/components/shared/`- Feature-specific components in `/src/features/[feature]/`- Database queries only through `/src/db/` layer- No direct database access from API routes

```



**Why this matters:** Consistent architecture makes codebases maintainable. Agents that follow patterns produce code that fits.



### Commit and PR Guidelines



Tell the agent your workflow:



```

## Git Workflow- Use conventional commits: `feat:`, `fix:`, `docs:`, `refactor:`- Branch names: `feature/description` or `fix/description`- One logical change per commit- PR titles should summarize the change, not describe activity- Include tests for new functionality

```



* * *



## The Hierarchy Rule



AGENTS.md supports project-specific granularity through a simple rule:



**The nearest AGENTS.md file takes precedence.**



Consider a monorepo:



```

my-company/â”œâ”€â”€ AGENTS.md                    â† Root: company-wide rulesâ”œâ”€â”€ packages/â”‚   â”œâ”€â”€ frontend/â”‚   â”‚   â”œâ”€â”€ AGENTS.md            â† Frontend-specific rulesâ”‚   â”‚   â””â”€â”€ src/â”‚   â”‚       â””â”€â”€ components/â”‚   â”‚           â””â”€â”€ Button.tsxâ”‚   â””â”€â”€ backend/â”‚       â”œâ”€â”€ AGENTS.md            â† Backend-specific rulesâ”‚       â””â”€â”€ src/â”‚           â””â”€â”€ routes/â”‚               â””â”€â”€ users.ts

```



**Scenario 1:** Agent edits `packages/frontend/src/components/Button.tsx`



-   Finds `packages/frontend/AGENTS.md`

-   Those rules apply (nearest file wins)

-   Might say: "Use React, prefer hooks, styled-components for CSS"



**Scenario 2:** Agent edits `packages/backend/src/routes/users.ts`



-   Finds `packages/backend/AGENTS.md`

-   Those rules apply

-   Might say: "Use Express, async/await, Prisma for database"



**Scenario 3:** Agent edits a file at root level



-   No subdirectory AGENTS.md found

-   Falls back to root `AGENTS.md`

-   Company-wide rules apply



This hierarchy lets you:



-   Define project-wide conventions at the root

-   Override with specific rules in subdirectories

-   Handle monorepos with different standards per package



* * *



## Adoption: 60,000+ Projects



AGENTS.md achieved rapid adoption since OpenAI introduced it in August 2025:



**By the numbers:**



-   **60,000+** open source projects now include AGENTS.md

-   **88** AGENTS.md files in OpenAI's own repository alone

-   Every major AI coding agent supports it



**Supported agents:**



-   Amp

-   Claude Code

-   Codex CLI

-   Cursor

-   Devin

-   Factory

-   Gemini CLI

-   GitHub Copilot

-   goose

-   Jules

-   VS Code



When you add AGENTS.md to your project, all these agents immediately benefit. When you deploy your Digital FTE to a client's project that has AGENTS.md, it immediately adapts.



* * *



## The Portability Advantage



Remember the Agent Factory vision: selling the same Digital FTE to many clients.



Without AGENTS.md:



Client



Environment



Your Work



Client A



Python, pytest, Black



Custom configuration



Client B



TypeScript, Jest, Prettier



Different custom configuration



Client C



Go, standard library, gofmt



Yet another configuration



With AGENTS.md:



Client



Environment



Your Work



Client A



Python, pytest, Black



Their AGENTS.md handles it



Client B



TypeScript, Jest, Prettier



Their AGENTS.md handles it



Client C



Go, standard library, gofmt



Their AGENTS.md handles it



**Same Digital FTE. Different environments. Zero customization from your side.**



You're not asking clients to change how they work. You're offering an agent that slots into their existing environment seamlessly. That's why AGENTS.md is foundational to the Agent Factory vision.



* * *



## Creating Effective AGENTS.md



Think about what you'd tell a new developer joining your team. Now filter for what an AI agent actually needs to know.



### Include:



-   Commands to run (build, test, lint)

-   Code style rules (formatting, naming)

-   Security constraints (what to never do)

-   Architecture patterns (where code goes)

-   Workflow rules (commits, PRs)



### Exclude:



-   Project motivation or history (agents don't need context)

-   Installation tutorials (agents have different setup)

-   Screenshots or demos (agents can't see them)

-   Long explanations (agents need rules, not rationales)



### Keep It Actionable:



âŒ "We value clean code and good practices." âœ… "Maximum function length: 50 lines. Use descriptive names."



âŒ "Security is important to us." âœ… "Never hardcode secrets. Use `process.env` for all credentials."



* * *



## Try With AI



Use your AI companion (Claude, ChatGPT, Gemini, or similar) to practice:



### Prompt 1: Create an AGENTS.md



```

I'm setting up a new TypeScript project with these characteristics:- Next.js 14 with App Router- Tailwind CSS for styling- Prisma for database- Jest for testing- GitHub Actions for CIHelp me create an AGENTS.md that covers:1. Build and test commands2. Code style guidelines3. Security considerations4. File organization patternsMake it specific enough that an AI agent could follow it precisely.

```



**What you're learning:** Specification writing. Good AGENTS.md files are precise and actionableâ€”skills that transfer to writing specs for Digital FTEs.



### Prompt 2: Test the Hierarchy Rule



```

I have a monorepo with this structure:

```



company/ â”œâ”€â”€ AGENTS.md (says: "Use tabs for indentation") â”œâ”€â”€ apps/ â”‚ â”œâ”€â”€ web/ â”‚ â”‚ â”œâ”€â”€ AGENTS.md (says: "Use Prettier defaults") â”‚ â”‚ â””â”€â”€ src/ â”‚ â””â”€â”€ api/ â”‚ â””â”€â”€ src/ â””â”€â”€ packages/ â””â”€â”€ shared/ â””â”€â”€ src/



```

For each file location below, tell me:1. Which AGENTS.md applies?2. What indentation rule would the agent use?Locations:- apps/web/src/components/Button.tsx- apps/api/src/routes/users.ts- packages/shared/src/utils.ts

```



**What you're learning:** Rule application in hierarchies. Understanding precedence helps you design AGENTS.md structures for complex projects.



### Prompt 3: Diagnose a Problem



```

My Digital SDR is deployed to a client's project, but it keeps making mistakes:- Generates JavaScript instead of TypeScript- Uses `console.log` for debugging- Creates commits with messages like "updates"- Puts new components in the wrong directoryThe client has a README.md but no AGENTS.md.Help me:1. Draft an AGENTS.md that would prevent these specific issues2. Explain to the client why AGENTS.md matters for AI tools

```



**What you're learning:** Diagnosis and remediation. Translating observed problems into preventive rules is key to maintaining Digital FTE quality.



### Core Concept



AGENTS.md is a README for AI agentsâ€”a Markdown file that tells agents your project's coding conventions, build commands, testing requirements, and security rules so they work correctly without guessing.



### Key Mental Models



-   **README vs AGENTS.md**: README is for humans (what the project is), AGENTS.md is for agents (how to work on it)

-   **Hierarchy Rule**: Nearest AGENTS.md winsâ€”root file for project-wide rules, subdirectory files for overrides

-   **Adaptability = Monetization**: A Digital FTE that respects client conventions can be sold to 100 organizations without customization



### Critical Patterns



-   Include: build commands, code style, testing requirements, security rules, commit formats

-   Plain Markdown with no required schemaâ€”easy to write, agents parse naturally

-   Filename must be `AGENTS.md` (uppercase)

-   Monorepos use multiple AGENTS.md files at different directory levels

-   60,000+ projects adopted; supported by Claude Code, Cursor, Copilot, VS Code, Devin, Zed



### Common Mistakes



-   Omitting AGENTS.md and expecting agents to guess your conventions

-   Putting only high-level descriptions instead of actionable commands

-   Forgetting security considerations (API keys, input sanitization)

-   Using a single AGENTS.md for a monorepo with different package standards



### Connections



-   **Builds on**: MCP for tool connectivity (Lesson 2)

-   **Leads to**: goose reference implementation (Lesson 4)



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/Introducing-AI-Driven-Development/aiff-foundation/agents-md-project-context

</AGENTS.md - project-context-for-agents>









<agent-skills>

-   [](/)

-   [Part 1: Introducing AI-Driven Development](/docs/Introducing-AI-Driven-Development)

-   [Chapter 2: AIFF Foundation & Agent Standards](/docs/Introducing-AI-Driven-Development/aiff-foundation)

-   Agent Skills - Packaging Expertise



# Agent Skills - Packaging Expertise



You've spent years mastering financial analysis, or legal document review, or sales qualification. That expertise lives in your headâ€”tacit knowledge that makes you valuable but can't scale.



Every time a client asks you to do what you're expert at, you trade time for money. You're the bottleneck.



**Agent Skills let you package that expertise.**



Remember the Matrix? Trinity needs to fly a helicopter. She doesn't know how. Tank loads the skill. Seconds later: "Let's go."



That's what you're building. Your domain expertiseâ€”years of pattern recognition, decision frameworks, workflow optimizationâ€”encoded into portable skills that any AI agent can load when needed.



The difference between you and Trinity: **you can sell the skill.**



* * *



## Skills Are Your Core Asset



Return to the Agent Factory vision from Chapter 1:



**YOUR domain expertise â†’ Skills â†’ Digital FTE â†’ Recurring Revenue**



Your Knowledge



Becomes



Sells As



Financial analysis expertise



Financial analysis skill



Financial Digital FTE



Legal document review



Contract review skill



Legal assistant FTE



Sales qualification methods



Lead scoring skill



Digital SDR



Customer service workflows



Support handling skill



Customer service FTE



Skills transform what's in your head (tacit knowledge) into what's in a file (explicit knowledge). That file becomes an assetâ€”portable, licensable, monetizable.



When Thomson Reuters paid $650 million for CoCounsel, they weren't buying software. They were buying encoded legal expertise. Skills are how you create similar value in any domain.



* * *



## The SKILL.md Format



Every skill needs a `SKILL.md` file at its root. The format is deliberately simpleâ€”just Markdown with YAML frontmatter:



```

---name: financial-analysisdescription: Analyze financial statements and generate investment reports. Use when reviewing quarterly earnings, comparing company metrics, or preparing investor summaries.---# Financial Analysis Skill## When to Use- User asks for financial statement analysis- Quarterly earnings data needs interpretation- Investment comparison is requested## How to Execute1. Gather the relevant financial documents2. Extract key metrics (revenue, margins, growth rates)3. Compare against industry benchmarks4. Generate structured report with recommendations## Output FormatReports should include:- Executive summary (3 sentences max)- Key metrics table- Year-over-year comparison- Risk factors- Recommendation

```



**The key elements:**



Element



Purpose



Example



**name**



Unique identifier



`financial-analysis`



**description**



When to activate (agent reads this)



"Use when reviewing quarterly earnings..."



**When to Use**



Activation conditions



User asks, data type present



**How to Execute**



Step-by-step workflow



Gather â†’ Extract â†’ Compare â†’ Generate



**Output Format**



Expected deliverable structure



Summary + Table + Analysis



That's it. No special syntax. No compilation. Just structured Markdown that agents parse.



The specification itself lives at agentskills.io/specificationâ€”you can read the entire thing in a few minutes.



* * *



## Progressive Disclosure: The Token Efficiency Secret



Here's the problem skills solve:



**Loading everything upfront wastes tokens.**



If an agent loaded all 50 available skills at startupâ€”full instructions, templates, examplesâ€”you'd burn through your context window before doing any actual work.



The solution is **progressive disclosure**: loading only what's needed, when it's needed.



```

Level 1: Agent Startup (~100 tokens per skill)â”œâ”€â”€ Name: "financial-analysis"â””â”€â”€ Description: "Analyze financial statements..."Level 2: When Skill Activated (< 5K tokens)â””â”€â”€ Full SKILL.md content (when-to-use, execution steps, output format)Level 3: When Actually Neededâ””â”€â”€ Supporting resources (templates, examples, scripts)

```



**How it works:**



**Level 1 (Always loaded):** Agent sees only names and descriptions. For 50 skills, that's roughly 5,000 tokensâ€”manageable.



**Level 2 (On activation):** When the agent decides to use a skill (based on description matching the current task), it loads the full SKILL.md. Still under 5K tokens.



**Level 3 (On demand):** Supporting filesâ€”templates, examples, reference dataâ€”load only when the skill explicitly requests them during execution.



**The efficiency gains are dramatic:**



Approach



Token Cost at Startup



Load everything upfront



14,000 - 80,000+ tokens



Progressive disclosure



~100 tokens per skill



**80-98% token reduction.** This means your Digital FTE can have dozens of capabilities available without bloating its context window. More capabilities, same cost.



* * *



## MCP + Skills: Complementary Standards



MCP and Skills solve different problems. Understanding the distinction is critical for Digital FTE design:



Standard



Purpose



Physical Metaphor



**MCP**



**Connectivity** â€” how agents talk to tools



The agent's **hands**



**Skills**



**Expertise** â€” what agents know how to do



The agent's **training**



**Example: Your Digital SDR Processing Stripe Payments**



Layer



Technology



What It Does



**MCP Server**



Stripe MCP connector



Connects to Stripe API (create charges, refund, list transactions)



**Skill**



Payment processing skill



Knows *how* to handle payment scenarios (retry logic, error recovery, customer communication)



The MCP server gives the agent *access* to Stripe. The skill gives the agent *expertise* in using Stripe properly.



**Without MCP:** Agent can't reach Stripe at all. **Without Skill:** Agent can reach Stripe but doesn't know payment best practices. **With both:** Agent handles payments like an experienced professional.



Your most powerful Digital FTEs combine both: connected via MCP, smart via Skills.



* * *



## Adoption: From Claude to Industry Standard



Agent Skills evolved from internal tool to open standard remarkably fast:



Date



Milestone



**October 16, 2025**



Anthropic launches Agent Skills for Claude Code



**December 18, 2025**



Anthropic releases Agent Skills as **open standard** at agentskills.io



**December 2025**



OpenAI adopts the same SKILL.md format for Codex CLI and ChatGPT



**Agent support (December 2025):**



-   Claude Code, Claude apps (Pro, Max, Team, Enterprise)

-   Codex CLI, ChatGPT (OpenAI adoption)

-   VS Code, GitHub Copilot (Microsoft adoption)

-   Cursor, Amp, goose, OpenCode, Letta



**Partner skills available at launch:**



-   Canva (design automation)

-   Stripe (payment processing)

-   Notion, Figma, Atlassian

-   Cloudflare, Ramp, Sentry, Zapier



The Anthropic skills repository contains community and official skills ranging from creative applications (art, music, design) to technical tasks (testing web apps, MCP server generation) to enterprise workflows.



**Skills you write today work across this entire ecosystem.**



* * *



## Monetization Paths



Skills open four revenue streams for your Agent Factory:



### 1\. License Individual Skills



Sell your financial-analysis skill to other agent builders. They pay per-seat or per-use.



**Example:** Your lead qualification skill, licensed to 50 marketing agencies at $200/month each = $10,000/month recurring.



### 2\. Skill Bundles



Package related skills for specific verticals.



**Example:** "Complete SDR Skill Set" includes lead qualification, email personalization, objection handling, and meeting scheduling. Premium pricing for the bundle.



### 3\. Skill-Enhanced Digital FTEs



Deploy agents with your skills as subscription productsâ€”the core Agent Factory model.



**Example:** Your Digital SDR subscription at $1,500/month, powered by skills you've encoded. 100 clients = $150,000/month.



### 4\. Skill Development Consulting



Help domain experts encode their expertise into skills.



**Example:** Law firms, financial advisors, healthcare organizations want custom skills but lack encoding expertise. You build skills for them.



* * *



## Your Agent Factory Blueprint



As you progress through this book, skills connect to everything:



**Part 3 (Claude Code Mastery):** Use existing skills. Experience progressive disclosure in action. Understand what makes skills effective.



**Part 5 (Custom MCP Servers):** Build MCP servers that your skills orchestrate. Skills become the brain; MCP servers become the hands.



**Part 6 (Custom Agents):** Architect Digital FTEs that combine your skills with MCP connectivity. The complete Agent Factory product.



The skills you design in Part 3 become components of the Digital FTEs you sell in Part 6.



* * *



## Try With AI



Use your AI companion (Claude, ChatGPT, Gemini, or similar) to practice skill design:



### Prompt 1: Create Your First Skill



```

I'm a [your profession/domain] expert. Help me design a SKILL.md for my core expertise.I need:1. A clear name and description (the agent reads this to decide when to activate)2. Specific "When to Use" conditions3. Step-by-step "How to Execute" instructions4. A structured "Output Format"Make it specific enough that an AI agent could follow it without asking clarifying questions.

```



**What you're learning:** Skill specification. The ability to extract tacit knowledge from your head and structure it for AI consumption. This is the core Agent Factory skill.



### Prompt 2: Test the MCP vs Skills Distinction



```

I'm building a Digital [your role] that needs both MCP servers and Skills.For each capability below, tell me:- Is this MCP (connectivity) or Skill (expertise)?- Why?Capabilities:1. Connect to my company's CRM to read lead data2. Know the best practices for qualifying B2B leads3. Send emails through our email platform4. Write personalized follow-up emails that convert5. Schedule calendar meetings6. Decide the optimal time to follow up with each lead

```



**What you're learning:** Architectural classification. Understanding which layer handles which concern prevents over-engineering and under-delivering.



### Prompt 3: Explore Monetization



```

I have expertise in [your domain]. Based on the four skill monetization paths:1. License individual skills2. Skill bundles3. Skill-enhanced Digital FTEs4. Skill development consultingHelp me think through:- Which path has the highest leverage for my expertise?- What would I need to build for each path?- What's the realistic monthly revenue potential for each?Push back if my expectations seem unrealistic.

```



**What you're learning:** Business model thinking. Connecting technical capability to revenue requires understanding both the skill and the market.



### Core Concept



Agent Skills are folders containing instructions (SKILL.md) that encode your domain expertise into reusable packages any AI agent can loadâ€”transforming tacit knowledge (in your head) into explicit knowledge (in a file) that's portable, licensable, and monetizable.



### Key Mental Models



-   **Matrix Skill Loading**: Like Trinity downloading helicopter skills, agents load expertise on-demandâ€”years of experience, instantly available

-   **Progressive Disclosure**: Three-level loading (names at startup ~100 tokens, full SKILL.md when activated <5K, supporting files on demand) achieves 80-98% token reduction

-   **MCP = Connectivity, Skills = Expertise**: MCP server gives agent *access* to Stripe API; skill gives agent *expertise* in handling payment scenarios properly



### Critical Patterns



-   SKILL.md format: YAML frontmatter (name, description for activation) + Markdown instructions (when to use, how to execute, output format)

-   Adoption timeline: October 2025 (Claude Code), December 2025 (open standard at agentskills.io), OpenAI/Microsoft adoption

-   Four monetization paths: license individual skills, sell skill bundles, deploy skill-enhanced Digital FTEs, consulting on skill development



### Common Mistakes



-   Loading all skill content upfront (wastes tokens, bloats contextâ€”defeats the purpose)

-   Confusing Skills with MCP (they're complementary: Skills = brain, MCP = hands)

-   Writing vague descriptions that don't help agents decide when to activate

-   Publishing proprietary methodology to public repositories without considering competitive advantage



### Connections



-   **Builds on**: MCP for connectivity (Lesson 2), goose as reference agent showing skills in action (Lesson 4)

-   **Leads to**: Part 3 Claude Code Mastery (using skills), Part 5-6 (building skills into Custom Agents)



ðŸ“‹Quick Reference



### Unlock Lesson Summary



Access condensed key takeaways and quick reference notes for efficient review.



-   Key concepts at a glance

-   Perfect for revision

-   Save study time



Free forever. No credit card required.



---

Source: https://agentfactory.panaversity.org/docs/Introducing-AI-Driven-Development/aiff-foundation/agent-skills-packaging-expertise

</agent-skills>









<spec-kit-monorepo>

Spec-Kit Monorepo Folder Structure

Your-Project/

â”œâ”€â”€ .spec-kit/                    # Spec-Kit configuration

â”‚   â””â”€â”€ config.yaml

â”œâ”€â”€ specs/                        # Spec-Kit managed specifications

â”‚   â”œâ”€â”€ overview.md               # Project overview

â”‚   â”œâ”€â”€ architecture.md           # System architecture

â”‚   â”œâ”€â”€ features/                 # Feature specifications

â”‚   â”‚   â”œâ”€â”€ task-crud.md

â”‚   â”‚   â”œâ”€â”€ authentication.md

â”‚   â”‚   â””â”€â”€ chatbot.md

â”‚   â”œâ”€â”€ api/                      # API specifications

â”‚   â”‚   â”œâ”€â”€ rest-endpoints.md

â”‚   â”‚   â””â”€â”€ mcp-tools.md

â”‚   â”œâ”€â”€ database/                 # Database specifications

â”‚   â”‚   â””â”€â”€ schema.md

â”‚   â””â”€â”€ ui/                       # UI specifications

â”‚       â”œâ”€â”€ components.md

â”‚       â””â”€â”€ pages.md

â”œâ”€â”€ CLAUDE.md                     # Root Claude Code instructions

â”œâ”€â”€ frontend/

â”‚   â”œâ”€â”€ CLAUDE.md               # frontend Claude Code instructions

â”‚   â””â”€â”€ ... (Next.js app)

â”œâ”€â”€ backend/

â”‚   â”œâ”€â”€ CLAUDE.md                # backend Claude Code instructions

â”‚   â””â”€â”€ ... (FastAPI app)

â”œâ”€â”€ docker-compose.yml

â””â”€â”€ README.md



</spec-kit-monorepo>







CLAUDE.md Files

Create multiple CLAUDE.md files to provide context at different levels:



<root-CLAUDE.md>

Root CLAUDE.md

# Todo App - Hackathon II

 

## Project Overview

This is a monorepo using GitHub Spec-Kit for spec-driven development.

 

## Spec-Kit Structure

Specifications are organized in /specs:

- /specs/overview.md - Project overview

- /specs/features/ - Feature specs (what to build)

- /specs/api/ - API endpoint and MCP tool specs

- /specs/database/ - Schema and model specs

- /specs/ui/ - Component and page specs

 

## How to Use Specs

1. Always read relevant spec before implementing

2. Reference specs with: @specs/features/task-crud.md

3. Update specs if requirements change

 

## Project Structure

- /frontend - Next.js 14 app

- /backend - Python FastAPI server

 

## Development Workflow

1. Read spec: @specs/features/[feature].md

2. Implement backend: @backend/CLAUDE.md

3. Implement frontend: @frontend/CLAUDE.md

4. Test and iterate

 

## Commands

- Frontend: cd frontend && npm run dev

- Backend: cd backend && uvicorn main:app --reload

- Both: docker-compose up



</root-CLAUDE.md>







<frontendCLAUDE.md>

Frontend CLAUDE.md

# Frontend Guidelines

 

## Stack

- Next.js 14 (App Router)

- TypeScript

- Tailwind CSS

 

## Patterns

- Use server components by default

- Client components only when needed (interactivity)

- API calls go through `/lib/api.ts`

 

## Component Structure

- `/components` - Reusable UI components

- `/app` - Pages and layouts

 

## API Client

All backend calls should use the api client:

 

import { api } from '@/lib/api'

const tasks = await api.getTasks()

 

## Styling

- Use Tailwind CSS classes

- No inline styles

- Follow existing component patterns



</frontendCLAUDE.md>









<backend CLAUDE.md>

Backend CLAUDE.md

# Backend Guidelines

 

## Stack

- FastAPI

- SQLModel (ORM)

- Neon PostgreSQL

 

## Project Structure

- `main.py` - FastAPI app entry point

- `models.py` - SQLModel database models

- `routes/` - API route handlers

- `db.py` - Database connection

 

## API Conventions

- All routes under `/api/`

- Return JSON responses

- Use Pydantic models for request/response

- Handle errors with HTTPException

 

## Database

- Use SQLModel for all database operations

- Connection string from environment variable: DATABASE_URL

 

## Running

uvicorn main:app --reload --port 8000



</backend CLAUDE.md>













<anthropic-resource>

1. Customize your setup

Claude Code is an agentic coding assistant that automatically pulls context into prompts. This context gathering consumes time and tokens, but you can optimize it through environment tuning.

a. Create CLAUDE.md files

CLAUDE.md is a special file that Claude automatically pulls into context when starting a conversation. This makes it an ideal place for documenting:

Common bash commands

Core files and utility functions

Code style guidelines

Testing instructions

Repository etiquette (e.g., branch naming, merge vs. rebase, etc.)

Developer environment setup (e.g., pyenv use, which compilers work)

Any unexpected behaviors or warnings particular to the project

Other information you want Claude to remember

Thereâ€™s no required format for CLAUDE.md files. We recommend keeping them concise and human-readable. For example:

# Bash commands

- npm run build: Build the project

- npm run typecheck: Run the typechecker



# Code style

- Use ES modules (import/export) syntax, not CommonJS (require)

- Destructure imports when possible (eg. import { foo } from 'bar')



# Workflow

- Be sure to typecheck when youâ€™re done making a series of code changes

- Prefer running single tests, and not the whole test suite, for performance



Copy

You can place CLAUDE.md files in several locations:

The root of your repo, or wherever you run claude from (the most common usage). Name it CLAUDE.md and check it into git so that you can share it across sessions and with your team (recommended), or name it CLAUDE.local.md and .gitignore it

Any parent of the directory where you run claude. This is most useful for monorepos, where you might run claude from root/foo, and have CLAUDE.md files in both root/CLAUDE.md and root/foo/CLAUDE.md. Both of these will be pulled into context automatically

Any child of the directory where you run claude. This is the inverse of the above, and in this case, Claude will pull in CLAUDE.md files on demand when you work with files in child directories

Your home folder (~/.claude/CLAUDE.md), which applies it to all your claude sessions

When you run the /init command, Claude will automatically generate a CLAUDE.md for you.

b. Tune your CLAUDE.md files

Your CLAUDE.md files become part of Claudeâ€™s prompts, so they should be refined like any frequently used prompt. A common mistake is adding extensive content without iterating on its effectiveness. Take time to experiment and determine what produces the best instruction following from the model.

You can add content to your CLAUDE.md manually or press the # key to give Claude an instruction that it will automatically incorporate into the relevant CLAUDE.md. Many engineers use # frequently to document commands, files, and style guidelines while coding, then include CLAUDE.md changes in commits so team members benefit as well.

At Anthropic, we occasionally run CLAUDE.md files through the prompt improver and often tune instructions (e.g. adding emphasis with "IMPORTANT" or "YOU MUST") to improve adherence.





</anthropic-resource>











<my-sub-agent-list>                                                        







 ux-frontend-developer Â· sonnet                                                                                     â”‚







â”‚   lead-architect Â· opus                                                                                              â”‚







â”‚   devops-rag-engineer Â· sonnet                                                                                       â”‚







â”‚   backend-builder Â· opus                                                                                             â”‚







â”‚   task-orchestrator Â· sonnet                                                                                         â”‚







â”‚   spec-architect Â· opus                                                                                              â”‚







â”‚   robotics-textbook-author Â· sonnet                                                                                  â”‚







â”‚   qa-overseer Â· opus                                                                                                 â”‚







â”‚   path-warden Â· sonnet                                                                                               â”‚







â”‚   modular-ai-architect Â· opus                                                                                        â”‚







â”‚   loop-controller Â· opus                                                                                             â”‚







â”‚   imperator Â· opus                                                                                                   â”‚







â”‚   enterprise-grade-validator Â· sonnet                                                                                â”‚







â”‚   docusaurus-librarian Â· sonnet                                                                                      â”‚







â”‚   agent-specialization-architect Â· sonnet                                                                            â”‚







â”‚   content-builder Â· sonnet                                                                                           â”‚







â”‚                                                                                                                      â”‚







â”‚   Built-in agents (always available)                                                                                 â”‚







â”‚   general-purpose Â· sonnet                                                                                           â”‚







â”‚   statusline-setup Â· sonnet                                                                                          â”‚







â”‚   Explore Â· haiku                                                                                                    â”‚







â”‚   Plan Â· inherit                                                                                                     â”‚







â”‚   claude-code-guide Â· haiku







</my-sub-agent-list>











<claude-skills-list>







  Skills







 84 skills















 User skills (C:\Users\PCW\.claude\skills)







 algorithmic-art Â· ~4.9k tokens







 auto-architect Â· ~1.4k tokens







 brand-guidelines Â· ~496 tokens







 browsing-with-playwright Â· ~1.2k tokens







 build-error-handler Â· ~1.3k tokens







 building-chat-interfaces Â· ~2.4k tokens







 building-chat-widgets Â· ~2.3k tokens







 building-chatgpt-apps Â· ~3.2k tokens







 building-mcp-servers Â· ~1.3k tokens







 building-nextjs-apps Â· ~1.8k tokens







 building-rag-systems Â· ~2.7k tokens







 canvas-design Â· ~2.9k tokens







 code-cleanliness Â· ~1.0k tokens







 command-orchestration Â· ~122 tokens







 configuring-better-auth Â· ~1.8k tokens







 configuring-dapr-pubsub Â· ~2.5k tokens







 containerizing-applications Â· ~1.9k tokens







 context-degradation Â· ~3.9k tokens







 context-fundamentals Â· ~3.0k tokens







 context-optimization Â· ~2.1k tokens







 cors-config Â· ~897 tokens







 creating-skills Â· ~2.7k tokens







 db-timeout-handler Â· ~1.1k tokens







 deploying-cloud-k8s Â· ~2.1k tokens







 deploying-kafka-k8s Â· ~1.8k tokens







 deploying-postgres-k8s Â· ~2.0k tokens







 deployment-preflight-check Â· ~751 tokens







 deployment-stability Â· ~885 tokens







 doc-coauthoring Â· ~3.9k tokens







 dockerfile-generator Â· ~1.4k tokens







 docx Â· ~2.5k tokens







 env-validator Â· ~611 tokens







 evaluation Â· ~2.6k tokens







 fetching-library-docs Â· ~1.5k tokens







 frontend-design Â· ~999 tokens







 generate-chapter Â· ~2.3k tokens







 hackathon-rules Â· ~140 tokens







 helm-chart-generator Â· ~1.9k tokens







 installing-skill-tracker Â· ~272 tokens







 internal-comms Â· ~282 tokens







 jwt-middleware Â· ~1.1k tokens







 k8s-manifests-generator Â· ~1.6k tokens







 k8s-pod-crash-handler Â· ~1.3k tokens







 mcp-builder Â· ~2.2k tokens







 memory-systems Â· ~3.2k tokens







 multi-agent-patterns Â· ~3.6k tokens







 neon-db-setup Â· ~721 tokens







 nextjs-devtools Â· ~519 tokens







 nx-monorepo Â· ~1.9k tokens







 operating-k8s-local Â· ~1.6k tokens







 operating-production-services Â· ~1.2k tokens







 pdf Â· ~1.8k tokens







 phase-execution-controller Â· ~241 tokens







 phase-progress-auditor Â· ~242 tokens







 phase-workflow-runner Â· ~394 tokens







 pptx Â· ~6.4k tokens







 project-context Â· ~218 tokens







 qa-overseer Â· ~2.0k tokens







 rag-ingest Â· ~954 tokens







 researching-with-deepwiki Â· ~1.2k tokens







 review-and-judge Â· ~118 tokens







 scaffolding-fastapi-dapr Â· ~2.2k tokens







 scaffolding-openai-agents Â· ~3.3k tokens







 security-scan Â· ~607 tokens







 session-state-manager Â· ~763 tokens







 skill-creator Â· ~4.4k tokens







 slack-gif-creator Â· ~1.9k tokens







 spec-architect Â· ~1.5k tokens







 spec-driven-development Â· ~162 tokens







 streaming-llm-responses Â· ~2.0k tokens







 styling-with-shadcn Â· ~2.0k tokens







 systematic-debugging Â· ~1.4k tokens







 template-skill Â· ~8 tokens







 template-selector Â· ~110 tokens







 test-ui Â· ~1.3k tokens







 theme-factory Â· ~708 tokens







 todo-domain-expert Â· ~122 tokens







 tool-design Â· ~3.2k tokens







 translate-urdu Â· ~1.8k tokens







 web-artifacts-builder Â· ~691 tokens







 webapp-testing Â· ~916 tokens







 working-with-documents Â· ~1.5k tokens







 working-with-spreadsheets Â· ~1.3k tokens







 xlsx Â· ~2.6k tokens







</claude-skills-list>













































<available-mcp-to-claude>







âœ” connected:







chrome-devtools







code-search







context7







docker







filesystem







github







hopx-sandbox







mysql







nx-mcp







playwright







postgres







ragie







scriptflow







vercel-awesome-ai







windows-cli















âœ˜ failed:







git







mcp-hfspace







n8n-local







Neon







MCP_DOCKER







mcp_server_mysql















â–³ needs authentication:















</available-mcp-to-claude>

